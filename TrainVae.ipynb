{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title 链接Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ARUEglBnfSmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 全连接"
      ],
      "metadata": {
        "id": "5IrhtWNs776H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SDJorEsCiaNz"
      },
      "outputs": [],
      "source": [
        "#@title vae全链接训练\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# 定义VAE模型\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # 编码器网络\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # 均值和对数方差\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "        # 解码器网络\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "# 计算VAE的损失函数\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# 参数设置\n",
        "input_dim = 28 * 28  # MNIST数据集的输入维度\n",
        "hidden_dim = 400\n",
        "latent_dim = 20\n",
        "lr = 1e-3\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "# 检查是否可以使用GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 加载MNIST数据集\n",
        "transform = transforms.ToTensor()\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 初始化模型、优化器\n",
        "model = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# 训练VAE\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.view(-1, input_dim).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {train_loss/len(train_loader.dataset):.4f}')\n",
        "\n",
        "# 生成样本\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(64, latent_dim).to(device)\n",
        "    sample = model.decode(z).cpu()\n",
        "    sample = sample.view(64, 1, 28, 28)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 验证模型\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_images(original, reconstructed, num_images=10):\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for i in range(num_images):\n",
        "        # 展示原始图像\n",
        "        ax = plt.subplot(2, num_images, i + 1)\n",
        "        plt.imshow(original[i].reshape(28, 28), cmap=\"gray\")\n",
        "        ax.axis('off')\n",
        "\n",
        "        # 展示重构图像\n",
        "        ax = plt.subplot(2, num_images, i + 1 + num_images)\n",
        "        plt.imshow(reconstructed[i].reshape(28, 28), cmap=\"gray\")\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# 从测试集加载数据\n",
        "test_loader = DataLoader(datasets.MNIST('./data', train=False, transform=transforms.ToTensor()), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 验证重构效果\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data, _ in test_loader:\n",
        "        data = data.view(-1, input_dim).to(device)\n",
        "        recon_data, _, _ = model(data)\n",
        "        show_images(data.cpu(), recon_data.cpu())\n",
        "        break"
      ],
      "metadata": {
        "id": "NEMgISdu4qn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 卷积\n"
      ],
      "metadata": {
        "id": "vz_uoKE48B8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title vae卷积训练\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# 参数设置\n",
        "input_dim = 28 * 28  # MNIST数据集的输入维度\n",
        "hidden_dim = 400\n",
        "latent_dim = 20\n",
        "lr = 1e-3\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "# 检查是否可以使用GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 定义卷积VAE模型\n",
        "class ConvVAE(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(ConvVAE, self).__init__()\n",
        "\n",
        "        # 编码器网络\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # [batch, 1, 28, 28] -> [batch, 32, 14, 14]\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # [batch, 32, 14, 14] -> [batch, 64, 7, 7]\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # [batch, 64, 7, 7] -> [batch, 128, 4, 4]\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()  # [batch, 128, 4, 4] -> [batch, 128*4*4]\n",
        "        )\n",
        "\n",
        "        self.fc_mu = nn.Linear(128 * 3 * 3, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(128 * 3 * 3, latent_dim)\n",
        "\n",
        "        # 解码器网络\n",
        "        self.decoder_input = nn.Linear(latent_dim, 128 * 3 * 3)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(72, 64, kernel_size=3, stride=2, padding=1),  # [batch, 128, 4, 4] -> [batch, 64, 8, 8]\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # [batch, 64, 8, 8] -> [batch, 32, 16, 16]\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),  # [batch, 32, 16, 16] -> [batch, 1, 28, 28]\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        # print(\"x shape:\", x.shape)\n",
        "        # print(\"h shape:\", h.shape)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = self.decoder_input(z)\n",
        "        # print(\"hh shape:\", h.shape)\n",
        "        h = h.view(128, -1, 4, 4)  # 重新reshape为解码器的输入维度\n",
        "        # print(\"hhh shape:\", h.shape)\n",
        "        return self.decoder(h)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        tmp = self.decode(z)\n",
        "        # print(\"z shape:\", z.shape)\n",
        "        # print(\"tmp shape:\", tmp.shape)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "# 计算VAE的损失函数\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# 参数设置\n",
        "latent_dim = 20\n",
        "lr = 1e-3\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "# 加载MNIST数据集\n",
        "transform = transforms.ToTensor()\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 初始化模型、优化器\n",
        "model = ConvVAE(latent_dim).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# 训练VAE\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        if (data.shape[0] < 128 ):\n",
        "          continue\n",
        "        recon_batch, mu, logvar = model(data.to(device))\n",
        "        loss = loss_function(recon_batch, data.to(device), mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        # print(\"recon_batch: \", recon_batch.shape)\n",
        "\n",
        "        # if (recon_batch.shape[1] < 128 ):\n",
        "        #   continue\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {train_loss/len(train_loader.dataset):.4f}')\n",
        "\n",
        "# # 生成样本\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     z = torch.randn(64, latent_dim).to(device)\n",
        "#     sample = model.decode(z).to(device).cpu()\n",
        "#     sample = sample.view(64, 1, 28, 28)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EEHlfOnK8Er8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 验证模型\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "def show_images(original, reconstructed, num_images=10):\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for i in range(num_images):\n",
        "        # 展示原始图像\n",
        "        ax = plt.subplot(2, num_images, i + 1)\n",
        "        plt.imshow(original[i].reshape(28, 28), cmap=\"gray\")\n",
        "        ax.axis('off')\n",
        "\n",
        "        # 展示重构图像\n",
        "        ax = plt.subplot(2, num_images, i + 1 + num_images)\n",
        "        plt.imshow(reconstructed[i].reshape(28, 28), cmap=\"gray\")\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# 设置设备\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 加载MNIST测试集数据\n",
        "test_loader = DataLoader(\n",
        "    datasets.MNIST('./data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 验证重构效果\n",
        "model.eval()\n",
        "model.to(device)  # 确保模型在正确的设备上\n",
        "with torch.no_grad():\n",
        "    for data, _ in test_loader:\n",
        "        data = data.to(device)  # 不需要展平数据\n",
        "        recon_data, _, _ = model(data)\n",
        "        show_images(data.cpu(), recon_data.cpu())\n",
        "        break  # 仅显示一个批次的数据\n"
      ],
      "metadata": {
        "id": "6n0JcCB18c-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vae和gan"
      ],
      "metadata": {
        "id": "YGyUig_ji-OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 训练\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 参数设置\n",
        "latent_dim = 20\n",
        "lr = 1e-3\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "# 加载MNIST数据集\n",
        "transform = transforms.ToTensor()\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # [batch, 1, 28, 28] -> [batch, 32, 14, 14]\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # [batch, 32, 14, 14] -> [batch, 64, 7, 7]\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # [batch, 64, 7, 7] -> [batch, 128, 3, 3]\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()  # [batch, 128, 3, 3] -> [batch, 128*3*3]\n",
        "        )\n",
        "\n",
        "        self.fc_mu = nn.Linear(128 * 3 * 3, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(128 * 3 * 3, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.decoder_input = nn.Linear(latent_dim, 128 * 3 * 3)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=1),  # [batch, 128, 3, 3] -> [batch, 64, 6, 6]\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # [batch, 64, 6, 6] -> [batch, 32, 12, 12]\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),  # [batch, 32, 12, 12] -> [batch, 1, 28, 28]\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        h = self.decoder_input(z)\n",
        "        h = h.view(z.size(0), 128, 3, 3)  # 调整为解码器输入维度\n",
        "        return self.decoder(h)\n",
        "\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.discriminator = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 3 * 3, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.discriminator(x)\n",
        "\n",
        "def reparameterize(mu, logvar):\n",
        "    std = torch.exp(0.5 * logvar)\n",
        "    eps = torch.randn_like(std)\n",
        "    return mu + eps * std\n",
        "\n",
        "def vae_gan_loss(recon_x, x, mu, logvar, D_real, D_fake):\n",
        "    # VAE损失\n",
        "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    # GAN损失\n",
        "    D_loss = nn.functional.binary_cross_entropy(D_real, torch.ones_like(D_real)) + \\\n",
        "             nn.functional.binary_cross_entropy(D_fake, torch.zeros_like(D_fake))\n",
        "\n",
        "    G_loss = nn.functional.binary_cross_entropy(D_fake, torch.ones_like(D_fake))\n",
        "\n",
        "    return BCE + KLD + G_loss, D_loss\n",
        "\n",
        "# 初始化模型\n",
        "latent_dim = 20\n",
        "encoder = Encoder(latent_dim).to(device)\n",
        "decoder = Decoder(latent_dim).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "optimizer_vae = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "# 训练循环\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "\n",
        "        # 训练判别器\n",
        "        optimizer_d.zero_grad()\n",
        "\n",
        "        # VAE前向传播\n",
        "        mu, logvar = encoder(data)\n",
        "        z = reparameterize(mu, logvar)\n",
        "        recon_batch = decoder(z)\n",
        "\n",
        "        D_real = discriminator(data)\n",
        "        D_fake = discriminator(recon_batch.detach())  # 注意这里使用了detach\n",
        "\n",
        "        # 计算判别器的损失并进行反向传播\n",
        "        loss_d = nn.functional.binary_cross_entropy(D_real, torch.ones_like(D_real)) + \\\n",
        "                 nn.functional.binary_cross_entropy(D_fake, torch.zeros_like(D_fake))\n",
        "        loss_d.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # 训练生成器（VAE）\n",
        "        optimizer_vae.zero_grad()\n",
        "\n",
        "        # 为生成器重新计算D_fake_for_g\n",
        "        D_fake_for_g = discriminator(recon_batch)\n",
        "        loss_g = nn.functional.binary_cross_entropy(D_fake_for_g, torch.ones_like(D_fake))\n",
        "\n",
        "        # 计算VAE的损失并进行反向传播\n",
        "        loss_vae = nn.functional.binary_cross_entropy(recon_batch, data, reduction='sum') + \\\n",
        "                   -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) + \\\n",
        "                   loss_g\n",
        "        loss_vae.backward()\n",
        "        optimizer_vae.step()\n",
        "\n",
        "        train_loss += loss_vae.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {train_loss / len(train_loader.dataset):.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "EtTe6kPxjOx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 验证\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(64, latent_dim).to(device)\n",
        "    generated_images = decoder(z).cpu()\n",
        "\n",
        "# 展示生成的图像\n",
        "fig, axes = plt.subplots(8, 8, figsize=(8, 8))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(generated_images[i].view(28, 28), cmap='gray')\n",
        "    ax.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Qf3maMxA9044"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}