{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmFxpN-lw-BG"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title 链接Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNDxnVDghowH"
      },
      "source": [
        "## 数据处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o-9UNoAhfUI"
      },
      "outputs": [],
      "source": [
        "#@title 读取一批次图片\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "\n",
        "# 定义数据转换\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # 归一化到 [-1, 1]\n",
        "])\n",
        "\n",
        "# 创建 DataLoader\n",
        "batch_size = 64\n",
        "dataloader = DataLoader(\n",
        "    datasets.MNIST('.', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 获取一个批次的数据\n",
        "data_iter = iter(dataloader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "# 反转处理，转换为 NumPy 数组以便显示\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # 反归一化\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# 显示前 8 张图像\n",
        "imshow(torchvision.utils.make_grid(images[:8]))\n",
        "\n",
        "# 打印标签\n",
        "print('Labels:', labels[:8].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0p0Pb63qrE7i"
      },
      "outputs": [],
      "source": [
        "#@title 简单数据\n",
        "# 数据集加载\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    datasets.MNIST('.', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0cHAYmwhtWw"
      },
      "source": [
        "##模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "OgmwT05QtRD9"
      },
      "outputs": [],
      "source": [
        "#@title 初始化模型参数\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "# 超参数\n",
        "latent_dim = 100\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "epochs = 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1p8zGjFFdymL"
      },
      "outputs": [],
      "source": [
        "#@title 1、线性模型\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "# 超参数\n",
        "latent_dim = 100\n",
        "batch_size = 64\n",
        "lr = 0.0002\n",
        "epochs = 1\n",
        "\n",
        "# 生成器网络\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(1024, 28*28),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        # print(\"img shape:\", img.shape)\n",
        "        # print(\"z img shape:\", z.shape)\n",
        "        img = img.view(img.size(0), 1, 28, 28)\n",
        "        return img\n",
        "\n",
        "# 判别器网络\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "        # print(\"validity shape:\", validity.shape)\n",
        "        return validity\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2sQuQaKpn3ag"
      },
      "outputs": [],
      "source": [
        "#@title   1.1、第三方全连接\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(opt.latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(img_shape))),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), *img_shape)\n",
        "        return img\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(img_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "\n",
        "        return validity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9a1GbCLvs7Z1"
      },
      "outputs": [],
      "source": [
        "#@title 2、conv2d模型\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim, 1024, kernel_size=2, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.ConvTranspose2d(1024, 512, kernel_size=3, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.ConvTranspose2d(128, 1, kernel_size=4, stride=2, padding=1),\n",
        "\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = z.view(z.size(0), latent_dim, 1, 1)  # 调整输入的形状\n",
        "        img = self.model(z)\n",
        "        print(\"img shape:\", img.shape)\n",
        "        return img\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, kernel_size=3, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        validity = self.model(img)\n",
        "        print(\"shape:\", validity.shape)\n",
        "        return validity.view(validity.size(0), -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jlGRcz3Koc3I"
      },
      "outputs": [],
      "source": [
        "#@title 2.1 自注意力\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query_conv = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_dim, in_dim // 8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_dim, in_dim, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, C, width, height = x.size()\n",
        "        proj_query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)\n",
        "        proj_key = self.key_conv(x).view(batch_size, -1, width * height)\n",
        "        energy = torch.bmm(proj_query, proj_key)\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "        proj_value = self.value_conv(x).view(batch_size, -1, width * height)\n",
        "\n",
        "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, C, width, height)\n",
        "\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim, 512, kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            SelfAttention(512),  # 加入自注意力层\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            SelfAttention(256),  # 加入自注意力层\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.ConvTranspose2d(128, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = z.view(z.size(0), latent_dim, 1, 1)  # 调整输入的形状\n",
        "        img = self.model(z)\n",
        "        print(\"img shape:\", img.shape)\n",
        "        return img\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            SelfAttention(64),  # 加入自注意力层\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, kernel_size=3, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        validity = self.model(img)\n",
        "        print(\"shape:\", validity.shape)\n",
        "        return validity.view(validity.size(0), -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "Y-j5uAuWtEw7"
      },
      "outputs": [],
      "source": [
        "#@title 2.2 自注意力stable diffusion\n",
        "import math\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_embed, in_proj_bias=True, out_proj_bias=True, n_heads=8):\n",
        "        super().__init__()\n",
        "        n_heads = 8\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_embed // n_heads\n",
        "        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)\n",
        "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (Batch_Size, Channels, Height, Width)\n",
        "        batch_size, channels, height, width = x.shape\n",
        "\n",
        "        # Reshape to (Batch_Size, Height * Width, Channels)\n",
        "        x = x.view(batch_size, channels, height * width).transpose(1, 2)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        seq_len = x.size(1)\n",
        "        d_embed = x.size(2)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim * 3) -> 3 tensor of shape (Batch_Size, Seq_Len, Dim)\n",
        "        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, H, Seq_Len, Dim / H)\n",
        "        q = q.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        k = k.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        v = v.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        # Self-attention calculation\n",
        "        weight = q @ k.transpose(-2, -1) / math.sqrt(self.d_head)\n",
        "        weight = F.softmax(weight, dim=-1)\n",
        "        output = weight @ v\n",
        "\n",
        "        # Reshape back to (Batch_Size, Seq_Len, Dim)\n",
        "        output = output.transpose(1, 2).reshape(batch_size, seq_len, d_embed)\n",
        "\n",
        "        # Apply the final linear projection\n",
        "        output = self.out_proj(output)\n",
        "\n",
        "        # Reshape back to (Batch_Size, Channels, Height, Width)\n",
        "        output = output.transpose(1, 2).view(batch_size, channels, height, width)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Generator 和 Discriminator 类的实现保持不变，只是在适当的位置使用 SelfAttention\n",
        "\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim, 512, kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            SelfAttention(512),  # 加入自注意力层\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            SelfAttention(256),  # 加入自注意力层\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.ConvTranspose2d(128, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = z.view(z.size(0), latent_dim, 1, 1)  # 调整输入的形状\n",
        "        img = self.model(z)\n",
        "        print(\"img shape:\", img.shape)\n",
        "        return img\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            SelfAttention(64),  # 加入自注意力层\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, kernel_size=3, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        validity = self.model(img)\n",
        "        print(\"shape:\", validity.shape)\n",
        "        return validity.view(validity.size(0), -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GGZOWSRPtLGF"
      },
      "outputs": [],
      "source": [
        "#@title 损失器\n",
        "\n",
        "# 初始化生成器和判别器\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# 损失函数\n",
        "adversarial_loss = nn.BCELoss()\n",
        "\n",
        "# 优化器\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr)\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2B6vBccqUQ8"
      },
      "source": [
        "## 训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--vnuxB1qSyl"
      },
      "outputs": [],
      "source": [
        "#@title 训练GAN\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# 初始化生成器和判别器\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# 检查生成器权重文件是否存在\n",
        "generator_weights_path = '/content/drive/MyDrive/generator_epoch.pth'\n",
        "if os.path.exists(generator_weights_path):\n",
        "    generator.load_state_dict(torch.load(generator_weights_path))\n",
        "    print(f\"加载生成器的权重成功：{generator_weights_path}\")\n",
        "else:\n",
        "    print(f\"生成器权重文件未找到：{generator_weights_path}\")\n",
        "\n",
        "# 检查判别器权重文件是否存在\n",
        "discriminator_weights_path = '/content/drive/MyDrive/discriminator_epoch.pth'\n",
        "if os.path.exists(discriminator_weights_path):\n",
        "    discriminator.load_state_dict(torch.load(discriminator_weights_path))\n",
        "    print(f\"加载判别器的权重成功：{discriminator_weights_path}\")\n",
        "else:\n",
        "    print(f\"判别器权重文件未找到：{discriminator_weights_path}\")\n",
        "\n",
        "\n",
        "\n",
        "# 损失函数\n",
        "adversarial_loss = nn.BCELoss()\n",
        "\n",
        "# 优化器\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr)\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "        # 标签\n",
        "        # valid = torch.ones(imgs.size(0), 1)\n",
        "        # fake = torch.zeros(imgs.size(0), 1)\n",
        "\n",
        "        valid = torch.full((imgs.size(0), 1), 0.9)  # 真实标签设置为 0.9\n",
        "        fake = torch.full((imgs.size(0), 1), 0.1)  # 生成标签设置为 0.1\n",
        "\n",
        "\n",
        "        # -----------------\n",
        "        # 训练生成器\n",
        "        # -----------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        z = torch.randn(imgs.size(0), latent_dim)\n",
        "        gen_imgs = generator(z)\n",
        "\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        # 训练判别器\n",
        "        # ---------------------\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        real_loss = adversarial_loss(discriminator(imgs), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # 打印损失\n",
        "        print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
        "\n",
        "        # if i > 2:\n",
        "        #   break\n",
        "\n",
        "    # 每个epoch后生成一些图像\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(64, latent_dim)\n",
        "        gen_imgs = generator(z)\n",
        "        gen_imgs = gen_imgs.view(gen_imgs.size(0), 1, 28, 28)  # 批量图像应为 [batch_size, channels, height, width]\n",
        "\n",
        "        # 将图像保存为单个文件\n",
        "        grid_img = vutils.make_grid(gen_imgs, nrow=8, normalize=True, scale_each=True)\n",
        "        # save_path = 'generated_images' + str(epoch) + '.png'\n",
        "        save_path = f'generated_images_{epoch}.png'\n",
        "        vutils.save_image(grid_img, save_path)\n",
        "\n",
        "        print(f'生成的图像已保存到 {save_path}')\n",
        "\n",
        "    # 保存模型权重\n",
        "    torch.save(generator.state_dict(), f\"/content/drive/MyDrive/generator_epoch.pth\")\n",
        "    torch.save(discriminator.state_dict(), f\"/content/drive/MyDrive/discriminator_epoch.pth\")\n",
        "\n",
        "    print(f\"模型权重已保存到 '/content/drive/MyDrive/generator_epoch.pth' 和 '/content/drive/MyDrive/discriminator_epoch.pth'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-MXY_5ryZMd"
      },
      "source": [
        "##验证图片"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_xarCFCe92y"
      },
      "outputs": [],
      "source": [
        "#@title 显示文件夹png后缀的图片\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# 定义图片文件夹路径\n",
        "folder_path = '/content'  # 替换为你的文件夹路径\n",
        "\n",
        "# 获取文件夹中所有以.png结尾的文件名\n",
        "png_files = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n",
        "\n",
        "# 遍历并显示每个PNG图片\n",
        "for file_name in png_files:\n",
        "    img_path = os.path.join(folder_path, file_name)\n",
        "    img = mpimg.imread(img_path)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')  # 隐藏坐标轴\n",
        "    plt.title(file_name)  # 可选：显示文件名作为标题\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoWKJwBs17Xv"
      },
      "source": [
        "##加解密生成图像"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_cQsYrN1-1J"
      },
      "outputs": [],
      "source": [
        "#@title 噪音生成图片模型\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(100, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, latent_dim, kernel_size=3, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(latent_dim),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim, 512, kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            SelfAttention(512),  # Self-attention layer\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            SelfAttention(256),  # Self-attention layer\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.ConvTranspose2d(128, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = z.view(z.size(0), latent_dim, 1, 1)  # Adjust input shape\n",
        "        return self.model(z)\n",
        "\n",
        "# Combine encoder and decoder into a single model\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, img):\n",
        "        latent_vec = self.encoder(img)\n",
        "        reconstructed_img = self.decoder(latent_vec)\n",
        "        return reconstructed_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR7V84GD2oWS"
      },
      "outputs": [],
      "source": [
        "#@title 加载权重进行训练\n",
        "\n",
        "\n",
        "import math\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_embed, in_proj_bias=True, out_proj_bias=True, n_heads=8):\n",
        "        super().__init__()\n",
        "        n_heads = 8\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_embed // n_heads\n",
        "        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)\n",
        "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (Batch_Size, Channels, Height, Width)\n",
        "        batch_size, channels, height, width = x.shape\n",
        "\n",
        "        # Reshape to (Batch_Size, Height * Width, Channels)\n",
        "        x = x.view(batch_size, channels, height * width).transpose(1, 2)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim)\n",
        "        seq_len = x.size(1)\n",
        "        d_embed = x.size(2)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim * 3) -> 3 tensor of shape (Batch_Size, Seq_Len, Dim)\n",
        "        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
        "\n",
        "        # (Batch_Size, Seq_Len, Dim) -> (Batch_Size, H, Seq_Len, Dim / H)\n",
        "        q = q.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        k = k.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        v = v.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "        # Self-attention calculation\n",
        "        weight = q @ k.transpose(-2, -1) / math.sqrt(self.d_head)\n",
        "        weight = F.softmax(weight, dim=-1)\n",
        "        output = weight @ v\n",
        "\n",
        "        # Reshape back to (Batch_Size, Seq_Len, Dim)\n",
        "        output = output.transpose(1, 2).reshape(batch_size, seq_len, d_embed)\n",
        "\n",
        "        # Apply the final linear projection\n",
        "        output = self.out_proj(output)\n",
        "\n",
        "        # Reshape back to (Batch_Size, Channels, Height, Width)\n",
        "        output = output.transpose(1, 2).view(batch_size, channels, height, width)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            SelfAttention(64),  # 加入自注意力层\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, kernel_size=3, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        validity = self.model(img)\n",
        "        # print(\"shape:\", validity.shape)\n",
        "        return validity.view(validity.size(0), -1)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 100, kernel_size=3, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(latent_dim),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.model(img)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim, 512, kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            SelfAttention(512),  # Self-attention layer\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            SelfAttention(256),  # Self-attention layer\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.ConvTranspose2d(128, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = z.view(64, 100, 2, 2)  # Adjust input shape\n",
        "        return self.model(z)\n",
        "\n",
        "# Combine encoder and decoder into a single model\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, img):\n",
        "        latent_vec = self.encoder(img)\n",
        "        # print(\"latent_vec:\", latent_vec.shape)\n",
        "\n",
        "        reconstructed_img = self.decoder(latent_vec)\n",
        "        # print(\"reconstructed_img:\", reconstructed_img.shape)\n",
        "        return reconstructed_img\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# 初始化判别器\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Initialize the autoencoder\n",
        "autoencoder = Autoencoder()\n",
        "\n",
        "# 检查是否可以使用GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 将模型移动到GPU\n",
        "discriminator = discriminator.to(device)\n",
        "autoencoder = autoencoder.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# Check if there are existing weights to load\n",
        "encoder_path = '/content/drive/MyDrive/encoder.pth'\n",
        "decoder_path = '/content/drive/MyDrive/decoder.pth'\n",
        "autoencoder_path = 'autoencoder.pth'\n",
        "\n",
        "if os.path.exists(encoder_path) and os.path.exists(decoder_path):\n",
        "    autoencoder.encoder.load_state_dict(torch.load(encoder_path))\n",
        "    autoencoder.decoder.load_state_dict(torch.load(decoder_path))\n",
        "    print(\"Loaded encoder and decoder weights.\")\n",
        "elif os.path.exists(autoencoder_path):\n",
        "    autoencoder.load_state_dict(torch.load(autoencoder_path))\n",
        "    print(\"Loaded full autoencoder weights.\")\n",
        "else:\n",
        "    print(\"No saved weights found. Starting training from scratch.\")\n",
        "\n",
        "# 检查判别器权重文件是否存在\n",
        "discriminator_weights_path = '/content/drive/MyDrive/discriminator_epoch.pth'\n",
        "if os.path.exists(discriminator_weights_path):\n",
        "    discriminator.load_state_dict(torch.load(discriminator_weights_path))\n",
        "    print(f\"加载判别器的权重成功：{discriminator_weights_path}\")\n",
        "else:\n",
        "    print(f\"判别器权重文件未找到：{discriminator_weights_path}\")\n",
        "\n",
        "\n",
        "\n",
        "# 损失函数\n",
        "adversarial_loss = nn.BCELoss()\n",
        "\n",
        "# 优化器\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "optimizer_G = optim.Adam(autoencoder.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (imgs, _) in enumerate(dataloader):\n",
        "        # 标签\n",
        "        # valid = torch.ones(imgs.size(0), 1)\n",
        "        # fake = torch.zeros(imgs.size(0), 1)\n",
        "\n",
        "        valid = torch.full((imgs.size(0), 1), 0.9).to(device)  # 真实标签设置为 0.9\n",
        "        fake = torch.full((imgs.size(0), 1), 0.1).to(device)  # 生成标签设置为 0.1\n",
        "\n",
        "\n",
        "        # -----------------\n",
        "        # 训练生成器\n",
        "        # -----------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # z = torch.randn(imgs.size(0), latent_dim)\n",
        "        z = torch.randn(imgs.size(0), 1, 28, 28).to(device)  # Adjust to 4D input\n",
        "        gen_imgs = autoencoder(z)\n",
        "        print(\"gen_imgs1：\", gen_imgs.shape)\n",
        "        # gen_imgs = autoencoder(z)\n",
        "\n",
        "        # print(\"valid shape\", valid.shape)\n",
        "        # print(\"discriminator(gen_imgs) shape\", discriminator(gen_imgs).shape)\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ---------------------\n",
        "        # 训练判别器\n",
        "        # ---------------------\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        real_loss = adversarial_loss(discriminator(imgs.to(device)), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # 打印损失\n",
        "        print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader)}] [D loss: {d_loss.item()}] [G loss: {g_loss.item()}]\")\n",
        "\n",
        "        if i > 930:\n",
        "          break\n",
        "\n",
        "    # 每个epoch后生成一些图像\n",
        "    with torch.no_grad():\n",
        "        # z = torch.randn(64, 1, 28, 28)\n",
        "        # gen_imgs = autoencoder(z)\n",
        "        z = torch.randn(64, 1, 28, 28).to(device)  # Adjust to 4D input\n",
        "        gen_imgs = autoencoder(z)\n",
        "        print(\"gen_imgs：\", gen_imgs.shape)\n",
        "        gen_imgs = gen_imgs.view(gen_imgs.size(0), 1, 28, 28)  # 批量图像应为 [batch_size, channels, height, width]\n",
        "\n",
        "        # 将图像保存为单个文件\n",
        "        grid_img = vutils.make_grid(gen_imgs, nrow=8, normalize=True, scale_each=True)\n",
        "        # save_path = 'generated_images' + str(epoch) + '.png'\n",
        "        save_path = f'generated_images_{epoch}.png'\n",
        "        vutils.save_image(grid_img, save_path)\n",
        "\n",
        "        print(f'生成的图像已保存到 {save_path}')\n",
        "\n",
        "    # 保存模型权重\n",
        "    # After training, save the weights\n",
        "    torch.save(autoencoder.encoder.state_dict(), '/content/drive/MyDrive/encoder.pth')\n",
        "    torch.save(autoencoder.decoder.state_dict(), '/content/drive/MyDrive/decoder.pth')\n",
        "    # Or save the entire model's state\n",
        "    torch.save(autoencoder.state_dict(), '/content/drive/MyDrive/autoencoder.pth')\n",
        "    torch.save(discriminator.state_dict(), f\"/content/drive/MyDrive/discriminator_epoch.pth\")\n",
        "\n",
        "    print(f\"模型权重已保存到 '/content/drive/MyDrive/encoder.pth' '/content/drive/MyDrive/decoder.pth' '/content/drive/MyDrive/autoencoder.pth' 和 '/content/drive/MyDrive/discriminator_epoch.pth'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC_q_gnQXNcb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UVCY-MNXNcb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}