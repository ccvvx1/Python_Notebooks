{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title 链接Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4mg38E-Oae38",
        "outputId": "2a538cf9-8e9a-49d8-8c7f-14ebdc403413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/workspace.zip"
      ],
      "metadata": {
        "id": "YJKreKKabBR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 加密部分"
      ],
      "metadata": {
        "id": "Uh8FBiFHNSax"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qMIdFFbuWoj"
      },
      "outputs": [],
      "source": [
        "#@title 加密encoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_ch, e_ch, opts=None, use_fp16=False):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.e_ch = e_ch\n",
        "        self.opts = opts if opts is not None else {}\n",
        "        self.use_fp16 = use_fp16\n",
        "\n",
        "        if 't' in self.opts:\n",
        "            self.down1 = Downscale(self.in_ch, self.e_ch, kernel_size=5)\n",
        "            self.res1 = ResidualBlock(self.e_ch)\n",
        "            self.down2 = Downscale(self.e_ch, self.e_ch * 2, kernel_size=5)\n",
        "            self.down3 = Downscale(self.e_ch * 2, self.e_ch * 4, kernel_size=5)\n",
        "            self.down4 = Downscale(self.e_ch * 4, self.e_ch * 8, kernel_size=5)\n",
        "            self.down5 = Downscale(self.e_ch * 8, selfa.e_ch * 8, kernel_size=5)\n",
        "            self.res5 = ResidualBlock(self.e_ch * 8)\n",
        "        else:\n",
        "            n_downscales = 4 if 't' not in self.opts else 5\n",
        "            self.down1 = DownscaleBlock(self.in_ch, self.e_ch, n_downscales=n_downscales, kernel_size=5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_fp16:\n",
        "            x = x.half()\n",
        "\n",
        "        if 't' in self.opts:\n",
        "            x = self.down1(x)\n",
        "            x = self.res1(x)\n",
        "            x = self.down2(x)\n",
        "            x = self.down3(x)\n",
        "            x = self.down4(x)\n",
        "            x = self.down5(x)\n",
        "            x = self.res5(x)\n",
        "        else:\n",
        "            x = self.down1(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        if 'u' in self.opts:\n",
        "            x = F.normalize(x, p=2, dim=-1)\n",
        "\n",
        "        if self.use_fp16:\n",
        "            x = x.float()\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_out_res(self, res):\n",
        "        return res // (2**4 if 't' not in self.opts else 2**5)\n",
        "\n",
        "    def get_out_ch(self):\n",
        "        return self.e_ch * 8\n",
        "\n",
        "# 下面是 Downscale 和 ResidualBlock 的示例实现（需要根据你的情况具体实现）\n",
        "class Downscale(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=5):\n",
        "        super(Downscale, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, stride=2, padding=kernel_size//2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.relu(self.conv(x))\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, ch):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(ch, ch, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(ch, ch, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.conv2(x)\n",
        "        return F.relu(x + residual)\n",
        "\n",
        "class DownscaleBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, n_downscales, kernel_size=5):\n",
        "        super(DownscaleBlock, self).__init__()\n",
        "        layers = []\n",
        "        for _ in range(n_downscales):\n",
        "            layers.append(Downscale(in_ch, out_ch, kernel_size))\n",
        "            in_ch = out_ch\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 保存权重\n",
        "\n",
        "# Example instantiation\n",
        "model = Encoder(in_ch=3, e_ch=64, opts={'t': False}, use_fp16=False)\n",
        "# Save model weights\n",
        "torch.save(model.state_dict(), 'encoder_weights.pth')\n"
      ],
      "metadata": {
        "id": "YYWpX1BZBl0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 解密部分"
      ],
      "metadata": {
        "id": "cvkA7WT3NYqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 解密decoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Upscale(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3):\n",
        "        super(Upscale, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, padding=kernel_size // 2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upsample(x)\n",
        "        return F.relu(self.conv(x))\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_ch, d_ch, d_mask_ch):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.upscale0 = Upscale(in_ch, d_ch * 8, kernel_size=3)\n",
        "        self.upscale1 = Upscale(d_ch * 8, d_ch * 4, kernel_size=3)\n",
        "        self.upscale2 = Upscale(d_ch * 4, d_ch * 2, kernel_size=3)\n",
        "        self.res0 = ResidualBlock(d_ch * 8, kernel_size=3)\n",
        "        self.res1 = ResidualBlock(d_ch * 4, kernel_size=3)\n",
        "        self.res2 = ResidualBlock(d_ch * 2, kernel_size=3)\n",
        "\n",
        "        self.upscalem0 = Upscale(in_ch, d_mask_ch * 8, kernel_size=3)\n",
        "        self.upscalem1 = Upscale(d_mask_ch * 8, d_mask_ch * 4, kernel_size=3)\n",
        "        self.upscalem2 = Upscale(d_mask_ch * 4, d_mask_ch * 2, kernel_size=3)\n",
        "\n",
        "        self.out_conv = nn.Conv2d(d_ch * 2, 3, kernel_size=1)\n",
        "        self.out_conv1 = nn.Conv2d(d_ch * 2, 3, kernel_size=3, padding=1)\n",
        "        self.out_conv2 = nn.Conv2d(d_ch * 2, 3, kernel_size=3, padding=1)\n",
        "        self.out_conv3 = nn.Conv2d(d_ch * 2, 3, kernel_size=3, padding=1)\n",
        "        self.upscalem3 = Upscale(d_mask_ch * 2, d_mask_ch * 1, kernel_size=3)\n",
        "        self.out_convm = nn.Conv2d(d_mask_ch * 1, 1, kernel_size=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # Decoder path\n",
        "        x = self.upscale0(z)\n",
        "        x = self.res0(x)\n",
        "        x = self.upscale1(x)\n",
        "        x = self.res1(x)\n",
        "        x = self.upscale2(x)\n",
        "        x = self.res2(x)\n",
        "\n",
        "        # Combine the output of multiple conv layers and apply pixel shuffle\n",
        "        x = torch.cat([\n",
        "            self.out_conv(x),\n",
        "            self.out_conv1(x),\n",
        "            self.out_conv2(x),\n",
        "            self.out_conv3(x)\n",
        "        ], dim=1)\n",
        "\n",
        "        x = F.pixel_shuffle(x, upscale_factor=2)  # Equivalent to depth_to_space\n",
        "\n",
        "        # Mask path\n",
        "        m = self.upscalem0(z)\n",
        "        m = self.upscalem1(m)\n",
        "        m = self.upscalem2(m)\n",
        "        m = self.upscalem3(m)\n",
        "        m = torch.sigmoid(self.out_convm(m))\n",
        "\n",
        "        return x, m\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, ch, kernel_size=3):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(ch, ch, kernel_size=kernel_size, padding=kernel_size//2)\n",
        "        self.conv2 = nn.Conv2d(ch, ch, kernel_size=kernel_size, padding=kernel_size//2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.conv2(x)\n",
        "        return F.relu(x + residual)\n"
      ],
      "metadata": {
        "id": "XQ55BNBLKZoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 保存权重\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the model\n",
        "in_ch = 3\n",
        "d_ch = 64\n",
        "d_mask_ch = 32\n",
        "decoder = Decoder(in_ch, d_ch, d_mask_ch)\n",
        "\n",
        "# Create a dummy input tensor (e.g., batch of images with 3 channels and 64x64 size)\n",
        "dummy_input = torch.randn(1, in_ch, 64, 64)  # Batch size of 1, 3 channels, 64x64\n",
        "\n",
        "# Forward pass\n",
        "x, m = decoder(dummy_input)\n",
        "\n",
        "print(x.shape)  # Output shape of the main decoder path\n",
        "print(m.shape)  # Output shape of the mask path\n"
      ],
      "metadata": {
        "id": "slfHdqx2NytA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 保存权重\n",
        "\n",
        "# Save the model weights\n",
        "torch.save(decoder.state_dict(), 'decoder_weights.pth')\n"
      ],
      "metadata": {
        "id": "UYafdLzPQ-2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 处理inner"
      ],
      "metadata": {
        "id": "A6oQsOybWa_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title inner模型\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Upscale(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3):\n",
        "        super(Upscale, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, padding=kernel_size // 2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upsample(x)\n",
        "        return F.relu(self.conv(x))\n",
        "\n",
        "class Inter(nn.Module):\n",
        "    def __init__(self, in_ch, ae_ch, ae_out_ch, lowest_dense_res, opts=None, use_fp16=False):\n",
        "        super(Inter, self).__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.ae_ch = ae_ch\n",
        "        self.ae_out_ch = ae_out_ch\n",
        "        self.lowest_dense_res = lowest_dense_res\n",
        "        self.opts = opts if opts is not None else {}\n",
        "        self.use_fp16 = use_fp16\n",
        "\n",
        "        self.dense1 = nn.Linear(in_ch, ae_ch)\n",
        "        self.dense2 = nn.Linear(ae_ch, lowest_dense_res * lowest_dense_res * ae_out_ch)\n",
        "\n",
        "        if 't' not in self.opts:\n",
        "            self.upscale1 = Upscale(ae_out_ch, ae_out_ch)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        x = inp\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "\n",
        "        # Reshape the tensor to 4D (batch_size, channels, height, width)\n",
        "        x = x.view(-1, self.ae_out_ch, self.lowest_dense_res, self.lowest_dense_res)\n",
        "\n",
        "        if self.use_fp16:\n",
        "            x = x.half()\n",
        "\n",
        "        if 't' not in self.opts:\n",
        "            x = self.upscale1(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_out_res(self):\n",
        "        return self.lowest_dense_res * 2 if 't' not in self.opts else self.lowest_dense_res\n",
        "\n",
        "    def get_out_ch(self):\n",
        "        return self.ae_out_ch\n"
      ],
      "metadata": {
        "id": "4s3jWc1vWesb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 调试inner\n",
        "\n",
        "# Parameters\n",
        "in_ch = 256\n",
        "ae_ch = 128\n",
        "ae_out_ch = 64\n",
        "lowest_dense_res = 16\n",
        "opts = {}  # or {'t': True} to modify behavior\n",
        "use_fp16 = False\n",
        "\n",
        "# Create model\n",
        "inter = Inter(in_ch, ae_ch, ae_out_ch, lowest_dense_res, opts, use_fp16)\n",
        "\n",
        "# Dummy input\n",
        "dummy_input = torch.randn(1, in_ch)  # Batch size of 1, flattened input\n",
        "\n",
        "# Forward pass\n",
        "output = inter(dummy_input)\n",
        "\n",
        "print(output.shape)  # Output shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcfRxKJdWomV",
        "outputId": "3a2d6300-f448-449c-d9e0-f479962e1f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 保存权重\n",
        "\n",
        "# Save the model weights\n",
        "torch.save(inter.state_dict(), 'inter_weights.pth')\n"
      ],
      "metadata": {
        "id": "YcFY16QsW3Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 读取png文件"
      ],
      "metadata": {
        "id": "5zZEqTimbuIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
        "        if not self.image_files:\n",
        "            raise ValueError(f\"No PNG files found in directory {image_dir}\")\n",
        "        print(f\"Found {len(self.image_files)} PNG files.\")  # Debug line\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "# Define your transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Specify the path to your image directory\n",
        "image_dir = '/content/data_dst/aligned'\n",
        "\n",
        "# Create dataset and DataLoader\n",
        "dataset = CustomImageDataset(image_dir=image_dir, transform=transform)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "# Example usage\n",
        "for images in data_loader:\n",
        "    print(images.shape)  # Output the shape of the image batch\n"
      ],
      "metadata": {
        "id": "yti--lFhbxbm",
        "outputId": "f933e084-acb9-494c-9d13-39f3a9bda514",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 72 PNG files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 128, 128])\n",
            "torch.Size([32, 3, 128, 128])\n",
            "torch.Size([8, 3, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 计算loss"
      ],
      "metadata": {
        "id": "6n7RAI9FexaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 计算loss\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms import functional as TF\n",
        "\n",
        "def ssim(x, y, max_val=1.0, filter_size=11):\n",
        "    # Placeholder for SSIM calculation\n",
        "    # You need to replace this with the actual implementation of SSIM\n",
        "    return torch.ones_like(x)  # Dummy implementation\n",
        "\n",
        "# Assume gpu_target_src_masked_opt, gpu_pred_src_src_masked_opt, gpu_target_srcm, gpu_pred_src_srcm are tensors\n",
        "# and resolution is defined\n",
        "\n",
        "# Example tensors and resolution (Replace with actual tensors and value)\n",
        "gpu_target_src_masked_opt = torch.rand((batch_size, channels, height, width))\n",
        "gpu_pred_src_src_masked_opt = torch.rand((batch_size, channels, height, width))\n",
        "gpu_target_srcm = torch.rand((batch_size, channels, height, width))\n",
        "gpu_pred_src_srcm = torch.rand((batch_size, channels, height, width))\n",
        "resolution = 224  # Example resolution\n",
        "\n",
        "# SSIM calculation\n",
        "filter_size = int(resolution / 11.6)\n",
        "dssim = ssim(gpu_target_src_masked_opt, gpu_pred_src_src_masked_opt, max_val=1.0, filter_size=filter_size)\n",
        "\n",
        "# Compute loss\n",
        "gpu_src_loss = torch.mean(10 * dssim, dim=1)\n",
        "gpu_src_loss += torch.mean(10 * torch.square(gpu_target_src_masked_opt - gpu_pred_src_src_masked_opt), dim=[1, 2, 3])\n",
        "gpu_src_loss += torch.mean(10 * torch.square(gpu_target_srcm - gpu_pred_src_srcm), dim=[1, 2, 3])\n"
      ],
      "metadata": {
        "id": "B54LghjzewbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 训练"
      ],
      "metadata": {
        "id": "DSi7xPdafw4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 训练\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "encoder = Encoder(in_ch=3, e_ch=64, opts={'t': False})  # Adjust parameters as needed\n",
        "\n",
        "# Example optimizer\n",
        "optimizer = optim.Adam(encoder.parameters(), lr=1e-3)\n",
        "num_epochs = 10\n",
        "# 检查是否可以使用GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Parameters\n",
        "in_ch = 256\n",
        "ae_ch = 128\n",
        "ae_out_ch = 64\n",
        "lowest_dense_res = 16\n",
        "opts = {}  # or {'t': True} to modify behavior\n",
        "use_fp16 = False\n",
        "\n",
        "# Create model\n",
        "inter = Inter(in_ch, ae_ch, ae_out_ch, lowest_dense_res, opts, use_fp16)\n",
        "\n",
        "# Initialize the model\n",
        "in_ch = 3\n",
        "d_ch = 64\n",
        "d_mask_ch = 32\n",
        "decoder = Decoder(in_ch, d_ch, d_mask_ch)\n",
        "\n",
        "# Create a dummy input tensor (e.g., batch of images with 3 channels and 64x64 size)\n",
        "# dummy_input = torch.randn(1, in_ch, 64, 64)  # Batch size of 1, 3 channels, 64x64\n",
        "\n",
        "# # Forward pass\n",
        "# x, m = decoder(dummy_input)\n",
        "\n",
        "# print(x.shape)  # Output shape of the main decoder path\n",
        "# print(m.shape)  # Output shape of the mask path\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in data_loader:\n",
        "        images = batch.to(device)  # Move images to the appropriate device (e.g., GPU)\n",
        "\n",
        "        # Forward pass\n",
        "        encoder_output = encoder(images)\n",
        "\n",
        "        # Forward pass\n",
        "        # inter_output = inter(encoder_output)\n",
        "        x, m = decoder(encoder_output)\n",
        "\n",
        "        # Example loss computation (Replace with your actual loss function)\n",
        "        loss = some_loss_function(encoder_output, target)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "X1eSnV0Zf3LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 通过keras的summary改成pytorch"
      ],
      "metadata": {
        "id": "O9vWuNJGfPHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 用pytorch写encoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = torch.relu(self.conv4(x))\n",
        "        return x\n",
        "\n",
        "# 创建模型实例\n",
        "model = Encoder()\n",
        "\n",
        "# 打印模型的概要信息\n",
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 128, 128))  # 这里假设输入图像大小为 224x224, 可以根据需要调整\n"
      ],
      "metadata": {
        "id": "WirI12zofVHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 改成inter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "\n",
        "class InterModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(InterModel, self).__init__()\n",
        "        # 定义卷积层\n",
        "        self.conv1 = nn.Conv2d(in_channels=128, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # 定义全连接层\n",
        "        # 计算全连接层的输入特征数需要知道卷积输出的特征图尺寸\n",
        "        # 这里假设卷积层的输出尺寸是 (batch_size, 512, 28, 28)\n",
        "        # 输入特征数 = 512 * 28 * 28 = 411,648\n",
        "        self.fc1 = nn.Linear(512 * 28 * 28, 2359424)  # 第一层全连接\n",
        "        self.fc2 = nn.Linear(2359424, 148608)          # 第二层全连接\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 创建模型实例\n",
        "model = InterModel()\n",
        "\n",
        "# 打印模型的概要信息\n",
        "summary(model, input_size=(128, 28, 28))  # 输入形状为 (128, 28, 28)，假设这里的特征图尺寸是 28x28\n",
        "\n",
        "# # 计算总参数数目\n",
        "# total_params = sum(p.numel() for p in model.parameters())\n",
        "# print(f\"Total params count: {total_params}\")\n"
      ],
      "metadata": {
        "id": "ZFxkkWLugqZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 改成decoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DecoderSrc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DecoderSrc, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=128, out_channels=2048, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.conv8 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.conv9 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.conv10 = nn.Conv2d(in_channels=128, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.conv11 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.conv12 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.conv13 = nn.Conv2d(in_channels=128, out_channels=3, kernel_size=1)\n",
        "        self.conv14 = nn.Conv2d(in_channels=128, out_channels=3, kernel_size=1)\n",
        "        self.conv15 = nn.Conv2d(in_channels=128, out_channels=3, kernel_size=1)\n",
        "        self.conv16 = nn.Conv2d(in_channels=128, out_channels=3, kernel_size=1)\n",
        "        self.conv17 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv18 = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7(x)\n",
        "        x = self.conv8(x)\n",
        "        x = self.conv9(x)\n",
        "        x = self.conv10(x)\n",
        "        x = self.conv11(x)\n",
        "        x = self.conv12(x)\n",
        "        x = self.conv13(x)\n",
        "        x = self.conv14(x)\n",
        "        x = self.conv15(x)\n",
        "        x = self.conv16(x)\n",
        "        x = self.conv17(x)\n",
        "        x = self.conv18(x)\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "model = DecoderSrc()\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "PKGyfyj5h2HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 训练\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "encoder = Encoder()  # Adjust parameters as needed\n",
        "\n",
        "# Example optimizer\n",
        "optimizer = optim.Adam(encoder.parameters(), lr=1e-3)\n",
        "num_epochs = 10\n",
        "# 检查是否可以使用GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Parameters\n",
        "in_ch = 256\n",
        "ae_ch = 128\n",
        "ae_out_ch = 64\n",
        "lowest_dense_res = 16\n",
        "opts = {}  # or {'t': True} to modify behavior\n",
        "use_fp16 = False\n",
        "\n",
        "# Create model\n",
        "# inter = Inter()\n",
        "\n",
        "# Initialize the model\n",
        "in_ch = 3\n",
        "d_ch = 64\n",
        "d_mask_ch = 32\n",
        "decoder = DecoderSrc()\n",
        "\n",
        "# Create a dummy input tensor (e.g., batch of images with 3 channels and 64x64 size)\n",
        "# dummy_input = torch.randn(1, in_ch, 64, 64)  # Batch size of 1, 3 channels, 64x64\n",
        "\n",
        "# # Forward pass\n",
        "# x, m = decoder(dummy_input)\n",
        "\n",
        "# print(x.shape)  # Output shape of the main decoder path\n",
        "# print(m.shape)  # Output shape of the mask path\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in data_loader:\n",
        "        images = batch.to(device)  # Move images to the appropriate device (e.g., GPU)\n",
        "\n",
        "        # Forward pass\n",
        "        encoder_output = encoder(images)\n",
        "        print(\"encoder shape:\", encoder_output.shape)\n",
        "\n",
        "        # Forward pass\n",
        "        # inter_output = inter(encoder_output)\n",
        "        x, m = decoder(encoder_output)\n",
        "\n",
        "        # Example loss computation (Replace with your actual loss function)\n",
        "        loss = some_loss_function(encoder_output, target)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")"
      ],
      "metadata": {
        "id": "9g4Q9s_YkzhY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}