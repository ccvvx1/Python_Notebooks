{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6anRp8jeuTf"
   },
   "outputs": [],
   "source": [
    "\n",
    "#@title 链接Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtGaXeHCtsnG"
   },
   "outputs": [],
   "source": [
    "# !mkdir /content/drive/MyDrive/ftTrainDataDetail\n",
    "!rm -rf /content/drive/MyDrive/ftTrainDataDetail/*\n",
    "!unzip -o /home/tmw/shared/ftTrainData.zip -d /content/drive/MyDrive/ftTrainDataDetail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KPXkrGlTxOV"
   },
   "source": [
    "## 获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "CbHXI5I4hp_o"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "#@title 拼接数据\n",
    "\n",
    "# filepath = '/kaggle/input/stock-time-series-20050101-to-20171231/AMZN_2006-01-01_to_2018-01-01.csv'\n",
    "# filepath = '/kaggle/input/amazon.csv'\n",
    "import pandas as pd\n",
    "\n",
    "# 获取/content/hello目录下所有的CSV文件\n",
    "file_paths = [os.path.join('/content/drive/MyDrive/ftTrainDataDetail', f) for f in os.listdir('/content/drive/MyDrive/ftTrainDataDetail') if f.endswith('.csv')]\n",
    "\n",
    "dataframes = []\n",
    "tgtLength = 500\n",
    "\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path, encoding='latin1')\n",
    "    df = df[::-1]  # 反转 DataFrame 的顺序\n",
    "\n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "    # 检查文件大小\n",
    "    if len(df) < tgtLength:\n",
    "        # 计算需要重复的行数\n",
    "        num_rows_to_add = tgtLength - len(df)\n",
    "        last_row = df.iloc[-1:]  # 获取最后一行\n",
    "        # 重复最后一行\n",
    "        repeated_rows = pd.concat([last_row] * num_rows_to_add, ignore_index=True)\n",
    "        df = pd.concat([df, repeated_rows], ignore_index=True)\n",
    "        # 创建补充的行，填充为零\n",
    "        # zero_rows = pd.DataFrame(0, index=range(num_rows_to_add), columns=df.columns)\n",
    "        # df = pd.concat([df, zero_rows], ignore_index=True)\n",
    "    else:\n",
    "        df = df.iloc[:tgtLength]\n",
    "\n",
    "    dataframes.append(df)\n",
    "\n",
    "# 合并所有数据\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "# 保存为 CSV 文件\n",
    "data.to_csv('hello.csv', index=False)\n",
    "\n",
    "#@title 数据去除特殊符号\n",
    "\n",
    "dataMain = data[['Main']]\n",
    "dataEven = data[['Even']]\n",
    "dataLoss = data[['Loss']]\n",
    "dataResult = data[['Result']]\n",
    "\n",
    "dataMain['Main'] = dataMain['Main'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
    "dataEven['Even'] = dataEven['Even'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
    "dataLoss['Loss'] = dataLoss['Loss'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
    "\n",
    "\n",
    "# # 检查非数字字符串\n",
    "# def clean_column(column):\n",
    "#     # 移除特定字符\n",
    "#     column = column.str.replace('↑', '').str.replace('↓', '')\n",
    "#     # 检查无法转换的值\n",
    "#     invalid_values = column[~column.str.replace('.', '', regex=False).str.isnumeric()]\n",
    "#     if not invalid_values.empty:\n",
    "#         print(\"无效值:\", invalid_values)\n",
    "#     # 转换为 float\n",
    "#     return column.astype(float, errors='coerce')\n",
    "\n",
    "# # 清理并转换\n",
    "# dataMain['Main'] = clean_column(dataMain['Main'])\n",
    "# dataMain['Even'] = clean_column(dataMain['Even'])\n",
    "# dataMain['Loss'] = clean_column(dataMain['Loss'])\n",
    "\n",
    "\n",
    "#@title 数据归一化\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scalerMain = MinMaxScaler(feature_range=(-1, 1))\n",
    "scalerEven = MinMaxScaler(feature_range=(-1, 1))\n",
    "scalerLoss = MinMaxScaler(feature_range=(-1, 1))\n",
    "scalerResult = MinMaxScaler(feature_range=(-1, 1))\n",
    "dataMain['Main'] = scalerMain.fit_transform(dataMain['Main'].values.reshape(-1,1))\n",
    "dataEven['Even'] = scalerEven.fit_transform(dataEven['Even'].values.reshape(-1,1))\n",
    "dataLoss['Loss'] = scalerLoss.fit_transform(dataLoss['Loss'].values.reshape(-1,1))\n",
    "dataResult['Result'] = dataResult['Result']\n",
    "print(dataMain['Main'] .shape)\n",
    "dataMain['Main']\n",
    "\n",
    "# def scale_in_chunks(data, chunk_size=500):\n",
    "#     scalers = []\n",
    "#     scaled_data = []\n",
    "\n",
    "#     # 将数据分成若干块\n",
    "#     data_chunks = np.array_split(data, np.arange(chunk_size, len(data), chunk_size))\n",
    "\n",
    "#     for chunk in data_chunks:\n",
    "#         scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "#         scaled_chunk = scaler.fit_transform(chunk.values.reshape(-1, 1))\n",
    "#         scalers.append(scaler)  # 如果需要在后续反向转换时使用\n",
    "#         scaled_data.append(scaled_chunk)\n",
    "\n",
    "#     return np.vstack(scaled_data), scalers\n",
    "\n",
    "# # 对每个数据集进行分块缩放\n",
    "# dataMain['Main'], scalers_main = scale_in_chunks(dataMain['Main'])\n",
    "# dataEven['Even'], scalers_even = scale_in_chunks(dataEven['Even'])\n",
    "# dataLoss['Loss'], scalers_loss = scale_in_chunks(dataLoss['Loss'])\n",
    "# print(dataMain['Main'], dataEven['Even'], dataLoss['Loss'])\n",
    "# dataResult['Result'] 保持原样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ehyV_uihp_s"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# sns.set_style(\"darkgrid\")\n",
    "# plt.figure(figsize = (15,9))\n",
    "# plt.plot(data[['Main']])\n",
    "# plt.plot(data[['Even']])\n",
    "# plt.plot(data[['Loss']])\n",
    "# plt.xticks(range(0,data.shape[0],500),data['Date'].loc[::500],rotation=45)\n",
    "# plt.title(\"Amazon Stock Price\",fontsize=18, fontweight='bold')\n",
    "# plt.xlabel('Date',fontsize=18)\n",
    "# plt.ylabel('Close Price (USD)',fontsize=18)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrjH2cFEhp_s"
   },
   "source": [
    "## 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fx4S1tcNhp_x"
   },
   "outputs": [],
   "source": [
    "#@title 训练模型\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_dim = 1\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "output_cls_dim = 3\n",
    "num_epochs = 100\n",
    "batch_size = 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def split_data(dataMain, dataEven, dataLoss, dataResult, lookback, batch_size = 20):\n",
    "    data_raw_main = dataMain.to_numpy()\n",
    "    data_raw_even = dataEven.to_numpy()\n",
    "    data_raw_loss = dataLoss.to_numpy()\n",
    "    data_raw_result = dataResult.to_numpy()\n",
    "    data_main = []\n",
    "    data_even = []\n",
    "    data_loss = []\n",
    "    data_result = []\n",
    "\n",
    "    # create all possible sequences of length lookback\n",
    "    for index in range(len(data_raw_main) - lookback):\n",
    "        data_main.append(data_raw_main[index: index + lookback])\n",
    "    for index in range(len(data_raw_even) - lookback):\n",
    "        data_even.append(data_raw_even[index: index + lookback])\n",
    "    for index in range(len(data_raw_loss) - lookback):\n",
    "        data_loss.append(data_raw_loss[index: index + lookback])\n",
    "    for index in range(len(data_raw_result) - lookback):\n",
    "        data_result.append(data_raw_result[index: index + lookback])\n",
    "\n",
    "    data_main = np.array(data_main)\n",
    "    data_even = np.array(data_even)\n",
    "    data_loss = np.array(data_loss)\n",
    "    data_result = np.array(data_result)\n",
    "\n",
    "    test_set_size = int(np.round(0.2 * data_main.shape[0]))\n",
    "    train_set_size = data_main.shape[0]\n",
    "\n",
    "    # 将训练数据切割成多个批次\n",
    "    num_batches = train_set_size // batch_size\n",
    "    print(\"num_batches: \", num_batches, \" data_main.shape[0]: \", data_main.shape[0])\n",
    "\n",
    "\n",
    "    x_train = {\n",
    "        'main': np.array_split(data_main[:train_set_size, :-1, :], num_batches),\n",
    "        'even': np.array_split(data_even[:train_set_size, :-1, :], num_batches),\n",
    "        'loss': np.array_split(data_loss[:train_set_size, :-1, :], num_batches),\n",
    "        'result': np.array_split(data_result[:train_set_size, :-1, :], num_batches)\n",
    "    }\n",
    "    y_train = {\n",
    "        'main': np.array_split(data_main[:train_set_size, -1, :], num_batches),\n",
    "        'even': np.array_split(data_even[:train_set_size, -1, :], num_batches),\n",
    "        'loss': np.array_split(data_loss[:train_set_size, -1, :], num_batches),\n",
    "        'result': np.array_split(data_result[:train_set_size, -1, :], num_batches)\n",
    "    }\n",
    "\n",
    "    x_test = {\n",
    "        'main': np.array_split(data_main[train_set_size:, :-1, :], num_batches),\n",
    "        'even': np.array_split(data_even[train_set_size:, :-1, :], num_batches),\n",
    "        'loss': np.array_split(data_loss[train_set_size:, :-1, :], num_batches),\n",
    "        'result': np.array_split(data_result[train_set_size:, :-1, :], num_batches)\n",
    "    }\n",
    "    y_test = {\n",
    "        'main': np.array_split(data_main[train_set_size:, -1, :], num_batches),\n",
    "        'even': np.array_split(data_even[train_set_size:, -1, :], num_batches),\n",
    "        'loss': np.array_split(data_loss[train_set_size:, -1, :], num_batches),\n",
    "        'result': np.array_split(data_result[train_set_size:, -1, :], num_batches)\n",
    "    }\n",
    "    # print(\"x_train['main']: \", x_train['main'][1])\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "\n",
    "lookback = 10 # choose sequence length\n",
    "x_train, y_train, x_test, y_test = split_data(dataMain, dataEven, dataLoss, dataResult, lookback, 495)\n",
    "# x_train_batches, y_train_batches, x_test, y_test = split_data(price, lookback=20, batch_size=32)\n",
    "\n",
    "# # 打印批次的数量和每个批次的形状\n",
    "# print('Number of training batches:', len(x_train_batches))\n",
    "# if len(x_train_batches) > 0:\n",
    "#     print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
    "#     print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
    "\n",
    "# print('x_test.shape = ', x_test.shape)\n",
    "# print('y_test.shape = ', y_test.shape)\n",
    "\n",
    "# 打印批次的数量和每个批次的形状\n",
    "print('Number of training batches:', len(x_train['main']))\n",
    "if len(x_train['main']) > 0:\n",
    "    # print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
    "    # print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
    "\n",
    "    print('x_train.shape = ',x_train['main'][0].shape)\n",
    "    print('y_train.shape = ',y_train['main'][0].shape)\n",
    "    print('x_test.shape = ',x_test['main'][0].shape)\n",
    "    print('y_test.shape = ',y_test['main'][0].shape)\n",
    "    print('x_result.shape = ',y_train['result'][0].shape)\n",
    "\n",
    "# print(\"x_train: \", x_train)\n",
    "# print(\"y_train: \", y_train)\n",
    "\n",
    "# x_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train_batches]\n",
    "# y_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train_batches]\n",
    "import torch\n",
    "\n",
    "# 转换为字典\n",
    "train_tensors = {\n",
    "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['main']],\n",
    "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['even']],\n",
    "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['loss']],\n",
    "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['result']],\n",
    "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['main']],\n",
    "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['even']],\n",
    "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['loss']],\n",
    "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['result']]\n",
    "}\n",
    "\n",
    "test_tensors = {\n",
    "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['main']],\n",
    "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['even']],\n",
    "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['loss']],\n",
    "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['result']],\n",
    "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['main']],\n",
    "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['even']],\n",
    "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['loss']],\n",
    "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['result']]\n",
    "}\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fcCls = nn.Linear(hidden_dim, output_cls_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out1 = self.fc(out[:, -1, :])\n",
    "\n",
    "        # 通过全连接层\n",
    "        outCls = self.fcCls(out[:, -1, :])\n",
    "        # 应用 softmax 激活\n",
    "        outCls = torch.softmax(outCls, dim=1)\n",
    "\n",
    "        return out1, outCls\n",
    "\n",
    "\n",
    "\n",
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
    "\n",
    "model_path = '/content/drive/MyDrive/ft_model_weights.pth'\n",
    "\n",
    "# 判断模型文件是否存在\n",
    "if os.path.exists(model_path):\n",
    "    # 创建模型实例\n",
    "    # model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "\n",
    "    # 加载模型参数\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # 移动模型到指定设备\n",
    "    model.to(device)\n",
    "    print(\"Model loaded successfully.\")\n",
    "else:\n",
    "    print(\"Model file does not exist.\")\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterionCls = nn.CrossEntropyLoss()\n",
    "# optimizerCls = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "from re import X\n",
    "import time\n",
    "\n",
    "hist = np.zeros(num_epochs)\n",
    "start_time = time.time()\n",
    "lstm = []\n",
    "\n",
    "y_train_pred_main = None\n",
    "y_train_pred_even = None\n",
    "y_train_pred_loss = None\n",
    "cls_to_draw = None\n",
    "\n",
    "for t in range(num_epochs):\n",
    "\n",
    "    for x_main_batch, x_even_batch, x_loss_batch, y_main_batch, y_even_batch, y_loss_batch, y_result_batch in zip(train_tensors['x_main'], train_tensors['x_even'], train_tensors['x_loss'], train_tensors['y_main'], train_tensors['y_even'], train_tensors['y_loss'], train_tensors['y_result']):\n",
    "\n",
    "        x_main_batch = x_main_batch.to(device)\n",
    "        x_even_batch = x_even_batch.to(device)\n",
    "        x_loss_batch = x_loss_batch.to(device)\n",
    "        y_main_batch = y_main_batch.to(device)\n",
    "        y_even_batch = y_even_batch.to(device)\n",
    "        y_loss_batch = y_loss_batch.to(device)\n",
    "        y_result_batch = y_result_batch.to(device)\n",
    "        # print(\"ok: \")\n",
    "        # 替换前10个元素的内容为第11个元素的内容\n",
    "        if len(x_main_batch) > 10:  # 确保有足够的元素\n",
    "            x_main_batch[:10] = x_main_batch[10:11]\n",
    "        # print(\"x_main_batch: \", x_main_batch)\n",
    "        y_train_pred_main, cls_main = model(x_main_batch)\n",
    "        y_train_pred_even, cls_even = model(x_even_batch)\n",
    "        y_train_pred_loss, cls_loss = model(x_loss_batch)\n",
    "\n",
    "        loss_main = criterion(y_train_pred_main, y_main_batch)\n",
    "        loss_even = criterion(y_train_pred_even, y_even_batch)\n",
    "        loss_loss = criterion(y_train_pred_loss, y_loss_batch)\n",
    "\n",
    "        tmp = torch.softmax(cls_main + cls_even + cls_loss, dim=1)\n",
    "        cls_to_draw = tmp\n",
    "\n",
    "        loss_result = criterionCls(tmp, y_result_batch.squeeze().long())\n",
    "        loss = loss_main + loss_even + loss_loss + loss_result\n",
    "\n",
    "        # print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "        hist[t] = loss.item()\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    print(\"Epoch \", t, \"MSE: \", loss.item(), \"cls_main + cls_even + cls_loss: \", tmp)\n",
    "\n",
    "training_time = time.time()-start_time\n",
    "print(\"Training time: {}\".format(training_time))\n",
    "\n",
    "\n",
    "predict_main = pd.DataFrame(scalerMain.inverse_transform(y_train_pred_main.detach().cpu().numpy()))\n",
    "# original_main = pd.DataFrame(scalerMain.inverse_transform(train_tensors['y_main'].detach().numpy()))\n",
    "\n",
    "predict_even = pd.DataFrame(scalerEven.inverse_transform(y_train_pred_even.detach().cpu().numpy()))\n",
    "# original_even = pd.DataFrame(scalerEven.inverse_transform(train_tensors['y_even'].detach().numpy()))\n",
    "\n",
    "predict_loss = pd.DataFrame(scalerLoss.inverse_transform(y_train_pred_loss.detach().cpu().numpy()))\n",
    "\n",
    "predict_cls = pd.DataFrame(scalerLoss.inverse_transform(cls_to_draw.detach().cpu().numpy()))\n",
    "# original_loss = pd.DataFrame(scalerLoss.inverse_transform(train_tensors['y_loss'].detach().numpy()))\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "# ax = sns.lineplot(x = original_main.index, y = original_main[0], label=\"Data Main\", color='royalblue')\n",
    "ax = sns.lineplot(x = predict_main.index, y = predict_main[0], label=\"Training Prediction Main(LSTM)\", color='tomato')\n",
    "\n",
    "# ax = sns.lineplot(x = original_even.index, y = original_even[0], label=\"Data Even\", color='red')\n",
    "ax = sns.lineplot(x = predict_even.index, y = predict_even[0], label=\"Training Prediction Even(LSTM)\", color='#FF6347')\n",
    "\n",
    "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
    "ax = sns.lineplot(x = predict_loss.index, y = predict_loss[0], label=\"Training Prediction Loss(LSTM)\", color='#F31347')\n",
    "\n",
    "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
    "ax = sns.lineplot(x = predict_cls.index, y = predict_cls[0], label=\"Training Prediction Class(LSTM)\", color='#a31347')\n",
    "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
    "ax.set_xlabel(\"Days\", size = 14)\n",
    "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
    "ax.set_xticklabels('', size=10)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "ax = sns.lineplot(data=hist, color='royalblue')\n",
    "ax.set_xlabel(\"Epoch\", size = 14)\n",
    "ax.set_ylabel(\"Loss\", size = 14)\n",
    "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(16)\n",
    "\n",
    "# 保存模型的状态字典\n",
    "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_weights.pth')\n",
    "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_weights_bk.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches:  604  data_main.shape[0]:  298990\n",
      "Number of training batches: 604\n",
      "x_train.shape =  (496, 9, 1)\n",
      "y_train.shape =  (496, 1)\n",
      "x_test.shape =  (0, 9, 1)\n",
      "y_test.shape =  (0, 1)\n",
      "x_result.shape =  (496, 1)\n",
      "Model loaded successfully.\n",
      "Epoch  0 MSE:  0.9697616100311279 cls_main + cls_even + cls_loss:  tensor([[0.4897, 0.1200, 0.3903],\n",
      "        [0.4906, 0.1190, 0.3904],\n",
      "        [0.4907, 0.1186, 0.3907],\n",
      "        ...,\n",
      "        [0.4719, 0.1250, 0.4031],\n",
      "        [0.4719, 0.1250, 0.4031],\n",
      "        [0.4719, 0.1250, 0.4031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  1 MSE:  0.9688915610313416 cls_main + cls_even + cls_loss:  tensor([[0.4923, 0.1213, 0.3864],\n",
      "        [0.4926, 0.1200, 0.3875],\n",
      "        [0.4913, 0.1197, 0.3890],\n",
      "        ...,\n",
      "        [0.4767, 0.1269, 0.3964],\n",
      "        [0.4767, 0.1269, 0.3964],\n",
      "        [0.4767, 0.1269, 0.3964]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  2 MSE:  0.9712332487106323 cls_main + cls_even + cls_loss:  tensor([[0.4891, 0.1230, 0.3879],\n",
      "        [0.4896, 0.1214, 0.3890],\n",
      "        [0.4878, 0.1213, 0.3909],\n",
      "        ...,\n",
      "        [0.4759, 0.1280, 0.3961],\n",
      "        [0.4759, 0.1280, 0.3961],\n",
      "        [0.4759, 0.1280, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  3 MSE:  0.9688122868537903 cls_main + cls_even + cls_loss:  tensor([[0.4905, 0.1216, 0.3879],\n",
      "        [0.4910, 0.1204, 0.3886],\n",
      "        [0.4902, 0.1201, 0.3897],\n",
      "        ...,\n",
      "        [0.4764, 0.1267, 0.3969],\n",
      "        [0.4764, 0.1267, 0.3969],\n",
      "        [0.4764, 0.1267, 0.3969]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  4 MSE:  0.9679355025291443 cls_main + cls_even + cls_loss:  tensor([[0.4910, 0.1204, 0.3886],\n",
      "        [0.4921, 0.1191, 0.3888],\n",
      "        [0.4920, 0.1188, 0.3892],\n",
      "        ...,\n",
      "        [0.4743, 0.1260, 0.3996],\n",
      "        [0.4743, 0.1260, 0.3996],\n",
      "        [0.4743, 0.1260, 0.3996]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  5 MSE:  0.9918632507324219 cls_main + cls_even + cls_loss:  tensor([[0.4771, 0.1255, 0.3974],\n",
      "        [0.4755, 0.1244, 0.4002],\n",
      "        [0.4716, 0.1241, 0.4042],\n",
      "        ...,\n",
      "        [0.4589, 0.1305, 0.4106],\n",
      "        [0.4589, 0.1305, 0.4106],\n",
      "        [0.4589, 0.1305, 0.4106]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  6 MSE:  0.9719455242156982 cls_main + cls_even + cls_loss:  tensor([[0.4897, 0.1217, 0.3885],\n",
      "        [0.4900, 0.1207, 0.3893],\n",
      "        [0.4888, 0.1204, 0.3909],\n",
      "        ...,\n",
      "        [0.4739, 0.1272, 0.3989],\n",
      "        [0.4739, 0.1272, 0.3989],\n",
      "        [0.4739, 0.1272, 0.3989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  7 MSE:  0.9725775122642517 cls_main + cls_even + cls_loss:  tensor([[0.4868, 0.1220, 0.3912],\n",
      "        [0.4873, 0.1210, 0.3917],\n",
      "        [0.4865, 0.1206, 0.3929],\n",
      "        ...,\n",
      "        [0.4732, 0.1269, 0.3999],\n",
      "        [0.4732, 0.1269, 0.3999],\n",
      "        [0.4732, 0.1269, 0.3999]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  8 MSE:  0.9688934683799744 cls_main + cls_even + cls_loss:  tensor([[0.4913, 0.1214, 0.3873],\n",
      "        [0.4916, 0.1204, 0.3880],\n",
      "        [0.4903, 0.1201, 0.3895],\n",
      "        ...,\n",
      "        [0.4767, 0.1267, 0.3966],\n",
      "        [0.4767, 0.1267, 0.3966],\n",
      "        [0.4767, 0.1267, 0.3966]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  9 MSE:  0.9727464914321899 cls_main + cls_even + cls_loss:  tensor([[0.4889, 0.1237, 0.3874],\n",
      "        [0.4882, 0.1232, 0.3886],\n",
      "        [0.4868, 0.1228, 0.3904],\n",
      "        ...,\n",
      "        [0.4753, 0.1285, 0.3962],\n",
      "        [0.4753, 0.1285, 0.3962],\n",
      "        [0.4753, 0.1285, 0.3962]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  10 MSE:  0.9697220325469971 cls_main + cls_even + cls_loss:  tensor([[0.4913, 0.1221, 0.3866],\n",
      "        [0.4915, 0.1211, 0.3873],\n",
      "        [0.4904, 0.1208, 0.3888],\n",
      "        ...,\n",
      "        [0.4775, 0.1270, 0.3954],\n",
      "        [0.4775, 0.1270, 0.3954],\n",
      "        [0.4775, 0.1270, 0.3954]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  11 MSE:  0.9681870341300964 cls_main + cls_even + cls_loss:  tensor([[0.4926, 0.1208, 0.3866],\n",
      "        [0.4934, 0.1197, 0.3869],\n",
      "        [0.4924, 0.1195, 0.3881],\n",
      "        ...,\n",
      "        [0.4776, 0.1261, 0.3963],\n",
      "        [0.4776, 0.1261, 0.3963],\n",
      "        [0.4776, 0.1261, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  12 MSE:  0.9676976799964905 cls_main + cls_even + cls_loss:  tensor([[0.4922, 0.1211, 0.3867],\n",
      "        [0.4930, 0.1201, 0.3870],\n",
      "        [0.4924, 0.1198, 0.3878],\n",
      "        ...,\n",
      "        [0.4776, 0.1264, 0.3959],\n",
      "        [0.4776, 0.1264, 0.3959],\n",
      "        [0.4776, 0.1264, 0.3959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  13 MSE:  0.962084174156189 cls_main + cls_even + cls_loss:  tensor([[0.4993, 0.1194, 0.3814],\n",
      "        [0.5004, 0.1183, 0.3813],\n",
      "        [0.5003, 0.1180, 0.3816],\n",
      "        ...,\n",
      "        [0.4812, 0.1253, 0.3935],\n",
      "        [0.4812, 0.1253, 0.3935],\n",
      "        [0.4812, 0.1253, 0.3935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  14 MSE:  0.9646924734115601 cls_main + cls_even + cls_loss:  tensor([[0.4976, 0.1194, 0.3830],\n",
      "        [0.4987, 0.1183, 0.3829],\n",
      "        [0.4982, 0.1181, 0.3837],\n",
      "        ...,\n",
      "        [0.4799, 0.1255, 0.3946],\n",
      "        [0.4799, 0.1255, 0.3946],\n",
      "        [0.4799, 0.1255, 0.3946]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  15 MSE:  0.9710972905158997 cls_main + cls_even + cls_loss:  tensor([[0.4906, 0.1214, 0.3880],\n",
      "        [0.4912, 0.1203, 0.3885],\n",
      "        [0.4901, 0.1201, 0.3899],\n",
      "        ...,\n",
      "        [0.4758, 0.1273, 0.3969],\n",
      "        [0.4758, 0.1273, 0.3969],\n",
      "        [0.4758, 0.1273, 0.3969]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  16 MSE:  0.9663161635398865 cls_main + cls_even + cls_loss:  tensor([[0.4966, 0.1195, 0.3840],\n",
      "        [0.4976, 0.1182, 0.3841],\n",
      "        [0.4968, 0.1181, 0.3851],\n",
      "        ...,\n",
      "        [0.4778, 0.1263, 0.3959],\n",
      "        [0.4778, 0.1263, 0.3959],\n",
      "        [0.4778, 0.1263, 0.3959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  17 MSE:  0.9633496403694153 cls_main + cls_even + cls_loss:  tensor([[0.4972, 0.1188, 0.3840],\n",
      "        [0.4985, 0.1176, 0.3838],\n",
      "        [0.4985, 0.1175, 0.3840],\n",
      "        ...,\n",
      "        [0.4777, 0.1257, 0.3965],\n",
      "        [0.4777, 0.1257, 0.3965],\n",
      "        [0.4777, 0.1257, 0.3965]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  18 MSE:  0.9719957709312439 cls_main + cls_even + cls_loss:  tensor([[0.4926, 0.1206, 0.3868],\n",
      "        [0.4934, 0.1191, 0.3876],\n",
      "        [0.4917, 0.1190, 0.3893],\n",
      "        ...,\n",
      "        [0.4736, 0.1277, 0.3987],\n",
      "        [0.4736, 0.1277, 0.3987],\n",
      "        [0.4736, 0.1277, 0.3987]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  19 MSE:  0.9613872766494751 cls_main + cls_even + cls_loss:  tensor([[0.4996, 0.1198, 0.3807],\n",
      "        [0.5016, 0.1180, 0.3804],\n",
      "        [0.5014, 0.1180, 0.3807],\n",
      "        ...,\n",
      "        [0.4794, 0.1274, 0.3933],\n",
      "        [0.4794, 0.1274, 0.3933],\n",
      "        [0.4794, 0.1274, 0.3933]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  20 MSE:  0.963462233543396 cls_main + cls_even + cls_loss:  tensor([[0.5001, 0.1198, 0.3800],\n",
      "        [0.5014, 0.1185, 0.3801],\n",
      "        [0.5010, 0.1181, 0.3809],\n",
      "        ...,\n",
      "        [0.4801, 0.1272, 0.3927],\n",
      "        [0.4801, 0.1272, 0.3927],\n",
      "        [0.4801, 0.1272, 0.3927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  21 MSE:  0.9693311452865601 cls_main + cls_even + cls_loss:  tensor([[0.4989, 0.1196, 0.3815],\n",
      "        [0.5000, 0.1181, 0.3819],\n",
      "        [0.4985, 0.1180, 0.3835],\n",
      "        ...,\n",
      "        [0.4749, 0.1280, 0.3971],\n",
      "        [0.4749, 0.1280, 0.3971],\n",
      "        [0.4749, 0.1280, 0.3971]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  22 MSE:  0.9702091217041016 cls_main + cls_even + cls_loss:  tensor([[0.4924, 0.1207, 0.3869],\n",
      "        [0.4937, 0.1192, 0.3871],\n",
      "        [0.4935, 0.1190, 0.3875],\n",
      "        ...,\n",
      "        [0.4719, 0.1290, 0.3992],\n",
      "        [0.4719, 0.1290, 0.3992],\n",
      "        [0.4719, 0.1290, 0.3992]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  23 MSE:  0.9731532335281372 cls_main + cls_even + cls_loss:  tensor([[0.5080, 0.1166, 0.3753],\n",
      "        [0.5085, 0.1156, 0.3758],\n",
      "        [0.5088, 0.1148, 0.3765],\n",
      "        ...,\n",
      "        [0.4638, 0.1245, 0.4117],\n",
      "        [0.4638, 0.1245, 0.4117],\n",
      "        [0.4638, 0.1245, 0.4117]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  24 MSE:  0.9774812459945679 cls_main + cls_even + cls_loss:  tensor([[0.4979, 0.1187, 0.3834],\n",
      "        [0.4985, 0.1172, 0.3843],\n",
      "        [0.4979, 0.1168, 0.3853],\n",
      "        ...,\n",
      "        [0.4619, 0.1274, 0.4108],\n",
      "        [0.4619, 0.1274, 0.4108],\n",
      "        [0.4619, 0.1274, 0.4108]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  25 MSE:  0.9748647212982178 cls_main + cls_even + cls_loss:  tensor([[0.4994, 0.1179, 0.3827],\n",
      "        [0.4999, 0.1168, 0.3833],\n",
      "        [0.5000, 0.1159, 0.3841],\n",
      "        ...,\n",
      "        [0.4647, 0.1268, 0.4085],\n",
      "        [0.4647, 0.1268, 0.4085],\n",
      "        [0.4647, 0.1268, 0.4085]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  26 MSE:  0.9692631363868713 cls_main + cls_even + cls_loss:  tensor([[0.5068, 0.1177, 0.3755],\n",
      "        [0.5079, 0.1160, 0.3761],\n",
      "        [0.5078, 0.1156, 0.3766],\n",
      "        ...,\n",
      "        [0.4694, 0.1266, 0.4040],\n",
      "        [0.4694, 0.1266, 0.4040],\n",
      "        [0.4694, 0.1266, 0.4040]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  27 MSE:  0.9746905565261841 cls_main + cls_even + cls_loss:  tensor([[0.4938, 0.1211, 0.3851],\n",
      "        [0.4946, 0.1193, 0.3861],\n",
      "        [0.4943, 0.1186, 0.3872],\n",
      "        ...,\n",
      "        [0.4665, 0.1307, 0.4028],\n",
      "        [0.4665, 0.1307, 0.4028],\n",
      "        [0.4665, 0.1307, 0.4028]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  28 MSE:  0.9740991592407227 cls_main + cls_even + cls_loss:  tensor([[0.4987, 0.1196, 0.3818],\n",
      "        [0.4999, 0.1177, 0.3825],\n",
      "        [0.4998, 0.1170, 0.3832],\n",
      "        ...,\n",
      "        [0.4668, 0.1289, 0.4044],\n",
      "        [0.4668, 0.1289, 0.4044],\n",
      "        [0.4668, 0.1289, 0.4044]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  29 MSE:  0.9756189584732056 cls_main + cls_even + cls_loss:  tensor([[0.4932, 0.1211, 0.3858],\n",
      "        [0.4948, 0.1183, 0.3869],\n",
      "        [0.4944, 0.1182, 0.3874],\n",
      "        ...,\n",
      "        [0.4659, 0.1299, 0.4042],\n",
      "        [0.4659, 0.1299, 0.4042],\n",
      "        [0.4659, 0.1299, 0.4042]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  30 MSE:  0.9790104627609253 cls_main + cls_even + cls_loss:  tensor([[0.4858, 0.1208, 0.3934],\n",
      "        [0.4869, 0.1188, 0.3942],\n",
      "        [0.4867, 0.1187, 0.3946],\n",
      "        ...,\n",
      "        [0.4611, 0.1292, 0.4097],\n",
      "        [0.4611, 0.1292, 0.4097],\n",
      "        [0.4611, 0.1292, 0.4097]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  31 MSE:  0.9726268649101257 cls_main + cls_even + cls_loss:  tensor([[0.4982, 0.1191, 0.3827],\n",
      "        [0.4995, 0.1173, 0.3832],\n",
      "        [0.4993, 0.1172, 0.3835],\n",
      "        ...,\n",
      "        [0.4675, 0.1278, 0.4048],\n",
      "        [0.4675, 0.1278, 0.4048],\n",
      "        [0.4675, 0.1278, 0.4048]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  32 MSE:  0.9849957227706909 cls_main + cls_even + cls_loss:  tensor([[0.4787, 0.1261, 0.3952],\n",
      "        [0.4790, 0.1240, 0.3970],\n",
      "        [0.4776, 0.1237, 0.3988],\n",
      "        ...,\n",
      "        [0.4618, 0.1354, 0.4029],\n",
      "        [0.4618, 0.1354, 0.4029],\n",
      "        [0.4618, 0.1354, 0.4029]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  33 MSE:  0.9783470034599304 cls_main + cls_even + cls_loss:  tensor([[0.4889, 0.1229, 0.3882],\n",
      "        [0.4899, 0.1205, 0.3896],\n",
      "        [0.4891, 0.1203, 0.3906],\n",
      "        ...,\n",
      "        [0.4648, 0.1326, 0.4026],\n",
      "        [0.4648, 0.1326, 0.4026],\n",
      "        [0.4648, 0.1326, 0.4026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  34 MSE:  0.9682208299636841 cls_main + cls_even + cls_loss:  tensor([[0.5045, 0.1195, 0.3760],\n",
      "        [0.5060, 0.1175, 0.3765],\n",
      "        [0.5056, 0.1174, 0.3771],\n",
      "        ...,\n",
      "        [0.4728, 0.1286, 0.3986],\n",
      "        [0.4728, 0.1286, 0.3986],\n",
      "        [0.4728, 0.1286, 0.3986]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  35 MSE:  0.9825202822685242 cls_main + cls_even + cls_loss:  tensor([[0.4840, 0.1254, 0.3906],\n",
      "        [0.4845, 0.1232, 0.3923],\n",
      "        [0.4834, 0.1226, 0.3940],\n",
      "        ...,\n",
      "        [0.4639, 0.1352, 0.4010],\n",
      "        [0.4639, 0.1352, 0.4010],\n",
      "        [0.4639, 0.1352, 0.4010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  36 MSE:  0.969171404838562 cls_main + cls_even + cls_loss:  tensor([[0.4957, 0.1251, 0.3793],\n",
      "        [0.4967, 0.1236, 0.3797],\n",
      "        [0.4975, 0.1222, 0.3803],\n",
      "        ...,\n",
      "        [0.4722, 0.1355, 0.3923],\n",
      "        [0.4722, 0.1355, 0.3923],\n",
      "        [0.4722, 0.1355, 0.3923]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  37 MSE:  0.9724466800689697 cls_main + cls_even + cls_loss:  tensor([[0.4896, 0.1252, 0.3852],\n",
      "        [0.4905, 0.1239, 0.3856],\n",
      "        [0.4899, 0.1240, 0.3862],\n",
      "        ...,\n",
      "        [0.4704, 0.1358, 0.3938],\n",
      "        [0.4704, 0.1358, 0.3938],\n",
      "        [0.4704, 0.1358, 0.3938]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  38 MSE:  0.9709970951080322 cls_main + cls_even + cls_loss:  tensor([[0.4935, 0.1230, 0.3835],\n",
      "        [0.4949, 0.1212, 0.3838],\n",
      "        [0.4942, 0.1215, 0.3842],\n",
      "        ...,\n",
      "        [0.4709, 0.1330, 0.3961],\n",
      "        [0.4709, 0.1330, 0.3961],\n",
      "        [0.4709, 0.1330, 0.3961]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  39 MSE:  0.9709682464599609 cls_main + cls_even + cls_loss:  tensor([[0.4939, 0.1227, 0.3835],\n",
      "        [0.4952, 0.1210, 0.3838],\n",
      "        [0.4946, 0.1213, 0.3841],\n",
      "        ...,\n",
      "        [0.4697, 0.1322, 0.3981],\n",
      "        [0.4697, 0.1322, 0.3981],\n",
      "        [0.4697, 0.1322, 0.3981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  40 MSE:  0.9717122912406921 cls_main + cls_even + cls_loss:  tensor([[0.4940, 0.1227, 0.3833],\n",
      "        [0.4954, 0.1208, 0.3838],\n",
      "        [0.4947, 0.1212, 0.3842],\n",
      "        ...,\n",
      "        [0.4695, 0.1326, 0.3979],\n",
      "        [0.4695, 0.1326, 0.3979],\n",
      "        [0.4695, 0.1326, 0.3979]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  41 MSE:  0.9745268821716309 cls_main + cls_even + cls_loss:  tensor([[0.4855, 0.1291, 0.3854],\n",
      "        [0.4865, 0.1270, 0.3865],\n",
      "        [0.4864, 0.1264, 0.3872],\n",
      "        ...,\n",
      "        [0.4684, 0.1412, 0.3904],\n",
      "        [0.4684, 0.1412, 0.3904],\n",
      "        [0.4684, 0.1412, 0.3904]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  42 MSE:  0.9719110131263733 cls_main + cls_even + cls_loss:  tensor([[0.4878, 0.1267, 0.3855],\n",
      "        [0.4889, 0.1250, 0.3861],\n",
      "        [0.4890, 0.1246, 0.3864],\n",
      "        ...,\n",
      "        [0.4690, 0.1375, 0.3935],\n",
      "        [0.4690, 0.1375, 0.3935],\n",
      "        [0.4690, 0.1375, 0.3935]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  43 MSE:  0.9744946956634521 cls_main + cls_even + cls_loss:  tensor([[0.4854, 0.1254, 0.3891],\n",
      "        [0.4865, 0.1239, 0.3896],\n",
      "        [0.4863, 0.1234, 0.3902],\n",
      "        ...,\n",
      "        [0.4693, 0.1356, 0.3951],\n",
      "        [0.4693, 0.1356, 0.3951],\n",
      "        [0.4693, 0.1356, 0.3951]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  44 MSE:  0.9780333042144775 cls_main + cls_even + cls_loss:  tensor([[0.4812, 0.1259, 0.3928],\n",
      "        [0.4822, 0.1242, 0.3936],\n",
      "        [0.4817, 0.1237, 0.3946],\n",
      "        ...,\n",
      "        [0.4685, 0.1363, 0.3952],\n",
      "        [0.4685, 0.1363, 0.3952],\n",
      "        [0.4685, 0.1363, 0.3952]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  45 MSE:  0.9746580719947815 cls_main + cls_even + cls_loss:  tensor([[0.4816, 0.1283, 0.3900],\n",
      "        [0.4827, 0.1266, 0.3907],\n",
      "        [0.4827, 0.1262, 0.3912],\n",
      "        ...,\n",
      "        [0.4731, 0.1371, 0.3898],\n",
      "        [0.4731, 0.1371, 0.3898],\n",
      "        [0.4731, 0.1371, 0.3898]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  46 MSE:  0.9696198105812073 cls_main + cls_even + cls_loss:  tensor([[0.4885, 0.1250, 0.3865],\n",
      "        [0.4902, 0.1228, 0.3870],\n",
      "        [0.4903, 0.1225, 0.3872],\n",
      "        ...,\n",
      "        [0.4736, 0.1342, 0.3922],\n",
      "        [0.4736, 0.1342, 0.3922],\n",
      "        [0.4736, 0.1342, 0.3922]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  47 MSE:  0.9716988205909729 cls_main + cls_even + cls_loss:  tensor([[0.4844, 0.1240, 0.3916],\n",
      "        [0.4859, 0.1220, 0.3921],\n",
      "        [0.4859, 0.1218, 0.3923],\n",
      "        ...,\n",
      "        [0.4705, 0.1329, 0.3966],\n",
      "        [0.4705, 0.1329, 0.3966],\n",
      "        [0.4705, 0.1329, 0.3966]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  48 MSE:  0.9733733534812927 cls_main + cls_even + cls_loss:  tensor([[0.4851, 0.1238, 0.3911],\n",
      "        [0.4866, 0.1216, 0.3918],\n",
      "        [0.4865, 0.1214, 0.3922],\n",
      "        ...,\n",
      "        [0.4692, 0.1332, 0.3976],\n",
      "        [0.4692, 0.1332, 0.3976],\n",
      "        [0.4692, 0.1332, 0.3976]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  49 MSE:  0.9701986908912659 cls_main + cls_even + cls_loss:  tensor([[0.4869, 0.1232, 0.3899],\n",
      "        [0.4884, 0.1211, 0.3904],\n",
      "        [0.4886, 0.1208, 0.3906],\n",
      "        ...,\n",
      "        [0.4707, 0.1323, 0.3970],\n",
      "        [0.4707, 0.1323, 0.3970],\n",
      "        [0.4707, 0.1323, 0.3970]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  50 MSE:  0.9755639433860779 cls_main + cls_even + cls_loss:  tensor([[0.4812, 0.1246, 0.3942],\n",
      "        [0.4827, 0.1223, 0.3951],\n",
      "        [0.4824, 0.1221, 0.3955],\n",
      "        ...,\n",
      "        [0.4689, 0.1340, 0.3971],\n",
      "        [0.4689, 0.1340, 0.3971],\n",
      "        [0.4689, 0.1340, 0.3971]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  51 MSE:  0.978297770023346 cls_main + cls_even + cls_loss:  tensor([[0.4773, 0.1261, 0.3966],\n",
      "        [0.4783, 0.1242, 0.3975],\n",
      "        [0.4777, 0.1238, 0.3985],\n",
      "        ...,\n",
      "        [0.4710, 0.1348, 0.3942],\n",
      "        [0.4710, 0.1348, 0.3942],\n",
      "        [0.4710, 0.1348, 0.3942]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  52 MSE:  0.9890636205673218 cls_main + cls_even + cls_loss:  tensor([[0.4711, 0.1230, 0.4059],\n",
      "        [0.4719, 0.1216, 0.4065],\n",
      "        [0.4711, 0.1215, 0.4074],\n",
      "        ...,\n",
      "        [0.4645, 0.1297, 0.4058],\n",
      "        [0.4645, 0.1297, 0.4058],\n",
      "        [0.4645, 0.1297, 0.4058]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  53 MSE:  0.9660657048225403 cls_main + cls_even + cls_loss:  tensor([[0.4932, 0.1230, 0.3838],\n",
      "        [0.4961, 0.1203, 0.3836],\n",
      "        [0.4954, 0.1206, 0.3840],\n",
      "        ...,\n",
      "        [0.4816, 0.1317, 0.3868],\n",
      "        [0.4816, 0.1317, 0.3868],\n",
      "        [0.4816, 0.1317, 0.3868]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  54 MSE:  1.0033528804779053 cls_main + cls_even + cls_loss:  tensor([[0.4771, 0.1283, 0.3946],\n",
      "        [0.4770, 0.1251, 0.3979],\n",
      "        [0.4732, 0.1256, 0.4013],\n",
      "        ...,\n",
      "        [0.4544, 0.1379, 0.4076],\n",
      "        [0.4544, 0.1379, 0.4076],\n",
      "        [0.4544, 0.1379, 0.4076]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  55 MSE:  1.0084595680236816 cls_main + cls_even + cls_loss:  tensor([[0.4750, 0.1279, 0.3971],\n",
      "        [0.4731, 0.1267, 0.4002],\n",
      "        [0.4709, 0.1264, 0.4027],\n",
      "        ...,\n",
      "        [0.4525, 0.1343, 0.4132],\n",
      "        [0.4525, 0.1343, 0.4132],\n",
      "        [0.4525, 0.1343, 0.4132]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  56 MSE:  0.9694703817367554 cls_main + cls_even + cls_loss:  tensor([[0.4906, 0.1243, 0.3851],\n",
      "        [0.4919, 0.1224, 0.3858],\n",
      "        [0.4917, 0.1222, 0.3861],\n",
      "        ...,\n",
      "        [0.4800, 0.1337, 0.3863],\n",
      "        [0.4800, 0.1337, 0.3863],\n",
      "        [0.4800, 0.1337, 0.3863]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  57 MSE:  0.9696126580238342 cls_main + cls_even + cls_loss:  tensor([[0.4883, 0.1245, 0.3872],\n",
      "        [0.4901, 0.1218, 0.3881],\n",
      "        [0.4897, 0.1219, 0.3884],\n",
      "        ...,\n",
      "        [0.4778, 0.1339, 0.3884],\n",
      "        [0.4778, 0.1339, 0.3884],\n",
      "        [0.4778, 0.1339, 0.3884]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  58 MSE:  0.9688482284545898 cls_main + cls_even + cls_loss:  tensor([[0.4902, 0.1236, 0.3862],\n",
      "        [0.4916, 0.1212, 0.3872],\n",
      "        [0.4915, 0.1211, 0.3875],\n",
      "        ...,\n",
      "        [0.4769, 0.1334, 0.3897],\n",
      "        [0.4769, 0.1334, 0.3897],\n",
      "        [0.4769, 0.1334, 0.3897]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  59 MSE:  0.9737583994865417 cls_main + cls_even + cls_loss:  tensor([[0.4825, 0.1255, 0.3920],\n",
      "        [0.4836, 0.1230, 0.3934],\n",
      "        [0.4834, 0.1228, 0.3938],\n",
      "        ...,\n",
      "        [0.4746, 0.1346, 0.3908],\n",
      "        [0.4746, 0.1346, 0.3908],\n",
      "        [0.4746, 0.1346, 0.3908]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  60 MSE:  0.9720434546470642 cls_main + cls_even + cls_loss:  tensor([[0.4841, 0.1258, 0.3900],\n",
      "        [0.4854, 0.1229, 0.3917],\n",
      "        [0.4849, 0.1229, 0.3922],\n",
      "        ...,\n",
      "        [0.4756, 0.1352, 0.3892],\n",
      "        [0.4756, 0.1352, 0.3892],\n",
      "        [0.4756, 0.1352, 0.3892]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  61 MSE:  0.974120557308197 cls_main + cls_even + cls_loss:  tensor([[0.4838, 0.1252, 0.3910],\n",
      "        [0.4836, 0.1225, 0.3939],\n",
      "        [0.4847, 0.1225, 0.3928],\n",
      "        ...,\n",
      "        [0.4732, 0.1342, 0.3927],\n",
      "        [0.4732, 0.1342, 0.3927],\n",
      "        [0.4732, 0.1342, 0.3927]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  62 MSE:  0.9716691374778748 cls_main + cls_even + cls_loss:  tensor([[0.4902, 0.1240, 0.3858],\n",
      "        [0.4895, 0.1214, 0.3892],\n",
      "        [0.4912, 0.1213, 0.3875],\n",
      "        ...,\n",
      "        [0.4740, 0.1355, 0.3905],\n",
      "        [0.4740, 0.1355, 0.3905],\n",
      "        [0.4740, 0.1355, 0.3905]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  63 MSE:  0.9740543961524963 cls_main + cls_even + cls_loss:  tensor([[0.4813, 0.1240, 0.3947],\n",
      "        [0.4813, 0.1220, 0.3968],\n",
      "        [0.4818, 0.1217, 0.3966],\n",
      "        ...,\n",
      "        [0.4712, 0.1346, 0.3942],\n",
      "        [0.4712, 0.1346, 0.3942],\n",
      "        [0.4712, 0.1346, 0.3942]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  64 MSE:  0.9779021143913269 cls_main + cls_even + cls_loss:  tensor([[0.4820, 0.1238, 0.3942],\n",
      "        [0.4816, 0.1218, 0.3966],\n",
      "        [0.4825, 0.1216, 0.3959],\n",
      "        ...,\n",
      "        [0.4693, 0.1343, 0.3964],\n",
      "        [0.4693, 0.1343, 0.3964],\n",
      "        [0.4693, 0.1343, 0.3964]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  65 MSE:  0.9762464165687561 cls_main + cls_even + cls_loss:  tensor([[0.4827, 0.1243, 0.3931],\n",
      "        [0.4824, 0.1221, 0.3955],\n",
      "        [0.4832, 0.1219, 0.3949],\n",
      "        ...,\n",
      "        [0.4722, 0.1339, 0.3939],\n",
      "        [0.4722, 0.1339, 0.3939],\n",
      "        [0.4722, 0.1339, 0.3939]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  66 MSE:  0.9703029990196228 cls_main + cls_even + cls_loss:  tensor([[0.5196, 0.1174, 0.3629],\n",
      "        [0.5205, 0.1155, 0.3640],\n",
      "        [0.5204, 0.1151, 0.3645],\n",
      "        ...,\n",
      "        [0.4640, 0.1260, 0.4100],\n",
      "        [0.4640, 0.1260, 0.4100],\n",
      "        [0.4640, 0.1260, 0.4100]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  67 MSE:  0.9819082617759705 cls_main + cls_even + cls_loss:  tensor([[0.4876, 0.1216, 0.3908],\n",
      "        [0.4882, 0.1190, 0.3928],\n",
      "        [0.4876, 0.1188, 0.3935],\n",
      "        ...,\n",
      "        [0.4598, 0.1311, 0.4091],\n",
      "        [0.4598, 0.1311, 0.4091],\n",
      "        [0.4598, 0.1311, 0.4091]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  68 MSE:  0.9865245223045349 cls_main + cls_even + cls_loss:  tensor([[0.4805, 0.1238, 0.3957],\n",
      "        [0.4805, 0.1210, 0.3985],\n",
      "        [0.4816, 0.1208, 0.3976],\n",
      "        ...,\n",
      "        [0.4542, 0.1346, 0.4112],\n",
      "        [0.4542, 0.1346, 0.4112],\n",
      "        [0.4542, 0.1346, 0.4112]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  69 MSE:  0.9845899343490601 cls_main + cls_even + cls_loss:  tensor([[0.4783, 0.1245, 0.3973],\n",
      "        [0.4781, 0.1223, 0.3996],\n",
      "        [0.4788, 0.1219, 0.3993],\n",
      "        ...,\n",
      "        [0.4573, 0.1360, 0.4067],\n",
      "        [0.4573, 0.1360, 0.4067],\n",
      "        [0.4573, 0.1360, 0.4067]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  70 MSE:  0.982821524143219 cls_main + cls_even + cls_loss:  tensor([[0.4755, 0.1259, 0.3987],\n",
      "        [0.4760, 0.1238, 0.4002],\n",
      "        [0.4761, 0.1234, 0.4005],\n",
      "        ...,\n",
      "        [0.4592, 0.1377, 0.4031],\n",
      "        [0.4592, 0.1377, 0.4031],\n",
      "        [0.4592, 0.1377, 0.4031]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  71 MSE:  0.9805606603622437 cls_main + cls_even + cls_loss:  tensor([[0.4867, 0.1241, 0.3892],\n",
      "        [0.4872, 0.1221, 0.3906],\n",
      "        [0.4872, 0.1211, 0.3917],\n",
      "        ...,\n",
      "        [0.4605, 0.1358, 0.4037],\n",
      "        [0.4605, 0.1358, 0.4037],\n",
      "        [0.4605, 0.1358, 0.4037]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  72 MSE:  0.98218834400177 cls_main + cls_even + cls_loss:  tensor([[0.4774, 0.1270, 0.3956],\n",
      "        [0.4783, 0.1242, 0.3975],\n",
      "        [0.4783, 0.1235, 0.3982],\n",
      "        ...,\n",
      "        [0.4587, 0.1395, 0.4018],\n",
      "        [0.4587, 0.1395, 0.4018],\n",
      "        [0.4587, 0.1395, 0.4018]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  73 MSE:  0.9884520769119263 cls_main + cls_even + cls_loss:  tensor([[0.4709, 0.1281, 0.4010],\n",
      "        [0.4708, 0.1263, 0.4029],\n",
      "        [0.4712, 0.1254, 0.4035],\n",
      "        ...,\n",
      "        [0.4556, 0.1420, 0.4023],\n",
      "        [0.4556, 0.1420, 0.4023],\n",
      "        [0.4556, 0.1420, 0.4023]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  74 MSE:  0.9807822704315186 cls_main + cls_even + cls_loss:  tensor([[0.4813, 0.1251, 0.3936],\n",
      "        [0.4811, 0.1228, 0.3961],\n",
      "        [0.4816, 0.1226, 0.3957],\n",
      "        ...,\n",
      "        [0.4629, 0.1373, 0.3998],\n",
      "        [0.4629, 0.1373, 0.3998],\n",
      "        [0.4629, 0.1373, 0.3998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  75 MSE:  0.9844767451286316 cls_main + cls_even + cls_loss:  tensor([[0.4732, 0.1279, 0.3990],\n",
      "        [0.4735, 0.1250, 0.4015],\n",
      "        [0.4735, 0.1249, 0.4016],\n",
      "        ...,\n",
      "        [0.4616, 0.1404, 0.3981],\n",
      "        [0.4616, 0.1404, 0.3981],\n",
      "        [0.4616, 0.1404, 0.3981]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  76 MSE:  0.9947046041488647 cls_main + cls_even + cls_loss:  tensor([[0.4486, 0.1360, 0.4155],\n",
      "        [0.4493, 0.1319, 0.4188],\n",
      "        [0.4494, 0.1336, 0.4170],\n",
      "        ...,\n",
      "        [0.4434, 0.1462, 0.4104],\n",
      "        [0.4434, 0.1462, 0.4104],\n",
      "        [0.4434, 0.1462, 0.4104]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  77 MSE:  0.9892531633377075 cls_main + cls_even + cls_loss:  tensor([[0.4565, 0.1325, 0.4110],\n",
      "        [0.4573, 0.1301, 0.4126],\n",
      "        [0.4576, 0.1302, 0.4122],\n",
      "        ...,\n",
      "        [0.4511, 0.1412, 0.4077],\n",
      "        [0.4511, 0.1412, 0.4077],\n",
      "        [0.4511, 0.1412, 0.4077]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  78 MSE:  0.9846892952919006 cls_main + cls_even + cls_loss:  tensor([[0.4630, 0.1311, 0.4059],\n",
      "        [0.4639, 0.1288, 0.4074],\n",
      "        [0.4641, 0.1290, 0.4069],\n",
      "        ...,\n",
      "        [0.4582, 0.1393, 0.4026],\n",
      "        [0.4582, 0.1393, 0.4026],\n",
      "        [0.4582, 0.1393, 0.4026]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  79 MSE:  0.9802870154380798 cls_main + cls_even + cls_loss:  tensor([[0.4694, 0.1281, 0.4025],\n",
      "        [0.4703, 0.1258, 0.4039],\n",
      "        [0.4705, 0.1260, 0.4035],\n",
      "        ...,\n",
      "        [0.4620, 0.1363, 0.4018],\n",
      "        [0.4620, 0.1363, 0.4018],\n",
      "        [0.4620, 0.1363, 0.4018]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  80 MSE:  0.9762643575668335 cls_main + cls_even + cls_loss:  tensor([[0.4747, 0.1254, 0.3999],\n",
      "        [0.4757, 0.1234, 0.4009],\n",
      "        [0.4757, 0.1235, 0.4008],\n",
      "        ...,\n",
      "        [0.4667, 0.1339, 0.3994],\n",
      "        [0.4667, 0.1339, 0.3994],\n",
      "        [0.4667, 0.1339, 0.3994]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  81 MSE:  0.9771880507469177 cls_main + cls_even + cls_loss:  tensor([[0.4759, 0.1256, 0.3985],\n",
      "        [0.4769, 0.1234, 0.3997],\n",
      "        [0.4766, 0.1236, 0.3998],\n",
      "        ...,\n",
      "        [0.4670, 0.1342, 0.3989],\n",
      "        [0.4670, 0.1342, 0.3989],\n",
      "        [0.4670, 0.1342, 0.3989]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  82 MSE:  0.9785088300704956 cls_main + cls_even + cls_loss:  tensor([[0.4798, 0.1263, 0.3939],\n",
      "        [0.4807, 0.1243, 0.3950],\n",
      "        [0.4801, 0.1241, 0.3958],\n",
      "        ...,\n",
      "        [0.4664, 0.1362, 0.3974],\n",
      "        [0.4664, 0.1362, 0.3974],\n",
      "        [0.4664, 0.1362, 0.3974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  83 MSE:  0.9757187962532043 cls_main + cls_even + cls_loss:  tensor([[0.4837, 0.1251, 0.3912],\n",
      "        [0.4841, 0.1235, 0.3924],\n",
      "        [0.4831, 0.1237, 0.3932],\n",
      "        ...,\n",
      "        [0.4689, 0.1355, 0.3956],\n",
      "        [0.4689, 0.1355, 0.3956],\n",
      "        [0.4689, 0.1355, 0.3956]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  84 MSE:  0.974503755569458 cls_main + cls_even + cls_loss:  tensor([[0.4943, 0.1205, 0.3852],\n",
      "        [0.4952, 0.1184, 0.3864],\n",
      "        [0.4947, 0.1179, 0.3874],\n",
      "        ...,\n",
      "        [0.4668, 0.1310, 0.4022],\n",
      "        [0.4668, 0.1310, 0.4022],\n",
      "        [0.4668, 0.1310, 0.4022]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  85 MSE:  0.9783962368965149 cls_main + cls_even + cls_loss:  tensor([[0.4855, 0.1222, 0.3923],\n",
      "        [0.4861, 0.1200, 0.3939],\n",
      "        [0.4855, 0.1195, 0.3950],\n",
      "        ...,\n",
      "        [0.4654, 0.1329, 0.4017],\n",
      "        [0.4654, 0.1329, 0.4017],\n",
      "        [0.4654, 0.1329, 0.4017]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  86 MSE:  0.9794878959655762 cls_main + cls_even + cls_loss:  tensor([[0.4749, 0.1263, 0.3988],\n",
      "        [0.4755, 0.1239, 0.4006],\n",
      "        [0.4749, 0.1237, 0.4014],\n",
      "        ...,\n",
      "        [0.4657, 0.1370, 0.3973],\n",
      "        [0.4657, 0.1370, 0.3973],\n",
      "        [0.4657, 0.1370, 0.3973]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  87 MSE:  0.9756578803062439 cls_main + cls_even + cls_loss:  tensor([[0.4781, 0.1245, 0.3975],\n",
      "        [0.4786, 0.1224, 0.3990],\n",
      "        [0.4781, 0.1219, 0.4001],\n",
      "        ...,\n",
      "        [0.4698, 0.1348, 0.3954],\n",
      "        [0.4698, 0.1348, 0.3954],\n",
      "        [0.4698, 0.1348, 0.3954]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  88 MSE:  0.9801294803619385 cls_main + cls_even + cls_loss:  tensor([[0.4772, 0.1252, 0.3976],\n",
      "        [0.4780, 0.1227, 0.3992],\n",
      "        [0.4774, 0.1223, 0.4003],\n",
      "        ...,\n",
      "        [0.4666, 0.1359, 0.3974],\n",
      "        [0.4666, 0.1359, 0.3974],\n",
      "        [0.4666, 0.1359, 0.3974]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  89 MSE:  0.9844263792037964 cls_main + cls_even + cls_loss:  tensor([[0.4735, 0.1278, 0.3987],\n",
      "        [0.4743, 0.1254, 0.4003],\n",
      "        [0.4736, 0.1249, 0.4015],\n",
      "        ...,\n",
      "        [0.4648, 0.1380, 0.3972],\n",
      "        [0.4648, 0.1380, 0.3972],\n",
      "        [0.4648, 0.1380, 0.3972]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  90 MSE:  0.990074634552002 cls_main + cls_even + cls_loss:  tensor([[0.4719, 0.1292, 0.3989],\n",
      "        [0.4725, 0.1270, 0.4004],\n",
      "        [0.4713, 0.1267, 0.4020],\n",
      "        ...,\n",
      "        [0.4629, 0.1384, 0.3988],\n",
      "        [0.4629, 0.1384, 0.3988],\n",
      "        [0.4629, 0.1384, 0.3988]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  91 MSE:  0.9770870804786682 cls_main + cls_even + cls_loss:  tensor([[0.4776, 0.1248, 0.3976],\n",
      "        [0.4783, 0.1227, 0.3990],\n",
      "        [0.4776, 0.1226, 0.3998],\n",
      "        ...,\n",
      "        [0.4711, 0.1333, 0.3956],\n",
      "        [0.4711, 0.1333, 0.3956],\n",
      "        [0.4711, 0.1333, 0.3956]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  92 MSE:  0.9772789478302002 cls_main + cls_even + cls_loss:  tensor([[0.4760, 0.1252, 0.3989],\n",
      "        [0.4769, 0.1233, 0.3998],\n",
      "        [0.4761, 0.1233, 0.4006],\n",
      "        ...,\n",
      "        [0.4690, 0.1340, 0.3970],\n",
      "        [0.4690, 0.1340, 0.3970],\n",
      "        [0.4690, 0.1340, 0.3970]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Epoch  93 MSE:  0.9929595589637756 cls_main + cls_even + cls_loss:  tensor([[0.4691, 0.1297, 0.4012],\n",
      "        [0.4694, 0.1279, 0.4027],\n",
      "        [0.4678, 0.1277, 0.4045],\n",
      "        ...,\n",
      "        [0.4599, 0.1388, 0.4012],\n",
      "        [0.4599, 0.1388, 0.4012],\n",
      "        [0.4599, 0.1388, 0.4012]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/output'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "#@title 获取数据\n",
    "\n",
    "# filepath = '/kaggle/input/stock-time-series-20050101-to-20171231/AMZN_2006-01-01_to_2018-01-01.csv'\n",
    "# filepath = '/kaggle/input/amazon.csv'#@title 训练模型\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_dim = 1\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "output_cls_dim = 3\n",
    "num_epochs = 100\n",
    "batch_size = 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def split_data(dataMain, dataEven, dataLoss, dataResult, lookback, batch_size = 20):\n",
    "    data_raw_main = dataMain.to_numpy()\n",
    "    data_raw_even = dataEven.to_numpy()\n",
    "    data_raw_loss = dataLoss.to_numpy()\n",
    "    data_raw_result = dataResult.to_numpy()\n",
    "    data_main = []\n",
    "    data_even = []\n",
    "    data_loss = []\n",
    "    data_result = []\n",
    "\n",
    "    # create all possible sequences of length lookback\n",
    "    for index in range(len(data_raw_main) - lookback):\n",
    "        data_main.append(data_raw_main[index: index + lookback])\n",
    "    for index in range(len(data_raw_even) - lookback):\n",
    "        data_even.append(data_raw_even[index: index + lookback])\n",
    "    for index in range(len(data_raw_loss) - lookback):\n",
    "        data_loss.append(data_raw_loss[index: index + lookback])\n",
    "    for index in range(len(data_raw_result) - lookback):\n",
    "        data_result.append(data_raw_result[index: index + lookback])\n",
    "\n",
    "    data_main = np.array(data_main)\n",
    "    data_even = np.array(data_even)\n",
    "    data_loss = np.array(data_loss)\n",
    "    data_result = np.array(data_result)\n",
    "\n",
    "    test_set_size = int(np.round(0.2 * data_main.shape[0]))\n",
    "    train_set_size = data_main.shape[0]\n",
    "\n",
    "    # 将训练数据切割成多个批次\n",
    "    num_batches = train_set_size // batch_size\n",
    "    print(\"num_batches: \", num_batches, \" data_main.shape[0]: \", data_main.shape[0])\n",
    "\n",
    "\n",
    "    x_train = {\n",
    "        'main': np.array_split(data_main[:train_set_size, :-1, :], num_batches),\n",
    "        'even': np.array_split(data_even[:train_set_size, :-1, :], num_batches),\n",
    "        'loss': np.array_split(data_loss[:train_set_size, :-1, :], num_batches),\n",
    "        'result': np.array_split(data_result[:train_set_size, :-1, :], num_batches)\n",
    "    }\n",
    "    y_train = {\n",
    "        'main': np.array_split(data_main[:train_set_size, -1, :], num_batches),\n",
    "        'even': np.array_split(data_even[:train_set_size, -1, :], num_batches),\n",
    "        'loss': np.array_split(data_loss[:train_set_size, -1, :], num_batches),\n",
    "        'result': np.array_split(data_result[:train_set_size, -1, :], num_batches)\n",
    "    }\n",
    "\n",
    "    x_test = {\n",
    "        'main': np.array_split(data_main[train_set_size:, :-1, :], num_batches),\n",
    "        'even': np.array_split(data_even[train_set_size:, :-1, :], num_batches),\n",
    "        'loss': np.array_split(data_loss[train_set_size:, :-1, :], num_batches),\n",
    "        'result': np.array_split(data_result[train_set_size:, :-1, :], num_batches)\n",
    "    }\n",
    "    y_test = {\n",
    "        'main': np.array_split(data_main[train_set_size:, -1, :], num_batches),\n",
    "        'even': np.array_split(data_even[train_set_size:, -1, :], num_batches),\n",
    "        'loss': np.array_split(data_loss[train_set_size:, -1, :], num_batches),\n",
    "        'result': np.array_split(data_result[train_set_size:, -1, :], num_batches)\n",
    "    }\n",
    "    # print(\"x_train['main']: \", x_train['main'][1])\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "\n",
    "lookback = 10 # choose sequence length\n",
    "x_train, y_train, x_test, y_test = split_data(dataMain, dataEven, dataLoss, dataResult, lookback, 495)\n",
    "# x_train_batches, y_train_batches, x_test, y_test = split_data(price, lookback=20, batch_size=32)\n",
    "\n",
    "# # 打印批次的数量和每个批次的形状\n",
    "# print('Number of training batches:', len(x_train_batches))\n",
    "# if len(x_train_batches) > 0:\n",
    "#     print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
    "#     print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
    "\n",
    "# print('x_test.shape = ', x_test.shape)\n",
    "# print('y_test.shape = ', y_test.shape)\n",
    "\n",
    "# 打印批次的数量和每个批次的形状\n",
    "print('Number of training batches:', len(x_train['main']))\n",
    "if len(x_train['main']) > 0:\n",
    "    # print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
    "    # print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
    "\n",
    "    print('x_train.shape = ',x_train['main'][0].shape)\n",
    "    print('y_train.shape = ',y_train['main'][0].shape)\n",
    "    print('x_test.shape = ',x_test['main'][0].shape)\n",
    "    print('y_test.shape = ',y_test['main'][0].shape)\n",
    "    print('x_result.shape = ',y_train['result'][0].shape)\n",
    "\n",
    "# print(\"x_train: \", x_train)\n",
    "# print(\"y_train: \", y_train)\n",
    "\n",
    "# x_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train_batches]\n",
    "# y_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train_batches]\n",
    "import torch\n",
    "\n",
    "# 转换为字典\n",
    "train_tensors = {\n",
    "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['main']],\n",
    "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['even']],\n",
    "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['loss']],\n",
    "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['result']],\n",
    "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['main']],\n",
    "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['even']],\n",
    "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['loss']],\n",
    "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['result']]\n",
    "}\n",
    "\n",
    "test_tensors = {\n",
    "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['main']],\n",
    "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['even']],\n",
    "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['loss']],\n",
    "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['result']],\n",
    "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['main']],\n",
    "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['even']],\n",
    "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['loss']],\n",
    "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['result']]\n",
    "}\n",
    "\n",
    "\n",
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.num_layers = num_layers\n",
    "\n",
    "#         self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "#         self.fcCls = nn.Linear(hidden_dim, output_cls_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
    "#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
    "#         out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "#         out1 = self.fc(out[:, -1, :])\n",
    "\n",
    "#         # 通过全连接层\n",
    "#         outCls = self.fcCls(out[:, -1, :])\n",
    "#         # 应用 softmax 激活\n",
    "#         outCls = torch.softmax(outCls, dim=1)\n",
    "\n",
    "#         return out1, outCls\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fcCls = nn.Linear(hidden_dim, output_cls_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
    "        out, (hn) = self.gru(x, (h0.detach()))\n",
    "        out1 = self.fc(out[:, -1, :])\n",
    "        # 通过全连接层\n",
    "        outCls = self.fcCls(out[:, -1, :])\n",
    "        # 应用 softmax 激活\n",
    "        outCls = torch.softmax(outCls, dim=1)\n",
    "\n",
    "        return out1, outCls\n",
    "\n",
    "\n",
    "\n",
    "# model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
    "\n",
    "model = GRU(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
    "# criterion = torch.nn.MSELoss(reduction='mean')\n",
    "# optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model_path = '/content/drive/MyDrive/ft_gru_model_weights.pth'\n",
    "\n",
    "# 判断模型文件是否存在\n",
    "if os.path.exists(model_path):\n",
    "    # 创建模型实例\n",
    "    # model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "\n",
    "    # 加载模型参数\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # 移动模型到指定设备\n",
    "    model.to(device)\n",
    "    print(\"Model loaded successfully.\")\n",
    "else:\n",
    "    print(\"Model file does not exist.\")\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterionCls = nn.CrossEntropyLoss()\n",
    "# optimizerCls = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "from re import X\n",
    "import time\n",
    "\n",
    "hist = np.zeros(num_epochs)\n",
    "start_time = time.time()\n",
    "lstm = []\n",
    "\n",
    "y_train_pred_main = None\n",
    "y_train_pred_even = None\n",
    "y_train_pred_loss = None\n",
    "cls_to_draw = None\n",
    "\n",
    "for t in range(num_epochs):\n",
    "\n",
    "    for x_main_batch, x_even_batch, x_loss_batch, y_main_batch, y_even_batch, y_loss_batch, y_result_batch in zip(train_tensors['x_main'], train_tensors['x_even'], train_tensors['x_loss'], train_tensors['y_main'], train_tensors['y_even'], train_tensors['y_loss'], train_tensors['y_result']):\n",
    "\n",
    "        x_main_batch = x_main_batch.to(device)\n",
    "        x_even_batch = x_even_batch.to(device)\n",
    "        x_loss_batch = x_loss_batch.to(device)\n",
    "        y_main_batch = y_main_batch.to(device)\n",
    "        y_even_batch = y_even_batch.to(device)\n",
    "        y_loss_batch = y_loss_batch.to(device)\n",
    "        y_result_batch = y_result_batch.to(device)\n",
    "        # print(\"ok: \")\n",
    "        # 替换前10个元素的内容为第11个元素的内容\n",
    "        if len(x_main_batch) > 10:  # 确保有足够的元素\n",
    "            x_main_batch[:10] = x_main_batch[10:11]\n",
    "        # print(\"x_main_batch: \", x_main_batch)\n",
    "        y_train_pred_main, cls_main = model(x_main_batch)\n",
    "        y_train_pred_even, cls_even = model(x_even_batch)\n",
    "        y_train_pred_loss, cls_loss = model(x_loss_batch)\n",
    "\n",
    "        loss_main = criterion(y_train_pred_main, y_main_batch)\n",
    "        loss_even = criterion(y_train_pred_even, y_even_batch)\n",
    "        loss_loss = criterion(y_train_pred_loss, y_loss_batch)\n",
    "\n",
    "        tmp = torch.softmax(cls_main + cls_even + cls_loss, dim=1)\n",
    "        cls_to_draw = tmp\n",
    "\n",
    "        loss_result = criterionCls(tmp, y_result_batch.squeeze().long())\n",
    "        loss = loss_main + loss_even + loss_loss + loss_result\n",
    "\n",
    "        # print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "        hist[t] = loss.item()\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    print(\"Epoch \", t, \"MSE: \", loss.item(), \"cls_main + cls_even + cls_loss: \", tmp)\n",
    "\n",
    "training_time = time.time()-start_time\n",
    "print(\"Training time: {}\".format(training_time))\n",
    "\n",
    "\n",
    "predict_main = pd.DataFrame(scalerMain.inverse_transform(y_train_pred_main.detach().cpu().numpy()))\n",
    "# original_main = pd.DataFrame(scalerMain.inverse_transform(train_tensors['y_main'].detach().numpy()))\n",
    "\n",
    "predict_even = pd.DataFrame(scalerEven.inverse_transform(y_train_pred_even.detach().cpu().numpy()))\n",
    "# original_even = pd.DataFrame(scalerEven.inverse_transform(train_tensors['y_even'].detach().numpy()))\n",
    "\n",
    "predict_loss = pd.DataFrame(scalerLoss.inverse_transform(y_train_pred_loss.detach().cpu().numpy()))\n",
    "\n",
    "predict_cls = pd.DataFrame(scalerLoss.inverse_transform(cls_to_draw.detach().cpu().numpy()))\n",
    "# original_loss = pd.DataFrame(scalerLoss.inverse_transform(train_tensors['y_loss'].detach().numpy()))\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "# ax = sns.lineplot(x = original_main.index, y = original_main[0], label=\"Data Main\", color='royalblue')\n",
    "ax = sns.lineplot(x = predict_main.index, y = predict_main[0], label=\"Training Prediction Main(LSTM)\", color='tomato')\n",
    "\n",
    "# ax = sns.lineplot(x = original_even.index, y = original_even[0], label=\"Data Even\", color='red')\n",
    "ax = sns.lineplot(x = predict_even.index, y = predict_even[0], label=\"Training Prediction Even(LSTM)\", color='#FF6347')\n",
    "\n",
    "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
    "ax = sns.lineplot(x = predict_loss.index, y = predict_loss[0], label=\"Training Prediction Loss(LSTM)\", color='#F31347')\n",
    "\n",
    "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
    "ax = sns.lineplot(x = predict_cls.index, y = predict_cls[0], label=\"Training Prediction Class(LSTM)\", color='#a31347')\n",
    "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
    "ax.set_xlabel(\"Days\", size = 14)\n",
    "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
    "ax.set_xticklabels('', size=10)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "ax = sns.lineplot(data=hist, color='royalblue')\n",
    "ax.set_xlabel(\"Epoch\", size = 14)\n",
    "ax.set_ylabel(\"Loss\", size = 14)\n",
    "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(16)\n",
    "\n",
    "# 保存模型的状态字典\n",
    "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_gru_model_weights.pth')\n",
    "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_gru_model_weights_bk.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESFPpJkHgBxL"
   },
   "outputs": [],
   "source": [
    "#@title 绘制概率\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 类别标签\n",
    "categories = ['Class 0', 'Class 1', 'Class 2']\n",
    "\n",
    "# 绘制每一行的柱形图\n",
    "for i in range(cls_to_draw.size(0)):\n",
    "    plt.figure()\n",
    "    plt.bar(categories, cls_to_draw[i].detach().numpy(), color=['blue', 'orange', 'green'])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(f'Class Probability Distribution for Sample {i + 1}')\n",
    "    plt.show()\n",
    "    if i > 5:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpNB_XiZfuRT"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # 类别标签\n",
    "# categories = ['Class 0', 'Class 1', 'Class 2']\n",
    "# num_samples = cls_to_draw.size(0)\n",
    "\n",
    "# # 设置柱形图的宽度\n",
    "# bar_width = 0.2\n",
    "# x = np.arange(num_samples)\n",
    "\n",
    "# # 绘制柱形图\n",
    "# for i in range(cls_to_draw.size(1)):  # 遍历每个类别\n",
    "#     plt.bar(x + i * bar_width, cls_to_draw[:, i].detach().numpy(), width=bar_width, label=categories[i])\n",
    "\n",
    "# # 设置标签和标题\n",
    "# plt.xlabel('Samples')\n",
    "# plt.ylabel('Probability')\n",
    "# plt.title('Class Probability Distribution')\n",
    "# plt.xticks(x + bar_width, [f'Sample {i + 1}' for i in range(num_samples)])\n",
    "# plt.ylim(0, 1)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTQEnT_LMqBu"
   },
   "source": [
    "## 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UijTuZJhp_y"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/output'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "#@title 获取数据\n",
    "\n",
    "# filepath = '/kaggle/input/stock-time-series-20050101-to-20171231/AMZN_2006-01-01_to_2018-01-01.csv'\n",
    "# filepath = '/kaggle/input/amazon.csv'\n",
    "import pandas as pd\n",
    "\n",
    "file_paths = ['/content/drive/MyDrive/ftTrainDataDetail/mw_1250309.csv']\n",
    "dataframes = []\n",
    "tgtLength = 500\n",
    "\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path, encoding='latin1')\n",
    "    df = df[::-1]  # 反转 DataFrame 的顺序\n",
    "\n",
    "    # 检查文件大小\n",
    "    if len(df) < tgtLength:\n",
    "        # 计算需要重复的行数\n",
    "        num_rows_to_add = tgtLength - len(df)\n",
    "        last_row = df.iloc[-1:]  # 获取最后一行\n",
    "        # 重复最后一行\n",
    "        repeated_rows = pd.concat([last_row] * num_rows_to_add, ignore_index=True)\n",
    "        df = pd.concat([df, repeated_rows], ignore_index=True)\n",
    "        # 创建补充的行，填充为零\n",
    "        # zero_rows = pd.DataFrame(0, index=range(num_rows_to_add), columns=df.columns)\n",
    "        # df = pd.concat([df, zero_rows], ignore_index=True)\n",
    "    else:\n",
    "        df = df.iloc[:tgtLength]\n",
    "\n",
    "    dataframes.append(df)\n",
    "\n",
    "# 合并所有数据\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "# 保存为 CSV 文件\n",
    "data.to_csv('hello_output.csv', index=False)\n",
    "\n",
    "#@title 数据去除特殊符号\n",
    "\n",
    "dataMain = data[['Main']]\n",
    "dataEven = data[['Even']]\n",
    "dataLoss = data[['Loss']]\n",
    "dataResult = data[['Result']]\n",
    "\n",
    "dataMain['Main'] = dataMain['Main'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
    "dataEven['Even'] = dataEven['Even'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
    "dataLoss['Loss'] = dataLoss['Loss'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
    "\n",
    "#@title 数据归一化\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scalerMain = MinMaxScaler(feature_range=(-1, 1))\n",
    "scalerEven = MinMaxScaler(feature_range=(-1, 1))\n",
    "scalerLoss = MinMaxScaler(feature_range=(-1, 1))\n",
    "scalerResult = MinMaxScaler(feature_range=(-1, 1))\n",
    "dataMain['Main'] = scalerMain.fit_transform(dataMain['Main'].values.reshape(-1,1))\n",
    "dataEven['Even'] = scalerEven.fit_transform(dataEven['Even'].values.reshape(-1,1))\n",
    "dataLoss['Loss'] = scalerLoss.fit_transform(dataLoss['Loss'].values.reshape(-1,1))\n",
    "dataResult['Result'] = dataResult['Result']\n",
    "print(dataMain['Main'] .shape)\n",
    "\n",
    "#@title 获取准备验证的数据\n",
    "\n",
    "import math, time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_dim = 1\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "output_dim = 1\n",
    "output_cls_dim = 3\n",
    "num_epochs = 100\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "def split_data(dataMain, dataEven, dataLoss, dataResult, lookback, batch_size = 20):\n",
    "    data_raw_main = dataMain.to_numpy()\n",
    "    data_raw_even = dataEven.to_numpy()\n",
    "    data_raw_loss = dataLoss.to_numpy()\n",
    "    data_raw_result = dataResult.to_numpy()\n",
    "    data_main = []\n",
    "    data_even = []\n",
    "    data_loss = []\n",
    "    data_result = []\n",
    "\n",
    "    # create all possible sequences of length lookback\n",
    "    for index in range(len(data_raw_main) - lookback):\n",
    "        data_main.append(data_raw_main[index: index + lookback])\n",
    "    for index in range(len(data_raw_even) - lookback):\n",
    "        data_even.append(data_raw_even[index: index + lookback])\n",
    "    for index in range(len(data_raw_loss) - lookback):\n",
    "        data_loss.append(data_raw_loss[index: index + lookback])\n",
    "    for index in range(len(data_raw_result) - lookback):\n",
    "        data_result.append(data_raw_result[index: index + lookback])\n",
    "\n",
    "    data_main = np.array(data_main)\n",
    "    data_even = np.array(data_even)\n",
    "    data_loss = np.array(data_loss)\n",
    "    data_result = np.array(data_result)\n",
    "\n",
    "    test_set_size = int(np.round(0.2 * data_main.shape[0]))\n",
    "    train_set_size = data_main.shape[0]\n",
    "\n",
    "    # 将训练数据切割成多个批次\n",
    "    num_batches = train_set_size // batch_size\n",
    "    print(\"num_batches: \", num_batches, \" data_main.shape[0]: \", data_main.shape[0])\n",
    "\n",
    "\n",
    "    x_train = {\n",
    "        'main': np.array_split(data_main[:train_set_size, :-1, :], num_batches),\n",
    "        'even': np.array_split(data_even[:train_set_size, :-1, :], num_batches),\n",
    "        'loss': np.array_split(data_loss[:train_set_size, :-1, :], num_batches),\n",
    "        'result': np.array_split(data_result[:train_set_size, :-1, :], num_batches)\n",
    "    }\n",
    "    y_train = {\n",
    "        'main': np.array_split(data_main[:train_set_size, -1, :], num_batches),\n",
    "        'even': np.array_split(data_even[:train_set_size, -1, :], num_batches),\n",
    "        'loss': np.array_split(data_loss[:train_set_size, -1, :], num_batches),\n",
    "        'result': np.array_split(data_result[:train_set_size, -1, :], num_batches)\n",
    "    }\n",
    "\n",
    "    x_test = {\n",
    "        'main': np.array_split(data_main[train_set_size:, :-1, :], num_batches),\n",
    "        'even': np.array_split(data_even[train_set_size:, :-1, :], num_batches),\n",
    "        'loss': np.array_split(data_loss[train_set_size:, :-1, :], num_batches),\n",
    "        'result': np.array_split(data_result[train_set_size:, :-1, :], num_batches)\n",
    "    }\n",
    "    y_test = {\n",
    "        'main': np.array_split(data_main[train_set_size:, -1, :], num_batches),\n",
    "        'even': np.array_split(data_even[train_set_size:, -1, :], num_batches),\n",
    "        'loss': np.array_split(data_loss[train_set_size:, -1, :], num_batches),\n",
    "        'result': np.array_split(data_result[train_set_size:, -1, :], num_batches)\n",
    "    }\n",
    "    # print(\"x_train['main']: \", x_train['main'][1])\n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "\n",
    "lookback = 10 # choose sequence length\n",
    "x_train, y_train, x_test, y_test = split_data(dataMain, dataEven, dataLoss, dataResult, lookback, 490)\n",
    "\n",
    "# 打印批次的数量和每个批次的形状\n",
    "print('Number of training batches:', len(x_train['main']))\n",
    "if len(x_train['main']) > 0:\n",
    "    # print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
    "    # print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
    "\n",
    "    print('x_train.shape = ',x_train['main'][0].shape)\n",
    "    print('y_train.shape = ',y_train['main'][0].shape)\n",
    "    print('x_test.shape = ',x_test['main'][0].shape)\n",
    "    print('y_test.shape = ',y_test['main'][0].shape)\n",
    "    print('x_result.shape = ',y_train['result'][0].shape)\n",
    "\n",
    "# print(\"x_train: \", x_train)\n",
    "# print(\"y_train: \", y_train)\n",
    "\n",
    "# x_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train_batches]\n",
    "# y_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train_batches]\n",
    "import torch\n",
    "\n",
    "# 转换为字典\n",
    "train_tensors = {\n",
    "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['main']],\n",
    "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['even']],\n",
    "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['loss']],\n",
    "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['result']],\n",
    "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['main']],\n",
    "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['even']],\n",
    "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['loss']],\n",
    "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['result']]\n",
    "}\n",
    "\n",
    "test_tensors = {\n",
    "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['main']],\n",
    "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['even']],\n",
    "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['loss']],\n",
    "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['result']],\n",
    "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['main']],\n",
    "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['even']],\n",
    "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['loss']],\n",
    "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['result']]\n",
    "}\n",
    "\n",
    "for x_main_batch, x_even_batch, x_loss_batch, y_main_batch, y_even_batch, y_loss_batch, y_result_batch in zip(train_tensors['x_main'], train_tensors['x_even'], train_tensors['x_loss'], train_tensors['y_main'], train_tensors['y_even'], train_tensors['y_loss'], train_tensors['y_result']):\n",
    "\n",
    "    # 将数据移动到设备\n",
    "    x_main_batch = x_main_batch.to(device)\n",
    "    x_even_batch = x_even_batch.to(device)\n",
    "    x_loss_batch = x_loss_batch.to(device)\n",
    "    y_main_batch = y_main_batch.to(device)\n",
    "    y_even_batch = y_even_batch.to(device)\n",
    "    y_loss_batch = y_loss_batch.to(device)\n",
    "    y_result_batch = y_result_batch.to(device)\n",
    "    # print(\"ok: \")\n",
    "    # 替换前10个元素的内容为第11个元素的内容\n",
    "    if len(x_main_batch) > 10:  # 确保有足够的元素\n",
    "        x_main_batch[:10] = x_main_batch[10:11]\n",
    "    # print(\"x_main_batch: \", x_main_batch)\n",
    "    y_test_main_pred, cls_main = model(x_main_batch)\n",
    "    y_test_even_pred, cls_even = model(x_even_batch)\n",
    "    y_test_loss_pred, cls_loss = model(x_loss_batch)\n",
    "    tmp = torch.softmax(cls_main + cls_even + cls_loss, dim=1)\n",
    "    cls_to_draw = tmp\n",
    "# # make predictions\n",
    "# y_test_pred = model(x_test_tensor)\n",
    "# lstm = []\n",
    "# # invert predictions\n",
    "# y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())\n",
    "# # y_train = scaler.inverse_transform(y_train_lstm.detach().numpy())\n",
    "# y_test_main_pred = scalerMain.inverse_transform(y_test_main_pred.detach().numpy())\n",
    "# # y_test = scaler.inverse_transform(y_test_lstm_tensor.detach().numpy())\n",
    "\n",
    "# # calculate root mean squared error\n",
    "# trainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0]))\n",
    "# print('Train Score: %.2f RMSE' % (trainScore))\n",
    "# testScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0]))\n",
    "# print('Test Score: %.2f RMSE' % (testScore))\n",
    "# lstm.append(trainScore)\n",
    "# lstm.append(testScore)\n",
    "# lstm.append(training_time)\n",
    "\n",
    "predict_main = pd.DataFrame(scalerMain.inverse_transform(y_test_main_pred.detach().cpu().numpy()))\n",
    "# original_main = pd.DataFrame(scalerMain.inverse_transform(train_tensors['y_main'].detach().numpy()))\n",
    "\n",
    "predict_even = pd.DataFrame(scalerEven.inverse_transform(y_test_even_pred.detach().cpu().numpy()))\n",
    "# # original_even = pd.DataFrame(scalerEven.inverse_transform(train_tensors['y_even'].detach().numpy()))\n",
    "\n",
    "predict_loss = pd.DataFrame(scalerLoss.inverse_transform(y_test_loss_pred.detach().cpu().numpy()))\n",
    "\n",
    "# predict_cls = pd.DataFrame(scalerLoss.inverse_transform(cls_to_draw.detach().numpy()))\n",
    "# original_loss = pd.DataFrame(scalerLoss.inverse_transform(train_tensors['y_loss'].detach().numpy()))\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "# ax = sns.lineplot(x = original_main.index, y = original_main[0], label=\"Data Main\", color='royalblue')\n",
    "ax = sns.lineplot(x = predict_main.index, y = predict_main[0], label=\"Training Prediction Main(LSTM)\", color='tomato')\n",
    "\n",
    "# ax = sns.lineplot(x = original_even.index, y = original_even[0], label=\"Data Even\", color='red')\n",
    "ax = sns.lineplot(x = predict_even.index, y = predict_even[0], label=\"Training Prediction Even(LSTM)\", color='#FF6347')\n",
    "\n",
    "# # ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
    "ax = sns.lineplot(x = predict_loss.index, y = predict_loss[0], label=\"Training Prediction Loss(LSTM)\", color='#F31347')\n",
    "\n",
    "# # ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
    "# ax = sns.lineplot(x = predict_cls.index, y = predict_cls[0], label=\"Training Prediction Class(LSTM)\", color='#a31347')\n",
    "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
    "ax.set_xlabel(\"Days\", size = 14)\n",
    "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
    "ax.set_xticklabels('', size=10)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "ax = sns.lineplot(data=hist, color='royalblue')\n",
    "ax.set_xlabel(\"Epoch\", size = 14)\n",
    "ax.set_ylabel(\"Loss\", size = 14)\n",
    "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLEZWHXwESPV"
   },
   "outputs": [],
   "source": [
    "#@title 绘制图像\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 类别标签\n",
    "categories = ['Class 0', 'Class 1', 'Class 2']\n",
    "\n",
    "# 绘制每一行的柱形图\n",
    "for i in range(cls_to_draw.size(0)):\n",
    "    plt.figure()\n",
    "    plt.bar(categories, cls_to_draw[i].detach().cpu().numpy(), color=['blue', 'orange', 'green'])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(f'Class Probability Distribution for Sample {i + 1}')\n",
    "    plt.show()\n",
    "    if i > 5:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B6pm3DN8D5-F"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 类别标签\n",
    "categories = ['Class 0', 'Class 1', 'Class 2']\n",
    "\n",
    "# 绘制每一行的柱形图\n",
    "for i in range(cls_to_draw.size(0)):\n",
    "    plt.figure()\n",
    "    plt.bar(categories, cls_to_draw[i].detach().numpy(), color=['blue', 'orange', 'green'])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(f'Class Probability Distribution for Sample {i + 1}')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePRLskNmhp_y"
   },
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(price)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[lookback:len(y_train_pred)+lookback, :] = y_train_pred\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(price)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(y_train_pred)+lookback-1:len(price)-1, :] = y_test_pred\n",
    "\n",
    "original = scaler.inverse_transform(price['Close'].values.reshape(-1,1))\n",
    "\n",
    "predictions = np.append(trainPredictPlot, testPredictPlot, axis=1)\n",
    "predictions = np.append(predictions, original, axis=1)\n",
    "result = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tH05FABhp_z"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[0],\n",
    "                    mode='lines',\n",
    "                    name='Train prediction')))\n",
    "fig.add_trace(go.Scatter(x=result.index, y=result[1],\n",
    "                    mode='lines',\n",
    "                    name='Test prediction'))\n",
    "fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[2],\n",
    "                    mode='lines',\n",
    "                    name='Actual Value')))\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        showline=True,\n",
    "        showgrid=True,\n",
    "        showticklabels=False,\n",
    "        linecolor='white',\n",
    "        linewidth=2\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title_text='Close (USD)',\n",
    "        titlefont=dict(\n",
    "            family='Rockwell',\n",
    "            size=12,\n",
    "            color='white',\n",
    "        ),\n",
    "        showline=True,\n",
    "        showgrid=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='white',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Rockwell',\n",
    "            size=12,\n",
    "            color='white',\n",
    "        ),\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    template = 'plotly_dark'\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "annotations = []\n",
    "annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n",
    "                              xanchor='left', yanchor='bottom',\n",
    "                              text='Results (LSTM)',\n",
    "                              font=dict(family='Rockwell',\n",
    "                                        size=26,\n",
    "                                        color='white'),\n",
    "                              showarrow=False))\n",
    "fig.update_layout(annotations=annotations)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ibf2gv4Qhp_z"
   },
   "outputs": [],
   "source": [
    "!pip install chart-studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXr3BZs6hp_z"
   },
   "outputs": [],
   "source": [
    "# import chart_studio.plotly as py\n",
    "# import chart_studio\n",
    "\n",
    "# chart_studio.tools.set_credentials_file(username='rodolfo_saldanha', api_key='zWJIVWJs23wfiAp516Mh')\n",
    "# py.iplot(fig, filename='stock_prediction_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjpe8i-Php_z"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, (hn) = self.gru(x, (h0.detach()))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfWFqY2nhp_0"
   },
   "outputs": [],
   "source": [
    "model = GRU(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhuqmiqMhp_0"
   },
   "outputs": [],
   "source": [
    "hist = np.zeros(num_epochs)\n",
    "start_time = time.time()\n",
    "gru = []\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    y_train_pred = model(x_train)\n",
    "\n",
    "    loss = criterion(y_train_pred, y_train_gru)\n",
    "    print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "    hist[t] = loss.item()\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "training_time = time.time()-start_time\n",
    "print(\"Training time: {}\".format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeuZe2BRhp_0"
   },
   "outputs": [],
   "source": [
    "predict = pd.DataFrame(scaler.inverse_transform(y_train_pred.detach().numpy()))\n",
    "original = pd.DataFrame(scaler.inverse_transform(y_train_gru.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lf5fFYRWhp_0"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "ax = sns.lineplot(x = original.index, y = original[0], label=\"Data\", color='royalblue')\n",
    "ax = sns.lineplot(x = predict.index, y = predict[0], label=\"Training Prediction (GRU)\", color='tomato')\n",
    "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
    "ax.set_xlabel(\"Days\", size = 14)\n",
    "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
    "ax.set_xticklabels('', size=10)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "ax = sns.lineplot(data=hist, color='royalblue')\n",
    "ax.set_xlabel(\"Epoch\", size = 14)\n",
    "ax.set_ylabel(\"Loss\", size = 14)\n",
    "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
    "fig.set_figheight(6)\n",
    "fig.set_figwidth(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVjVWgzThp_1"
   },
   "outputs": [],
   "source": [
    "import math, time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# make predictions\n",
    "y_test_pred = model(x_test)\n",
    "\n",
    "# invert predictions\n",
    "y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())\n",
    "y_train = scaler.inverse_transform(y_train_gru.detach().numpy())\n",
    "y_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy())\n",
    "y_test = scaler.inverse_transform(y_test_gru.detach().numpy())\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "gru.append(trainScore)\n",
    "gru.append(testScore)\n",
    "gru.append(training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0SJoPGthp_1"
   },
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(price)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[lookback:len(y_train_pred)+lookback, :] = y_train_pred\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(price)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(y_train_pred)+lookback-1:len(price)-1, :] = y_test_pred\n",
    "\n",
    "original = scaler.inverse_transform(price['Close'].values.reshape(-1,1))\n",
    "\n",
    "predictions = np.append(trainPredictPlot, testPredictPlot, axis=1)\n",
    "predictions = np.append(predictions, original, axis=1)\n",
    "result = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHNND6KThp_1"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[0],\n",
    "                    mode='lines',\n",
    "                    name='Train prediction')))\n",
    "fig.add_trace(go.Scatter(x=result.index, y=result[1],\n",
    "                    mode='lines',\n",
    "                    name='Test prediction'))\n",
    "fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[2],\n",
    "                    mode='lines',\n",
    "                    name='Actual Value')))\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        showline=True,\n",
    "        showgrid=True,\n",
    "        showticklabels=False,\n",
    "        linecolor='white',\n",
    "        linewidth=2\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title_text='Close (USD)',\n",
    "        titlefont=dict(\n",
    "            family='Rockwell',\n",
    "            size=12,\n",
    "            color='white',\n",
    "        ),\n",
    "        showline=True,\n",
    "        showgrid=True,\n",
    "        showticklabels=True,\n",
    "        linecolor='white',\n",
    "        linewidth=2,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            family='Rockwell',\n",
    "            size=12,\n",
    "            color='white',\n",
    "        ),\n",
    "    ),\n",
    "    showlegend=True,\n",
    "    template = 'plotly_dark'\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "annotations = []\n",
    "annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n",
    "                              xanchor='left', yanchor='bottom',\n",
    "                              text='Results (GRU)',\n",
    "                              font=dict(family='Rockwell',\n",
    "                                        size=26,\n",
    "                                        color='white'),\n",
    "                              showarrow=False))\n",
    "fig.update_layout(annotations=annotations)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrpcQQ68hp_1"
   },
   "outputs": [],
   "source": [
    "lstm = pd.DataFrame(lstm, columns=['LSTM'])\n",
    "gru = pd.DataFrame(gru, columns=['GRU'])\n",
    "result = pd.concat([lstm, gru], axis=1, join='inner')\n",
    "result.index = ['Train RMSE', 'Test RMSE', 'Train Time']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5k92uuQ4hp_2"
   },
   "outputs": [],
   "source": [
    "py.iplot(fig, filename='stock_prediction_gru')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
