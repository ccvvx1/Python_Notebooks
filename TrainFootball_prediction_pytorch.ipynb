{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPaj2y19XmCg"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6anRp8jeuTf"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title 链接Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtGaXeHCtsnG"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/drive/MyDrive/ftTrainDataDetail\n",
        "!mkdir /content/drive/MyDrive/ftTrainDataTest\n",
        "# !rm -rf /content/drive/MyDrive/ftTrainDataDetail/*\n",
        "!unzip -o /content/drive/MyDrive/ftTrainDataTrain.zip -d /content/drive/MyDrive/ftTrainDataDetail\n",
        "!unzip -o /content/drive/MyDrive/ftTrainDataTest.zip -d /content/drive/MyDrive/ftTrainDataTest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KPXkrGlTxOV"
      },
      "source": [
        "## 获取数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "CbHXI5I4hp_o"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "#@title 拼接数据\n",
        "\n",
        "# filepath = '/kaggle/input/stock-time-series-20050101-to-20171231/AMZN_2006-01-01_to_2018-01-01.csv'\n",
        "# filepath = '/kaggle/input/amazon.csv'\n",
        "import pandas as pd\n",
        "\n",
        "# 获取/content/hello目录下所有的CSV文件\n",
        "file_paths = [os.path.join('/content/drive/MyDrive/ftTrainDataDetail', f) for f in os.listdir('/content/drive/MyDrive/ftTrainDataDetail') if f.endswith('.csv')]\n",
        "\n",
        "dataframes = []\n",
        "tgtLength = 500\n",
        "\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path, encoding='latin1')\n",
        "    df = df[::-1]  # 反转 DataFrame 的顺序\n",
        "\n",
        "    if len(df) == 0:\n",
        "        continue\n",
        "    # 检查文件大小\n",
        "    if len(df) < tgtLength:\n",
        "        # 计算需要重复的行数\n",
        "        num_rows_to_add = tgtLength - len(df)\n",
        "        last_row = df.iloc[-1:]  # 获取最后一行\n",
        "        # 重复最后一行\n",
        "        repeated_rows = pd.concat([last_row] * num_rows_to_add, ignore_index=True)\n",
        "        df = pd.concat([df, repeated_rows], ignore_index=True)\n",
        "        # 创建补充的行，填充为零\n",
        "        # zero_rows = pd.DataFrame(0, index=range(num_rows_to_add), columns=df.columns)\n",
        "        # df = pd.concat([df, zero_rows], ignore_index=True)\n",
        "    else:\n",
        "        df = df.iloc[:tgtLength]\n",
        "\n",
        "    dataframes.append(df)\n",
        "\n",
        "# 合并所有数据\n",
        "data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "\n",
        "# 保存为 CSV 文件\n",
        "data.to_csv('hello.csv', index=False)\n",
        "\n",
        "#@title 数据去除特殊符号\n",
        "\n",
        "dataMain = data[['Main']]\n",
        "dataEven = data[['Even']]\n",
        "dataLoss = data[['Loss']]\n",
        "dataResult = data[['Result']]\n",
        "dataResultHome = data[['ResultHome']]\n",
        "dataResultAway = data[['ResultAway']]\n",
        "\n",
        "dataMain['Main'] = dataMain['Main'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "dataEven['Even'] = dataEven['Even'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "dataLoss['Loss'] = dataLoss['Loss'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "\n",
        "\n",
        "# # 检查非数字字符串\n",
        "# def clean_column(column):\n",
        "#     # 移除特定字符\n",
        "#     column = column.str.replace('↑', '').str.replace('↓', '')\n",
        "#     # 检查无法转换的值\n",
        "#     invalid_values = column[~column.str.replace('.', '', regex=False).str.isnumeric()]\n",
        "#     if not invalid_values.empty:\n",
        "#         print(\"无效值:\", invalid_values)\n",
        "#     # 转换为 float\n",
        "#     return column.astype(float, errors='coerce')\n",
        "\n",
        "# # 清理并转换\n",
        "# dataMain['Main'] = clean_column(dataMain['Main'])\n",
        "# dataMain['Even'] = clean_column(dataMain['Even'])\n",
        "# dataMain['Loss'] = clean_column(dataMain['Loss'])\n",
        "\n",
        "\n",
        "#@title 数据归一化\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scalerMain = MinMaxScaler(feature_range=(-1, 1))\n",
        "scalerEven = MinMaxScaler(feature_range=(-1, 1))\n",
        "scalerLoss = MinMaxScaler(feature_range=(-1, 1))\n",
        "scalerResult = MinMaxScaler(feature_range=(-1, 1))\n",
        "dataMain['Main'] = scalerMain.fit_transform(dataMain['Main'].values.reshape(-1,1))\n",
        "dataEven['Even'] = scalerEven.fit_transform(dataEven['Even'].values.reshape(-1,1))\n",
        "dataLoss['Loss'] = scalerLoss.fit_transform(dataLoss['Loss'].values.reshape(-1,1))\n",
        "dataResult['Result'] = dataResult['Result']\n",
        "dataResultHome['ResultHome'] = dataResultHome['ResultHome']\n",
        "dataResultAway['ResultAway'] = dataResultAway['ResultAway']\n",
        "print(dataMain['Main'] .shape)\n",
        "# dataResultHome['ResultHome']\n",
        "\n",
        "has_less_than_0 = (dataResultAway['ResultAway'] < 0).any()\n",
        "\n",
        "if has_less_than_0:\n",
        "    print(\"有小于 0 的数值，具体内容如下：\")\n",
        "    # 打印所有小于 0 的值\n",
        "    negative_values = dataResultAway[dataResultAway['ResultAway'] < 0]['ResultAway']\n",
        "    print(negative_values.tolist())  # 转换为列表并打印\n",
        "\n",
        "    # 将所有小于 0 的值变成 0\n",
        "    dataResultAway.loc[dataResultAway['ResultAway'] < 0, 'ResultAway'] = 0\n",
        "else:\n",
        "    print(\"没有小于 0 的数值\")\n",
        "\n",
        "has_less_than_0 = (dataResultAway['ResultAway'] < 0).any()\n",
        "\n",
        "if has_less_than_0:\n",
        "    print(\"有小于 0 的数值，具体内容如下：\")\n",
        "else:\n",
        "    print(\"没有小于 0 的数值\")\n",
        "# dataResultAway['ResultAway']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "has_less_than_0 = (dataResultHome['ResultHome'] < 0).any()\n",
        "\n",
        "if has_less_than_0:\n",
        "    print(\"有小于 0 的数值，具体内容如下：\")\n",
        "    # 打印所有小于 0 的值\n",
        "    negative_values = dataResultHome[dataResultHome['ResultHome'] < 0]['ResultHome']\n",
        "    print(negative_values.tolist())  # 转换为列表并打印\n",
        "\n",
        "    # 将所有小于 0 的值变成 0\n",
        "    dataResultHome.loc[dataResultHome['ResultHome'] < 0, 'ResultHome'] = 0\n",
        "else:\n",
        "    print(\"没有小于 0 的数值\")\n",
        "\n",
        "has_less_than_0 = (dataResultHome['ResultHome']< 0).any()\n",
        "\n",
        "if has_less_than_0:\n",
        "    print(\"有小于 0 的数值，具体内容如下：\")\n",
        "else:\n",
        "    print(\"没有小于 0 的数值\")\n",
        "dataResultHome['ResultHome']\n",
        "# 检查是否有非整数或浮点数的值\n",
        "# has_non_integer_or_float = dataResultHome['ResultHome'].apply(lambda x: not isinstance(x, (int, float))).any()\n",
        "\n",
        "# if has_non_integer_or_float:\n",
        "#     print(\"有非整数或浮点数的值\")\n",
        "# else:\n",
        "#     print(\"没有非整数或浮点数的值\")\n",
        "\n",
        "# def scale_in_chunks(data, chunk_size=500):\n",
        "#     scalers = []\n",
        "#     scaled_data = []\n",
        "\n",
        "#     # 将数据分成若干块\n",
        "#     data_chunks = np.array_split(data, np.arange(chunk_size, len(data), chunk_size))\n",
        "\n",
        "#     for chunk in data_chunks:\n",
        "#         scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "#         scaled_chunk = scaler.fit_transform(chunk.values.reshape(-1, 1))\n",
        "#         scalers.append(scaler)  # 如果需要在后续反向转换时使用\n",
        "#         scaled_data.append(scaled_chunk)\n",
        "\n",
        "#     return np.vstack(scaled_data), scalers\n",
        "\n",
        "# # 对每个数据集进行分块缩放\n",
        "# dataMain['Main'], scalers_main = scale_in_chunks(dataMain['Main'])\n",
        "# dataEven['Even'], scalers_even = scale_in_chunks(dataEven['Even'])\n",
        "# dataLoss['Loss'], scalers_loss = scale_in_chunks(dataLoss['Loss'])\n",
        "# print(dataMain['Main'], dataEven['Even'], dataLoss['Loss'])\n",
        "# dataResult['Result'] 保持原样"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ehyV_uihp_s"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# sns.set_style(\"darkgrid\")\n",
        "# plt.figure(figsize = (15,9))\n",
        "# plt.plot(data[['Main']])\n",
        "# plt.plot(data[['Even']])\n",
        "# plt.plot(data[['Loss']])\n",
        "# plt.xticks(range(0,data.shape[0],500),data['Date'].loc[::500],rotation=45)\n",
        "# plt.title(\"Amazon Stock Price\",fontsize=18, fontweight='bold')\n",
        "# plt.xlabel('Date',fontsize=18)\n",
        "# plt.ylabel('Close Price (USD)',fontsize=18)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrjH2cFEhp_s"
      },
      "source": [
        "## 数据处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fx4S1tcNhp_x",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 训练模型\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_dim = 1\n",
        "hidden_dim = 32\n",
        "num_layers = 2\n",
        "output_dim = 1\n",
        "output_cls_dim = 3\n",
        "num_epochs = 300\n",
        "batch_size = 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def split_data(dataMain, dataEven, dataLoss, dataResult, lookback, batch_size = 20):\n",
        "    data_raw_main = dataMain.to_numpy()\n",
        "    data_raw_even = dataEven.to_numpy()\n",
        "    data_raw_loss = dataLoss.to_numpy()\n",
        "    data_raw_result = dataResult.to_numpy()\n",
        "    data_main = []\n",
        "    data_even = []\n",
        "    data_loss = []\n",
        "    data_result = []\n",
        "\n",
        "    # create all possible sequences of length lookback\n",
        "    for index in range(len(data_raw_main) - lookback):\n",
        "        data_main.append(data_raw_main[index: index + lookback])\n",
        "    for index in range(len(data_raw_even) - lookback):\n",
        "        data_even.append(data_raw_even[index: index + lookback])\n",
        "    for index in range(len(data_raw_loss) - lookback):\n",
        "        data_loss.append(data_raw_loss[index: index + lookback])\n",
        "    for index in range(len(data_raw_result) - lookback):\n",
        "        data_result.append(data_raw_result[index: index + lookback])\n",
        "\n",
        "    data_main = np.array(data_main)\n",
        "    data_even = np.array(data_even)\n",
        "    data_loss = np.array(data_loss)\n",
        "    data_result = np.array(data_result)\n",
        "\n",
        "    test_set_size = int(np.round(0.2 * data_main.shape[0]))\n",
        "    train_set_size = data_main.shape[0]\n",
        "\n",
        "    # 将训练数据切割成多个批次\n",
        "    num_batches = train_set_size // batch_size\n",
        "    print(\"num_batches: \", num_batches, \" data_main.shape[0]: \", data_main.shape[0])\n",
        "\n",
        "\n",
        "    x_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, :-1, :], num_batches)\n",
        "    }\n",
        "    y_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, -1, :], num_batches)\n",
        "    }\n",
        "\n",
        "    x_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, :-1, :], num_batches)\n",
        "    }\n",
        "    y_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, -1, :], num_batches)\n",
        "    }\n",
        "    # print(\"x_train['main']: \", x_train['main'][1])\n",
        "    return [x_train, y_train, x_test, y_test]\n",
        "\n",
        "\n",
        "lookback = 10 # choose sequence length\n",
        "x_train, y_train, x_test, y_test = split_data(dataMain, dataEven, dataLoss, dataResult, lookback, 495)\n",
        "# x_train_batches, y_train_batches, x_test, y_test = split_data(price, lookback=20, batch_size=32)\n",
        "\n",
        "# # 打印批次的数量和每个批次的形状\n",
        "# print('Number of training batches:', len(x_train_batches))\n",
        "# if len(x_train_batches) > 0:\n",
        "#     print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "#     print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "# print('x_test.shape = ', x_test.shape)\n",
        "# print('y_test.shape = ', y_test.shape)\n",
        "\n",
        "# 打印批次的数量和每个批次的形状\n",
        "print('Number of training batches:', len(x_train['main']))\n",
        "if len(x_train['main']) > 0:\n",
        "    # print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "    # print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "    print('x_train.shape = ',x_train['main'][0].shape)\n",
        "    print('y_train.shape = ',y_train['main'][0].shape)\n",
        "    print('x_test.shape = ',x_test['main'][0].shape)\n",
        "    print('y_test.shape = ',y_test['main'][0].shape)\n",
        "    print('x_result.shape = ',y_train['result'][0].shape)\n",
        "\n",
        "# print(\"x_train: \", x_train)\n",
        "# print(\"y_train: \", y_train)\n",
        "\n",
        "# x_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train_batches]\n",
        "# y_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train_batches]\n",
        "import torch\n",
        "\n",
        "# 转换为字典\n",
        "train_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['result']]\n",
        "}\n",
        "\n",
        "test_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['result']]\n",
        "}\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.fcCls = nn.Linear(hidden_dim, output_cls_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "        out1 = self.fc(out[:, -1, :])\n",
        "\n",
        "        # 通过全连接层\n",
        "        outCls = self.fcCls(out[:, -1, :])\n",
        "        # 应用 softmax 激活\n",
        "        outCls = torch.softmax(outCls, dim=1)\n",
        "\n",
        "        return out1, outCls\n",
        "\n",
        "\n",
        "\n",
        "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "\n",
        "model_path = '/content/drive/MyDrive/ft_model_weights.pth'\n",
        "\n",
        "# 判断模型文件是否存在\n",
        "if os.path.exists(model_path):\n",
        "    # 创建模型实例\n",
        "    # model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
        "\n",
        "    # 加载模型参数\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # 移动模型到指定设备\n",
        "    model.to(device)\n",
        "    print(\"Model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Model file does not exist.\")\n",
        "\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterionCls = nn.CrossEntropyLoss()\n",
        "# optimizerCls = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "from re import X\n",
        "import time\n",
        "\n",
        "hist = np.zeros(num_epochs)\n",
        "start_time = time.time()\n",
        "lstm = []\n",
        "\n",
        "y_train_pred_main = None\n",
        "y_train_pred_even = None\n",
        "y_train_pred_loss = None\n",
        "cls_to_draw = None\n",
        "\n",
        "for t in range(num_epochs):\n",
        "\n",
        "    for x_main_batch, x_even_batch, x_loss_batch, y_main_batch, y_even_batch, y_loss_batch, y_result_batch in zip(train_tensors['x_main'], train_tensors['x_even'], train_tensors['x_loss'], train_tensors['y_main'], train_tensors['y_even'], train_tensors['y_loss'], train_tensors['y_result']):\n",
        "\n",
        "        x_main_batch = x_main_batch.to(device)\n",
        "        x_even_batch = x_even_batch.to(device)\n",
        "        x_loss_batch = x_loss_batch.to(device)\n",
        "        y_main_batch = y_main_batch.to(device)\n",
        "        y_even_batch = y_even_batch.to(device)\n",
        "        y_loss_batch = y_loss_batch.to(device)\n",
        "        y_result_batch = y_result_batch.to(device)\n",
        "        # print(\"ok: \")\n",
        "        # 替换前10个元素的内容为第11个元素的内容\n",
        "        if len(x_main_batch) > 10:  # 确保有足够的元素\n",
        "            x_main_batch[:10] = x_main_batch[10:11]\n",
        "        # print(\"x_main_batch: \", x_main_batch)\n",
        "        y_train_pred_main, cls_main = model(x_main_batch)\n",
        "        y_train_pred_even, cls_even = model(x_even_batch)\n",
        "        y_train_pred_loss, cls_loss = model(x_loss_batch)\n",
        "\n",
        "        loss_main = criterion(y_train_pred_main, y_main_batch)\n",
        "        loss_even = criterion(y_train_pred_even, y_even_batch)\n",
        "        loss_loss = criterion(y_train_pred_loss, y_loss_batch)\n",
        "\n",
        "        tmp = torch.softmax(cls_main + cls_even + cls_loss, dim=1)\n",
        "        cls_to_draw = tmp\n",
        "\n",
        "        loss_result = criterionCls(tmp, y_result_batch.squeeze().long())\n",
        "        loss = loss_main + loss_even + loss_loss + loss_result\n",
        "\n",
        "        # print(\"Epoch \", t, \"MSE: \", loss.item())\n",
        "        hist[t] = loss.item()\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "    print(\"Epoch \", t, \"MSE: \", loss.item(), \"cls_main + cls_even + cls_loss: \", tmp)\n",
        "\n",
        "training_time = time.time()-start_time\n",
        "print(\"Training time: {}\".format(training_time))\n",
        "\n",
        "\n",
        "predict_main = pd.DataFrame(scalerMain.inverse_transform(y_train_pred_main.detach().cpu().numpy()))\n",
        "# original_main = pd.DataFrame(scalerMain.inverse_transform(train_tensors['y_main'].detach().numpy()))\n",
        "\n",
        "predict_even = pd.DataFrame(scalerEven.inverse_transform(y_train_pred_even.detach().cpu().numpy()))\n",
        "# original_even = pd.DataFrame(scalerEven.inverse_transform(train_tensors['y_even'].detach().numpy()))\n",
        "\n",
        "predict_loss = pd.DataFrame(scalerLoss.inverse_transform(y_train_pred_loss.detach().cpu().numpy()))\n",
        "\n",
        "predict_cls = pd.DataFrame(scalerLoss.inverse_transform(cls_to_draw.detach().cpu().numpy()))\n",
        "# original_loss = pd.DataFrame(scalerLoss.inverse_transform(train_tensors['y_loss'].detach().numpy()))\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# ax = sns.lineplot(x = original_main.index, y = original_main[0], label=\"Data Main\", color='royalblue')\n",
        "ax = sns.lineplot(x = predict_main.index, y = predict_main[0], label=\"Training Prediction Main(LSTM)\", color='tomato')\n",
        "\n",
        "# ax = sns.lineplot(x = original_even.index, y = original_even[0], label=\"Data Even\", color='red')\n",
        "ax = sns.lineplot(x = predict_even.index, y = predict_even[0], label=\"Training Prediction Even(LSTM)\", color='#FF6347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_loss.index, y = predict_loss[0], label=\"Training Prediction Loss(LSTM)\", color='#F31347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_cls.index, y = predict_cls[0], label=\"Training Prediction Class(LSTM)\", color='#a31347')\n",
        "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
        "ax.set_xlabel(\"Days\", size = 14)\n",
        "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
        "ax.set_xticklabels('', size=10)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "ax = sns.lineplot(data=hist, color='royalblue')\n",
        "ax.set_xlabel(\"Epoch\", size = 14)\n",
        "ax.set_ylabel(\"Loss\", size = 14)\n",
        "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(16)\n",
        "\n",
        "# 保存模型的状态字典\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_weights.pth')\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_weights_bk.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbepwiwHXmCo"
      },
      "source": [
        "## GRU模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOV3wLQpXmCp"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/output'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "#@title 获取数据\n",
        "\n",
        "# filepath = '/kaggle/input/stock-time-series-20050101-to-20171231/AMZN_2006-01-01_to_2018-01-01.csv'\n",
        "# filepath = '/kaggle/input/amazon.csv'#@title 训练模型\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_dim = 1\n",
        "hidden_dim = 32\n",
        "num_layers = 2\n",
        "output_dim = 1\n",
        "output_cls_dim = 3\n",
        "num_epochs = 100\n",
        "batch_size = 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def split_data(dataMain, dataEven, dataLoss, dataResult, lookback, batch_size = 20):\n",
        "    data_raw_main = dataMain.to_numpy()\n",
        "    data_raw_even = dataEven.to_numpy()\n",
        "    data_raw_loss = dataLoss.to_numpy()\n",
        "    data_raw_result = dataResult.to_numpy()\n",
        "    data_main = []\n",
        "    data_even = []\n",
        "    data_loss = []\n",
        "    data_result = []\n",
        "\n",
        "    # create all possible sequences of length lookback\n",
        "    for index in range(len(data_raw_main) - lookback):\n",
        "        data_main.append(data_raw_main[index: index + lookback])\n",
        "    for index in range(len(data_raw_even) - lookback):\n",
        "        data_even.append(data_raw_even[index: index + lookback])\n",
        "    for index in range(len(data_raw_loss) - lookback):\n",
        "        data_loss.append(data_raw_loss[index: index + lookback])\n",
        "    for index in range(len(data_raw_result) - lookback):\n",
        "        data_result.append(data_raw_result[index: index + lookback])\n",
        "\n",
        "    data_main = np.array(data_main)\n",
        "    data_even = np.array(data_even)\n",
        "    data_loss = np.array(data_loss)\n",
        "    data_result = np.array(data_result)\n",
        "\n",
        "    test_set_size = int(np.round(0.2 * data_main.shape[0]))\n",
        "    train_set_size = data_main.shape[0]\n",
        "\n",
        "    # 将训练数据切割成多个批次\n",
        "    num_batches = train_set_size // batch_size\n",
        "    print(\"num_batches: \", num_batches, \" data_main.shape[0]: \", data_main.shape[0])\n",
        "\n",
        "\n",
        "    x_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, :-1, :], num_batches)\n",
        "    }\n",
        "    y_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, -1, :], num_batches)\n",
        "    }\n",
        "\n",
        "    x_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, :-1, :], num_batches)\n",
        "    }\n",
        "    y_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, -1, :], num_batches)\n",
        "    }\n",
        "    # print(\"x_train['main']: \", x_train['main'][1])\n",
        "    return [x_train, y_train, x_test, y_test]\n",
        "\n",
        "\n",
        "lookback = 10 # choose sequence length\n",
        "x_train, y_train, x_test, y_test = split_data(dataMain, dataEven, dataLoss, dataResult, lookback, 495)\n",
        "# x_train_batches, y_train_batches, x_test, y_test = split_data(price, lookback=20, batch_size=32)\n",
        "\n",
        "# # 打印批次的数量和每个批次的形状\n",
        "# print('Number of training batches:', len(x_train_batches))\n",
        "# if len(x_train_batches) > 0:\n",
        "#     print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "#     print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "# print('x_test.shape = ', x_test.shape)\n",
        "# print('y_test.shape = ', y_test.shape)\n",
        "\n",
        "# 打印批次的数量和每个批次的形状\n",
        "print('Number of training batches:', len(x_train['main']))\n",
        "if len(x_train['main']) > 0:\n",
        "    # print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "    # print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "    print('x_train.shape = ',x_train['main'][0].shape)\n",
        "    print('y_train.shape = ',y_train['main'][0].shape)\n",
        "    print('x_test.shape = ',x_test['main'][0].shape)\n",
        "    print('y_test.shape = ',y_test['main'][0].shape)\n",
        "    print('x_result.shape = ',y_train['result'][0].shape)\n",
        "\n",
        "# print(\"x_train: \", x_train)\n",
        "# print(\"y_train: \", y_train)\n",
        "\n",
        "# x_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train_batches]\n",
        "# y_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train_batches]\n",
        "import torch\n",
        "\n",
        "# 转换为字典\n",
        "train_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['result']]\n",
        "}\n",
        "\n",
        "test_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['result']]\n",
        "}\n",
        "\n",
        "\n",
        "# class LSTM(nn.Module):\n",
        "#     def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "#         super(LSTM, self).__init__()\n",
        "#         self.hidden_dim = hidden_dim\n",
        "#         self.num_layers = num_layers\n",
        "\n",
        "#         self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "#         self.fcCls = nn.Linear(hidden_dim, output_cls_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "#         out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "#         out1 = self.fc(out[:, -1, :])\n",
        "\n",
        "#         # 通过全连接层\n",
        "#         outCls = self.fcCls(out[:, -1, :])\n",
        "#         # 应用 softmax 激活\n",
        "#         outCls = torch.softmax(outCls, dim=1)\n",
        "\n",
        "#         return out1, outCls\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.fcCls = nn.Linear(hidden_dim, output_cls_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "        out, (hn) = self.gru(x, (h0.detach()))\n",
        "        out1 = self.fc(out[:, -1, :])\n",
        "        # 通过全连接层\n",
        "        outCls = self.fcCls(out[:, -1, :])\n",
        "        # 应用 softmax 激活\n",
        "        outCls = torch.softmax(outCls, dim=1)\n",
        "\n",
        "        return out1, outCls\n",
        "\n",
        "\n",
        "\n",
        "# model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "\n",
        "model = GRU(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "# criterion = torch.nn.MSELoss(reduction='mean')\n",
        "# optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "model_path = '/content/drive/MyDrive/ft_gru_model_weights.pth'\n",
        "\n",
        "# 判断模型文件是否存在\n",
        "if os.path.exists(model_path):\n",
        "    # 创建模型实例\n",
        "    # model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
        "\n",
        "    # 加载模型参数\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # 移动模型到指定设备\n",
        "    model.to(device)\n",
        "    print(\"Model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Model file does not exist.\")\n",
        "\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterionCls = nn.CrossEntropyLoss()\n",
        "# optimizerCls = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "from re import X\n",
        "import time\n",
        "\n",
        "hist = np.zeros(num_epochs)\n",
        "start_time = time.time()\n",
        "lstm = []\n",
        "\n",
        "y_train_pred_main = None\n",
        "y_train_pred_even = None\n",
        "y_train_pred_loss = None\n",
        "cls_to_draw = None\n",
        "\n",
        "for t in range(num_epochs):\n",
        "\n",
        "    for x_main_batch, x_even_batch, x_loss_batch, y_main_batch, y_even_batch, y_loss_batch, y_result_batch in zip(train_tensors['x_main'], train_tensors['x_even'], train_tensors['x_loss'], train_tensors['y_main'], train_tensors['y_even'], train_tensors['y_loss'], train_tensors['y_result']):\n",
        "\n",
        "        x_main_batch = x_main_batch.to(device)\n",
        "        x_even_batch = x_even_batch.to(device)\n",
        "        x_loss_batch = x_loss_batch.to(device)\n",
        "        y_main_batch = y_main_batch.to(device)\n",
        "        y_even_batch = y_even_batch.to(device)\n",
        "        y_loss_batch = y_loss_batch.to(device)\n",
        "        y_result_batch = y_result_batch.to(device)\n",
        "        # print(\"ok: \")\n",
        "        # 替换前10个元素的内容为第11个元素的内容\n",
        "        if len(x_main_batch) > 10:  # 确保有足够的元素\n",
        "            x_main_batch[:10] = x_main_batch[10:11]\n",
        "        # print(\"x_main_batch: \", x_main_batch)\n",
        "        y_train_pred_main, cls_main = model(x_main_batch)\n",
        "        y_train_pred_even, cls_even = model(x_even_batch)\n",
        "        y_train_pred_loss, cls_loss = model(x_loss_batch)\n",
        "\n",
        "        loss_main = criterion(y_train_pred_main, y_main_batch)\n",
        "        loss_even = criterion(y_train_pred_even, y_even_batch)\n",
        "        loss_loss = criterion(y_train_pred_loss, y_loss_batch)\n",
        "\n",
        "        tmp = torch.softmax(cls_main + cls_even + cls_loss, dim=1)\n",
        "        cls_to_draw = tmp\n",
        "\n",
        "        loss_result = criterionCls(tmp, y_result_batch.squeeze().long())\n",
        "        loss = loss_main + loss_even + loss_loss + loss_result\n",
        "\n",
        "        # print(\"Epoch \", t, \"MSE: \", loss.item())\n",
        "        hist[t] = loss.item()\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "    print(\"Epoch \", t, \"MSE: \", loss.item(), \"cls_main + cls_even + cls_loss: \", tmp)\n",
        "\n",
        "training_time = time.time()-start_time\n",
        "print(\"Training time: {}\".format(training_time))\n",
        "\n",
        "\n",
        "predict_main = pd.DataFrame(scalerMain.inverse_transform(y_train_pred_main.detach().cpu().numpy()))\n",
        "# original_main = pd.DataFrame(scalerMain.inverse_transform(train_tensors['y_main'].detach().numpy()))\n",
        "\n",
        "predict_even = pd.DataFrame(scalerEven.inverse_transform(y_train_pred_even.detach().cpu().numpy()))\n",
        "# original_even = pd.DataFrame(scalerEven.inverse_transform(train_tensors['y_even'].detach().numpy()))\n",
        "\n",
        "predict_loss = pd.DataFrame(scalerLoss.inverse_transform(y_train_pred_loss.detach().cpu().numpy()))\n",
        "\n",
        "predict_cls = pd.DataFrame(scalerLoss.inverse_transform(cls_to_draw.detach().cpu().numpy()))\n",
        "# original_loss = pd.DataFrame(scalerLoss.inverse_transform(train_tensors['y_loss'].detach().numpy()))\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# ax = sns.lineplot(x = original_main.index, y = original_main[0], label=\"Data Main\", color='royalblue')\n",
        "ax = sns.lineplot(x = predict_main.index, y = predict_main[0], label=\"Training Prediction Main(LSTM)\", color='tomato')\n",
        "\n",
        "# ax = sns.lineplot(x = original_even.index, y = original_even[0], label=\"Data Even\", color='red')\n",
        "ax = sns.lineplot(x = predict_even.index, y = predict_even[0], label=\"Training Prediction Even(LSTM)\", color='#FF6347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_loss.index, y = predict_loss[0], label=\"Training Prediction Loss(LSTM)\", color='#F31347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_cls.index, y = predict_cls[0], label=\"Training Prediction Class(LSTM)\", color='#a31347')\n",
        "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
        "ax.set_xlabel(\"Days\", size = 14)\n",
        "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
        "ax.set_xticklabels('', size=10)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "ax = sns.lineplot(data=hist, color='royalblue')\n",
        "ax.set_xlabel(\"Epoch\", size = 14)\n",
        "ax.set_ylabel(\"Loss\", size = 14)\n",
        "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(16)\n",
        "\n",
        "# 保存模型的状态字典\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_gru_model_weights.pth')\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_gru_model_weights_bk.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKWpgLhkXmCq"
      },
      "source": [
        "## 更多步骤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unFV0lUQXmCq"
      },
      "outputs": [],
      "source": [
        "#@title 训练模型\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_dim = 1\n",
        "hidden_dim = 32\n",
        "num_layers = 2\n",
        "output_dim = 1\n",
        "output_cls_dim = 3\n",
        "num_epochs = 30000\n",
        "batch_size = 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def split_data(dataMain, dataEven, dataLoss, dataResult, lookback, batch_size = 20):\n",
        "    data_raw_main = dataMain.to_numpy()\n",
        "    data_raw_even = dataEven.to_numpy()\n",
        "    data_raw_loss = dataLoss.to_numpy()\n",
        "    data_raw_result = dataResult.to_numpy()\n",
        "    data_main = []\n",
        "    data_even = []\n",
        "    data_loss = []\n",
        "    data_result = []\n",
        "\n",
        "    # create all possible sequences of length lookback\n",
        "    for index in range(len(data_raw_main) - lookback):\n",
        "        data_main.append(data_raw_main[index: index + lookback])\n",
        "    for index in range(len(data_raw_even) - lookback):\n",
        "        data_even.append(data_raw_even[index: index + lookback])\n",
        "    for index in range(len(data_raw_loss) - lookback):\n",
        "        data_loss.append(data_raw_loss[index: index + lookback])\n",
        "    for index in range(len(data_raw_result) - lookback):\n",
        "        data_result.append(data_raw_result[index: index + lookback])\n",
        "\n",
        "    data_main = np.array(data_main)\n",
        "    data_even = np.array(data_even)\n",
        "    data_loss = np.array(data_loss)\n",
        "    data_result = np.array(data_result)\n",
        "\n",
        "    test_set_size = int(np.round(0.2 * data_main.shape[0]))\n",
        "    train_set_size = data_main.shape[0]\n",
        "\n",
        "    # 将训练数据切割成多个批次\n",
        "    num_batches = train_set_size // batch_size\n",
        "    print(\"num_batches: \", num_batches, \" data_main.shape[0]: \", data_main.shape[0])\n",
        "\n",
        "\n",
        "    x_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, :-1, :], num_batches)\n",
        "    }\n",
        "    y_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, -1, :], num_batches)\n",
        "    }\n",
        "\n",
        "    x_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, :-1, :], num_batches)\n",
        "    }\n",
        "    y_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, -1, :], num_batches)\n",
        "    }\n",
        "    # print(\"x_train['main']: \", x_train['main'][1])\n",
        "    return [x_train, y_train, x_test, y_test]\n",
        "\n",
        "\n",
        "lookback = 20 # choose sequence length\n",
        "x_train, y_train, x_test, y_test = split_data(dataMain, dataEven, dataLoss, dataResult, lookback, 495)\n",
        "# x_train_batches, y_train_batches, x_test, y_test = split_data(price, lookback=20, batch_size=32)\n",
        "\n",
        "# # 打印批次的数量和每个批次的形状\n",
        "# print('Number of training batches:', len(x_train_batches))\n",
        "# if len(x_train_batches) > 0:\n",
        "#     print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "#     print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "# print('x_test.shape = ', x_test.shape)\n",
        "# print('y_test.shape = ', y_test.shape)\n",
        "\n",
        "# 打印批次的数量和每个批次的形状\n",
        "print('Number of training batches:', len(x_train['main']))\n",
        "if len(x_train['main']) > 0:\n",
        "    # print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "    # print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "    print('x_train.shape = ',x_train['main'][0].shape)\n",
        "    print('y_train.shape = ',y_train['main'][0].shape)\n",
        "    print('x_test.shape = ',x_test['main'][0].shape)\n",
        "    print('y_test.shape = ',y_test['main'][0].shape)\n",
        "    print('x_result.shape = ',y_train['result'][0].shape)\n",
        "\n",
        "# print(\"x_train: \", x_train)\n",
        "# print(\"y_train: \", y_train)\n",
        "\n",
        "# x_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train_batches]\n",
        "# y_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train_batches]\n",
        "import torch\n",
        "\n",
        "# 转换为字典\n",
        "train_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['result']]\n",
        "}\n",
        "\n",
        "test_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['result']]\n",
        "}\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.fcCls = nn.Linear(hidden_dim, output_cls_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "        out1 = self.fc(out[:, -1, :])\n",
        "\n",
        "        # 通过全连接层\n",
        "        outCls = self.fcCls(out[:, -1, :])\n",
        "        # 应用 softmax 激活\n",
        "        outCls = torch.softmax(outCls, dim=1)\n",
        "\n",
        "        return out1, outCls\n",
        "\n",
        "\n",
        "\n",
        "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "\n",
        "model_path = '/content/drive/MyDrive/ft_model_20_weights.pth'\n",
        "\n",
        "# 判断模型文件是否存在\n",
        "if os.path.exists(model_path):\n",
        "    # 创建模型实例\n",
        "    # model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
        "\n",
        "    # 加载模型参数\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # 移动模型到指定设备\n",
        "    model.to(device)\n",
        "    print(\"Model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Model file does not exist.\")\n",
        "\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterionCls = nn.CrossEntropyLoss()\n",
        "# optimizerCls = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "from re import X\n",
        "import time\n",
        "\n",
        "hist = np.zeros(num_epochs)\n",
        "start_time = time.time()\n",
        "lstm = []\n",
        "\n",
        "y_train_pred_main = None\n",
        "y_train_pred_even = None\n",
        "y_train_pred_loss = None\n",
        "cls_to_draw = None\n",
        "\n",
        "for t in range(num_epochs):\n",
        "\n",
        "    for x_main_batch, x_even_batch, x_loss_batch, y_main_batch, y_even_batch, y_loss_batch, y_result_batch in zip(train_tensors['x_main'], train_tensors['x_even'], train_tensors['x_loss'], train_tensors['y_main'], train_tensors['y_even'], train_tensors['y_loss'], train_tensors['y_result']):\n",
        "\n",
        "        x_main_batch = x_main_batch.to(device)\n",
        "        x_even_batch = x_even_batch.to(device)\n",
        "        x_loss_batch = x_loss_batch.to(device)\n",
        "        y_main_batch = y_main_batch.to(device)\n",
        "        y_even_batch = y_even_batch.to(device)\n",
        "        y_loss_batch = y_loss_batch.to(device)\n",
        "        y_result_batch = y_result_batch.to(device)\n",
        "        # print(\"ok: \")\n",
        "        # 替换前10个元素的内容为第11个元素的内容\n",
        "        if len(x_main_batch) > 10:  # 确保有足够的元素\n",
        "            x_main_batch[:10] = x_main_batch[10:11]\n",
        "        # print(\"x_main_batch: \", x_main_batch)\n",
        "        y_train_pred_main, cls_main = model(x_main_batch)\n",
        "        y_train_pred_even, cls_even = model(x_even_batch)\n",
        "        y_train_pred_loss, cls_loss = model(x_loss_batch)\n",
        "\n",
        "        loss_main = criterion(y_train_pred_main, y_main_batch)\n",
        "        loss_even = criterion(y_train_pred_even, y_even_batch)\n",
        "        loss_loss = criterion(y_train_pred_loss, y_loss_batch)\n",
        "\n",
        "        tmp = torch.softmax(cls_main + cls_even + cls_loss, dim=1)\n",
        "        cls_to_draw = tmp\n",
        "\n",
        "        loss_result = criterionCls(tmp, y_result_batch.squeeze().long())\n",
        "        loss = loss_main + loss_even + loss_loss + loss_result\n",
        "\n",
        "        # print(\"Epoch \", t, \"MSE: \", loss.item())\n",
        "        hist[t] = loss.item()\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "    print(\"Epoch \", t, \"MSE: \", loss.item(), \"cls_main + cls_even + cls_loss: \", tmp)\n",
        "    if t % 30 == 0:\n",
        "        # 保存模型的状态字典\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_20_weights.pth')\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_20_weights_bk.pth')\n",
        "\n",
        "training_time = time.time()-start_time\n",
        "print(\"Training time: {}\".format(training_time))\n",
        "\n",
        "\n",
        "predict_main = pd.DataFrame(scalerMain.inverse_transform(y_train_pred_main.detach().cpu().numpy()))\n",
        "# original_main = pd.DataFrame(scalerMain.inverse_transform(train_tensors['y_main'].detach().numpy()))\n",
        "\n",
        "predict_even = pd.DataFrame(scalerEven.inverse_transform(y_train_pred_even.detach().cpu().numpy()))\n",
        "# original_even = pd.DataFrame(scalerEven.inverse_transform(train_tensors['y_even'].detach().numpy()))\n",
        "\n",
        "predict_loss = pd.DataFrame(scalerLoss.inverse_transform(y_train_pred_loss.detach().cpu().numpy()))\n",
        "\n",
        "predict_cls = pd.DataFrame(scalerLoss.inverse_transform(cls_to_draw.detach().cpu().numpy()))\n",
        "# original_loss = pd.DataFrame(scalerLoss.inverse_transform(train_tensors['y_loss'].detach().numpy()))\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# ax = sns.lineplot(x = original_main.index, y = original_main[0], label=\"Data Main\", color='royalblue')\n",
        "ax = sns.lineplot(x = predict_main.index, y = predict_main[0], label=\"Training Prediction Main(LSTM)\", color='tomato')\n",
        "\n",
        "# ax = sns.lineplot(x = original_even.index, y = original_even[0], label=\"Data Even\", color='red')\n",
        "ax = sns.lineplot(x = predict_even.index, y = predict_even[0], label=\"Training Prediction Even(LSTM)\", color='#FF6347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_loss.index, y = predict_loss[0], label=\"Training Prediction Loss(LSTM)\", color='#F31347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_cls.index, y = predict_cls[0], label=\"Training Prediction Class(LSTM)\", color='#a31347')\n",
        "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
        "ax.set_xlabel(\"Days\", size = 14)\n",
        "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
        "ax.set_xticklabels('', size=10)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "ax = sns.lineplot(data=hist, color='royalblue')\n",
        "ax.set_xlabel(\"Epoch\", size = 14)\n",
        "ax.set_ylabel(\"Loss\", size = 14)\n",
        "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(16)\n",
        "\n",
        "# 保存模型的状态字典\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_20_weights.pth')\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_20_weights_bk.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 模型更多输出"
      ],
      "metadata": {
        "id": "uZynZXu6rDry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 训练模型\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_dim = 1\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "output_dim = 1\n",
        "output_cls_dim = 3\n",
        "output_main_loss_cls_dim = 10\n",
        "num_epochs = 30000\n",
        "batch_size = 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def split_data(dataMain, dataEven, dataLoss, dataResult, dataResultHome, dataResultAway, lookback, batch_size = 20):\n",
        "\n",
        "\n",
        "    data_raw_main = dataMain.to_numpy()\n",
        "    data_raw_even = dataEven.to_numpy()\n",
        "    data_raw_loss = dataLoss.to_numpy()\n",
        "    data_raw_result = dataResult.to_numpy()\n",
        "    data_raw_result_home = dataResultHome.to_numpy()\n",
        "    data_raw_result_away = dataResultAway.to_numpy()\n",
        "    data_main = []\n",
        "    data_even = []\n",
        "    data_loss = []\n",
        "    data_result = []\n",
        "    data_result_home = []\n",
        "    data_result_away = []\n",
        "\n",
        "    # create all possible sequences of length lookback\n",
        "    for index in range(len(data_raw_main) - lookback):\n",
        "        data_main.append(data_raw_main[index: index + lookback])\n",
        "    for index in range(len(data_raw_even) - lookback):\n",
        "        data_even.append(data_raw_even[index: index + lookback])\n",
        "    for index in range(len(data_raw_loss) - lookback):\n",
        "        data_loss.append(data_raw_loss[index: index + lookback])\n",
        "    for index in range(len(data_raw_result) - lookback):\n",
        "        data_result.append(data_raw_result[index: index + lookback])\n",
        "    for index in range(len(data_raw_result_home) - lookback):\n",
        "        data_result_home.append(data_raw_result_home[index: index + lookback])\n",
        "    for index in range(len(data_raw_result_away) - lookback):\n",
        "        data_result_away.append(data_raw_result_away[index: index + lookback])\n",
        "\n",
        "    data_main = np.array(data_main)\n",
        "    data_even = np.array(data_even)\n",
        "    data_loss = np.array(data_loss)\n",
        "    data_result = np.array(data_result)\n",
        "    data_result_home = np.array(data_result_home)\n",
        "    data_result_away = np.array(data_result_away)\n",
        "    # print(\"data_result_home: \", data_result_home)\n",
        "\n",
        "    test_set_size = int(np.round(0.2 * data_main.shape[0]))\n",
        "    train_set_size = data_main.shape[0]\n",
        "\n",
        "    # 将训练数据切割成多个批次\n",
        "    num_batches = train_set_size // batch_size\n",
        "    print(\"num_batches: \", num_batches, \" data_main.shape[0]: \", data_main.shape[0])\n",
        "\n",
        "\n",
        "    x_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, :-1, :], num_batches),\n",
        "        'resultHome': np.array_split(data_result_home[:train_set_size, :-1, :], num_batches),\n",
        "        'resultAway': np.array_split(data_result_away[:train_set_size, :-1, :], num_batches)\n",
        "    }\n",
        "    y_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, -1, :], num_batches),\n",
        "        'resultHome': np.array_split(data_result_home[:train_set_size, -1, :], num_batches),\n",
        "        'resultAway': np.array_split(data_result_away[:train_set_size, -1, :], num_batches)\n",
        "    }\n",
        "\n",
        "    x_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, :-1, :], num_batches),\n",
        "        'resultHome': np.array_split(data_result_home[train_set_size:, :-1, :], num_batches),\n",
        "        'resultAway': np.array_split(data_result_away[train_set_size:, :-1, :], num_batches)\n",
        "    }\n",
        "    y_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, -1, :], num_batches),\n",
        "        'resultHome': np.array_split(data_result_home[train_set_size:, -1, :], num_batches),\n",
        "        'resultAway': np.array_split(data_result_away[train_set_size:, -1, :], num_batches)\n",
        "    }\n",
        "    # print(\"x_train['main']: \", x_train['main'][1])\n",
        "    return [x_train, y_train, x_test, y_test]\n",
        "\n",
        "\n",
        "lookback = 20 # choose sequence length\n",
        "x_train, y_train, x_test, y_test = split_data(dataMain, dataEven, dataLoss, dataResult, dataResultHome, dataResultAway, lookback, 495)\n",
        "# x_train_batches, y_train_batches, x_test, y_test = split_data(price, lookback=20, batch_size=32)\n",
        "\n",
        "# # 打印批次的数量和每个批次的形状\n",
        "# print('Number of training batches:', len(x_train_batches))\n",
        "# if len(x_train_batches) > 0:\n",
        "#     print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "#     print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "# print('x_test.shape = ', x_test.shape)\n",
        "# print('y_test.shape = ', y_test.shape)\n",
        "\n",
        "# 打印批次的数量和每个批次的形状\n",
        "print('Number of training batches:', len(x_train['main']))\n",
        "if len(x_train['main']) > 0:\n",
        "    # print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "    # print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "    print('x_train.shape = ',x_train['main'][0].shape)\n",
        "    print('y_train.shape = ',y_train['main'][0].shape)\n",
        "    print('x_test.shape = ',x_test['main'][0].shape)\n",
        "    print('y_test.shape = ',y_test['main'][0].shape)\n",
        "    print('x_result.shape = ',y_train['result'][0].shape)\n",
        "    print('resultHome.shape = ',y_train['resultHome'][0].shape)\n",
        "    print('resultAway.shape = ',y_train['resultAway'][0].shape)\n",
        "\n",
        "# print(\"x_train: \", x_train)\n",
        "# print(\"y_train: \", y_train)\n",
        "\n",
        "# x_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train_batches]\n",
        "# y_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train_batches]\n",
        "import torch\n",
        "\n",
        "# 转换为字典\n",
        "train_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['result']],\n",
        "    'x_result_home': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['resultHome']],\n",
        "    'x_result_away': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['resultAway']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['result']],\n",
        "    'y_result_home': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['resultHome']],\n",
        "    'y_result_away': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['resultAway']]\n",
        "}\n",
        "\n",
        "test_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['result']],\n",
        "    'x_result_home': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['resultHome']],\n",
        "    'x_result_away': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['resultAway']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['result']],\n",
        "    'y_result_home': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['resultHome']],\n",
        "    'y_result_away': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['resultAway']]\n",
        "}\n",
        "\n",
        "# class LSTM(nn.Module):\n",
        "#     def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "#         super(LSTM, self).__init__()\n",
        "#         self.hidden_dim = hidden_dim\n",
        "#         self.num_layers = num_layers\n",
        "\n",
        "#         self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "#         self.fcCls = nn.Linear(hidden_dim, output_cls_dim)\n",
        "#         self.fcMainCls = nn.Linear(hidden_dim, output_main_loss_cls_dim)\n",
        "#         self.fcLossCls = nn.Linear(hidden_dim, output_main_loss_cls_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "#         out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "#         out1 = self.fc(out[:, -1, :])\n",
        "\n",
        "#         # 通过全连接层\n",
        "#         outCls = self.fcCls(out[:, -1, :])\n",
        "#         # 应用 softmax 激活\n",
        "#         outCls = torch.softmax(outCls, dim=1)\n",
        "\n",
        "#         outMainCls = self.fcMainCls(out[:, -1, :])\n",
        "#         # 应用 softmax 激活\n",
        "#         outMainCls = torch.softmax(outMainCls, dim=1)\n",
        "\n",
        "#         outLossCls = self.fcLossCls(out[:, -1, :])\n",
        "#         # 应用 softmax 激活\n",
        "#         outLossCls = torch.softmax(outLossCls, dim=1)\n",
        "\n",
        "#         return out1, outCls, outMainCls, outLossCls\n",
        "\n",
        "\n",
        "\n",
        "# model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.fcCls = nn.Linear(hidden_dim, output_cls_dim)\n",
        "        self.fcMainCls = nn.Linear(hidden_dim, output_main_loss_cls_dim)\n",
        "        self.fcLossCls = nn.Linear(hidden_dim, output_main_loss_cls_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "        out, (hn) = self.gru(x, (h0.detach()))\n",
        "        out1 = self.fc(out[:, -1, :])\n",
        "        # 通过全连接层\n",
        "        outCls = self.fcCls(out[:, -1, :])\n",
        "        # 应用 softmax 激活\n",
        "        outCls = torch.softmax(outCls, dim=1)\n",
        "\n",
        "        outMainCls = self.fcMainCls(out[:, -1, :])\n",
        "        # 应用 softmax 激活\n",
        "        outMainCls = torch.softmax(outMainCls, dim=1)\n",
        "\n",
        "        outLossCls = self.fcLossCls(out[:, -1, :])\n",
        "        # 应用 softmax 激活\n",
        "        outLossCls = torch.softmax(outLossCls, dim=1)\n",
        "\n",
        "        return out1, outCls, outMainCls, outLossCls\n",
        "\n",
        "model = GRU(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "\n",
        "model_path = '/content/drive/MyDrive/ft_model_20_home_away_weights1.pth'\n",
        "\n",
        "# 判断模型文件是否存在\n",
        "if os.path.exists(model_path):\n",
        "    # 创建模型实例\n",
        "    # model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
        "\n",
        "    # 加载模型参数\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # 移动模型到指定设备\n",
        "    model.to(device)\n",
        "    print(\"Model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Model file does not exist.\")\n",
        "\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterionCls = nn.CrossEntropyLoss()\n",
        "criterionClsMain = nn.CrossEntropyLoss()\n",
        "criterionClsLoss = nn.CrossEntropyLoss()\n",
        "# optimizerCls = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "from re import X\n",
        "import time\n",
        "\n",
        "hist = np.zeros(num_epochs)\n",
        "start_time = time.time()\n",
        "lstm = []\n",
        "\n",
        "y_train_pred_main = None\n",
        "y_train_pred_even = None\n",
        "y_train_pred_loss = None\n",
        "cls_to_draw = None\n",
        "\n",
        "for t in range(num_epochs):\n",
        "\n",
        "    for x_main_batch, x_even_batch, x_loss_batch, y_main_batch, y_even_batch, y_loss_batch, y_result_batch, y_result_main_batch, y_result_loss_batch in zip(train_tensors['x_main'], train_tensors['x_even'], train_tensors['x_loss'], train_tensors['y_main'], train_tensors['y_even'], train_tensors['y_loss'], train_tensors['y_result'], train_tensors['y_result_home'], train_tensors['y_result_away']):\n",
        "\n",
        "\n",
        "        x_main_batch = x_main_batch.to(device)\n",
        "        x_even_batch = x_even_batch.to(device)\n",
        "        x_loss_batch = x_loss_batch.to(device)\n",
        "        y_main_batch = y_main_batch.to(device)\n",
        "        y_even_batch = y_even_batch.to(device)\n",
        "        y_loss_batch = y_loss_batch.to(device)\n",
        "        y_result_batch = y_result_batch.to(device)\n",
        "        y_result_main_batch = y_result_main_batch.to(device)\n",
        "        y_result_loss_batch = y_result_loss_batch.to(device)\n",
        "        # print(\"y_result_main_batch:\", y_result_main_batch)\n",
        "        # print(\"ok: \")\n",
        "        # 替换前10个元素的内容为第11个元素的内容\n",
        "        if len(x_main_batch) > 10:  # 确保有足够的元素\n",
        "            x_main_batch[:10] = x_main_batch[10:11]\n",
        "        # print(\"x_main_batch: \", x_main_batch)\n",
        "        y_train_pred_main, cls_main, cls_main_main, cls_main_loss = model(x_main_batch)\n",
        "        y_train_pred_even, cls_even, cls_even_main, cls_even_loss = model(x_even_batch)\n",
        "        y_train_pred_loss, cls_loss, cls_loss_main, cls_loss_loss = model(x_loss_batch)\n",
        "\n",
        "        loss_main = criterion(y_train_pred_main, y_main_batch)\n",
        "        loss_even = criterion(y_train_pred_even, y_even_batch)\n",
        "        loss_loss = criterion(y_train_pred_loss, y_loss_batch)\n",
        "\n",
        "        tmp = torch.softmax(cls_main + cls_even + cls_loss, dim=1)\n",
        "        cls_to_draw = tmp\n",
        "\n",
        "        tmpMain = torch.softmax(cls_main_main + cls_even_main + cls_loss_main, dim=1)\n",
        "        cls_to_draw_main = tmpMain\n",
        "\n",
        "        tmpLoss = torch.softmax(cls_main_loss + cls_even_loss + cls_loss_loss, dim=1)\n",
        "        cls_to_draw_loss = tmpLoss\n",
        "\n",
        "        loss_result = criterionCls(tmp, y_result_batch.squeeze().long())\n",
        "\n",
        "        loss_result_main = criterionClsMain(tmpMain, y_result_main_batch.squeeze().long())\n",
        "        loss_result_loss = criterionClsLoss(tmpLoss, y_result_loss_batch.squeeze().long())\n",
        "        # print(\"loss_result_main:\", loss_result_main)\n",
        "        loss = loss_main + loss_even + loss_loss + loss_result + loss_result_main + loss_result_loss\n",
        "\n",
        "        # print(\"Epoch \", t, \"MSE: \", loss.item())\n",
        "        # hist[t] = loss.item()\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "        # print(\"cls_to_draw_main: \", cls_to_draw_main)\n",
        "    print(\"Epoch \", t, \"MSE: \", loss.item(), \"cls_main + cls_even + cls_loss: \", cls_to_draw_main)\n",
        "    # print(\"y_result_main_batch: \", y_result_main_batch)\n",
        "    if t % 30 == 0:\n",
        "        # 保存模型的状态字典\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_20_home_away_weights.pth')\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_20_home_away_weights_bk.pth')\n",
        "\n",
        "training_time = time.time()-start_time\n",
        "print(\"Training time: {}\".format(training_time))\n",
        "\n",
        "\n",
        "predict_main = pd.DataFrame(scalerMain.inverse_transform(y_train_pred_main.detach().cpu().numpy()))\n",
        "# original_main = pd.DataFrame(scalerMain.inverse_transform(train_tensors['y_main'].detach().numpy()))\n",
        "\n",
        "predict_even = pd.DataFrame(scalerEven.inverse_transform(y_train_pred_even.detach().cpu().numpy()))\n",
        "# original_even = pd.DataFrame(scalerEven.inverse_transform(train_tensors['y_even'].detach().numpy()))\n",
        "\n",
        "predict_loss = pd.DataFrame(scalerLoss.inverse_transform(y_train_pred_loss.detach().cpu().numpy()))\n",
        "\n",
        "predict_cls = pd.DataFrame(scalerLoss.inverse_transform(cls_to_draw.detach().cpu().numpy()))\n",
        "# original_loss = pd.DataFrame(scalerLoss.inverse_transform(train_tensors['y_loss'].detach().numpy()))\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# ax = sns.lineplot(x = original_main.index, y = original_main[0], label=\"Data Main\", color='royalblue')\n",
        "ax = sns.lineplot(x = predict_main.index, y = predict_main[0], label=\"Training Prediction Main(LSTM)\", color='tomato')\n",
        "\n",
        "# ax = sns.lineplot(x = original_even.index, y = original_even[0], label=\"Data Even\", color='red')\n",
        "ax = sns.lineplot(x = predict_even.index, y = predict_even[0], label=\"Training Prediction Even(LSTM)\", color='#FF6347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_loss.index, y = predict_loss[0], label=\"Training Prediction Loss(LSTM)\", color='#F31347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_cls.index, y = predict_cls[0], label=\"Training Prediction Class(LSTM)\", color='#a31347')\n",
        "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
        "ax.set_xlabel(\"Days\", size = 14)\n",
        "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
        "ax.set_xticklabels('', size=10)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "ax = sns.lineplot(data=hist, color='royalblue')\n",
        "ax.set_xlabel(\"Epoch\", size = 14)\n",
        "ax.set_ylabel(\"Loss\", size = 14)\n",
        "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(16)\n",
        "\n",
        "# 保存模型的状态字典\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_20_home_away_weights.pth')\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_20_home_away_weights_bk.pth')"
      ],
      "metadata": {
        "id": "WbFF-g_vrPtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 总数"
      ],
      "metadata": {
        "id": "3fsETEULnzje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 训练模型\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_dim = 1\n",
        "hidden_dim = 32\n",
        "num_layers = 2\n",
        "output_dim = 1\n",
        "output_cls_dim = 3\n",
        "output_main_loss_cls_dim = 10\n",
        "num_epochs = 30000\n",
        "batch_size = 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def split_data(dataMain, dataEven, dataLoss, dataResult, dataResultHome, dataResultAway, lookback, batch_size = 20):\n",
        "\n",
        "\n",
        "    data_raw_main = dataMain.to_numpy()\n",
        "    data_raw_even = dataEven.to_numpy()\n",
        "    data_raw_loss = dataLoss.to_numpy()\n",
        "    data_raw_result = dataResult.to_numpy()\n",
        "    data_raw_result_home = dataResultHome.to_numpy()\n",
        "    data_raw_result_away = dataResultAway.to_numpy()\n",
        "    data_main = []\n",
        "    data_even = []\n",
        "    data_loss = []\n",
        "    data_result = []\n",
        "    data_result_home = []\n",
        "    data_result_away = []\n",
        "\n",
        "    # create all possible sequences of length lookback\n",
        "    for index in range(len(data_raw_main) - lookback):\n",
        "        data_main.append(data_raw_main[index: index + lookback])\n",
        "    for index in range(len(data_raw_even) - lookback):\n",
        "        data_even.append(data_raw_even[index: index + lookback])\n",
        "    for index in range(len(data_raw_loss) - lookback):\n",
        "        data_loss.append(data_raw_loss[index: index + lookback])\n",
        "    for index in range(len(data_raw_result) - lookback):\n",
        "        data_result.append(data_raw_result[index: index + lookback])\n",
        "    for index in range(len(data_raw_result_home) - lookback):\n",
        "        data_result_home.append(data_raw_result_home[index: index + lookback])\n",
        "    for index in range(len(data_raw_result_away) - lookback):\n",
        "        data_result_away.append(data_raw_result_away[index: index + lookback])\n",
        "\n",
        "    data_main = np.array(data_main)\n",
        "    data_even = np.array(data_even)\n",
        "    data_loss = np.array(data_loss)\n",
        "    data_result = np.array(data_result)\n",
        "    data_result_home = np.array(data_result_home)\n",
        "    data_result_away = np.array(data_result_away)\n",
        "    # print(\"data_result_home: \", data_result_home)\n",
        "\n",
        "    test_set_size = int(np.round(0.2 * data_main.shape[0]))\n",
        "    train_set_size = data_main.shape[0]\n",
        "\n",
        "    # 将训练数据切割成多个批次\n",
        "    num_batches = train_set_size // batch_size\n",
        "    print(\"num_batches: \", num_batches, \" data_main.shape[0]: \", data_main.shape[0])\n",
        "\n",
        "\n",
        "    x_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, :-1, :], num_batches),\n",
        "        'resultHome': np.array_split(data_result_home[:train_set_size, :-1, :], num_batches),\n",
        "        'resultAway': np.array_split(data_result_away[:train_set_size, :-1, :], num_batches)\n",
        "    }\n",
        "    y_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, -1, :], num_batches),\n",
        "        'resultHome': np.array_split(data_result_home[:train_set_size, -1, :], num_batches),\n",
        "        'resultAway': np.array_split(data_result_away[:train_set_size, -1, :], num_batches)\n",
        "    }\n",
        "\n",
        "    x_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, :-1, :], num_batches),\n",
        "        'resultHome': np.array_split(data_result_home[train_set_size:, :-1, :], num_batches),\n",
        "        'resultAway': np.array_split(data_result_away[train_set_size:, :-1, :], num_batches)\n",
        "    }\n",
        "    y_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, -1, :], num_batches),\n",
        "        'resultHome': np.array_split(data_result_home[train_set_size:, -1, :], num_batches),\n",
        "        'resultAway': np.array_split(data_result_away[train_set_size:, -1, :], num_batches)\n",
        "    }\n",
        "    # print(\"x_train['main']: \", x_train['main'][1])\n",
        "    return [x_train, y_train, x_test, y_test]\n",
        "\n",
        "\n",
        "lookback = 20 # choose sequence length\n",
        "x_train, y_train, x_test, y_test = split_data(dataMain, dataEven, dataLoss, dataResult, dataResultHome, dataResultAway, lookback, 495)\n",
        "# x_train_batches, y_train_batches, x_test, y_test = split_data(price, lookback=20, batch_size=32)\n",
        "\n",
        "# # 打印批次的数量和每个批次的形状\n",
        "# print('Number of training batches:', len(x_train_batches))\n",
        "# if len(x_train_batches) > 0:\n",
        "#     print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "#     print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "# print('x_test.shape = ', x_test.shape)\n",
        "# print('y_test.shape = ', y_test.shape)\n",
        "\n",
        "# 打印批次的数量和每个批次的形状\n",
        "print('Number of training batches:', len(x_train['main']))\n",
        "if len(x_train['main']) > 0:\n",
        "    # print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "    # print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "    print('x_train.shape = ',x_train['main'][0].shape)\n",
        "    print('y_train.shape = ',y_train['main'][0].shape)\n",
        "    print('x_test.shape = ',x_test['main'][0].shape)\n",
        "    print('y_test.shape = ',y_test['main'][0].shape)\n",
        "    print('x_result.shape = ',y_train['result'][0].shape)\n",
        "    print('resultHome.shape = ',y_train['resultHome'][0].shape)\n",
        "    print('resultAway.shape = ',y_train['resultAway'][0].shape)\n",
        "\n",
        "# print(\"x_train: \", x_train)\n",
        "# print(\"y_train: \", y_train)\n",
        "\n",
        "# x_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train_batches]\n",
        "# y_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train_batches]\n",
        "import torch\n",
        "\n",
        "# 转换为字典\n",
        "train_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['result']],\n",
        "    'x_result_home': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['resultHome']],\n",
        "    'x_result_away': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['resultAway']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['result']],\n",
        "    'y_result_home': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['resultHome']],\n",
        "    'y_result_away': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['resultAway']]\n",
        "}\n",
        "\n",
        "test_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['result']],\n",
        "    'x_result_home': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['resultHome']],\n",
        "    'x_result_away': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['resultAway']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['result']],\n",
        "    'y_result_home': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['resultHome']],\n",
        "    'y_result_away': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['resultAway']]\n",
        "}\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.fcCls = nn.Linear(hidden_dim, output_cls_dim)\n",
        "        self.fcMainCls = nn.Linear(hidden_dim, output_main_loss_cls_dim)\n",
        "        self.fcLossCls = nn.Linear(hidden_dim, output_main_loss_cls_dim)\n",
        "        self.fcAllCls = nn.Linear(hidden_dim, output_main_loss_cls_dim * 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "        out1 = self.fc(out[:, -1, :])\n",
        "\n",
        "        # 通过全连接层\n",
        "        outCls = self.fcCls(out[:, -1, :])\n",
        "        # 应用 softmax 激活\n",
        "        outCls = torch.softmax(outCls, dim=1)\n",
        "\n",
        "        outMainCls = self.fcMainCls(out[:, -1, :])\n",
        "        # 应用 softmax 激活\n",
        "        outMainCls = torch.softmax(outMainCls, dim=1)\n",
        "\n",
        "        outLossCls = self.fcLossCls(out[:, -1, :])\n",
        "        # 应用 softmax 激活\n",
        "        outLossCls = torch.softmax(outLossCls, dim=1)\n",
        "\n",
        "        outAllCls = self.fcAllCls(out[:, -1, :])\n",
        "        # 应用 softmax 激活\n",
        "        outAllCls = torch.softmax(outAllCls, dim=1)\n",
        "\n",
        "        return out1, outCls, outMainCls, outLossCls, outAllCls\n",
        "\n",
        "\n",
        "\n",
        "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "\n",
        "model_path = '/content/drive/MyDrive/ft_model_20_home_away_weights_all.pth'\n",
        "\n",
        "# 判断模型文件是否存在\n",
        "if os.path.exists(model_path):\n",
        "    # 创建模型实例\n",
        "    # model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
        "\n",
        "    # 加载模型参数\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # 移动模型到指定设备\n",
        "    model.to(device)\n",
        "    print(\"Model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Model file does not exist.\")\n",
        "\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterionCls = nn.CrossEntropyLoss()\n",
        "criterionClsMain = nn.CrossEntropyLoss()\n",
        "criterionClsLoss = nn.CrossEntropyLoss()\n",
        "criterionClsAll = nn.CrossEntropyLoss()\n",
        "# optimizerCls = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "from re import X\n",
        "import time\n",
        "\n",
        "hist = np.zeros(num_epochs)\n",
        "start_time = time.time()\n",
        "lstm = []\n",
        "\n",
        "y_train_pred_main = None\n",
        "y_train_pred_even = None\n",
        "y_train_pred_loss = None\n",
        "cls_to_draw = None\n",
        "\n",
        "for t in range(num_epochs):\n",
        "\n",
        "    for x_main_batch, x_even_batch, x_loss_batch, y_main_batch, y_even_batch, y_loss_batch, y_result_batch, y_result_main_batch, y_result_loss_batch in zip(train_tensors['x_main'], train_tensors['x_even'], train_tensors['x_loss'], train_tensors['y_main'], train_tensors['y_even'], train_tensors['y_loss'], train_tensors['y_result'], train_tensors['y_result_home'], train_tensors['y_result_away']):\n",
        "\n",
        "        x_main_batch = x_main_batch.to(device)\n",
        "        x_even_batch = x_even_batch.to(device)\n",
        "        x_loss_batch = x_loss_batch.to(device)\n",
        "        y_main_batch = y_main_batch.to(device)\n",
        "        y_even_batch = y_even_batch.to(device)\n",
        "        y_loss_batch = y_loss_batch.to(device)\n",
        "        y_result_batch = y_result_batch.to(device)\n",
        "        y_result_main_batch = y_result_main_batch.to(device)\n",
        "        y_result_loss_batch = y_result_loss_batch.to(device)\n",
        "        # print(\"y_result_main_batch:\", y_result_main_batch)\n",
        "        # print(\"ok: \")\n",
        "        # 替换前10个元素的内容为第11个元素的内容\n",
        "        if len(x_main_batch) > 10:  # 确保有足够的元素\n",
        "            x_main_batch[:10] = x_main_batch[10:11]\n",
        "        # print(\"x_main_batch: \", x_main_batch)\n",
        "        y_train_pred_main, cls_main, cls_main_main, cls_main_loss, cls_main_all = model(x_main_batch)\n",
        "        y_train_pred_even, cls_even, cls_even_main, cls_even_loss, cls_even_all = model(x_even_batch)\n",
        "        y_train_pred_loss, cls_loss, cls_loss_main, cls_loss_loss, cls_loss_all = model(x_loss_batch)\n",
        "\n",
        "        loss_main = criterion(y_train_pred_main, y_main_batch)\n",
        "        loss_even = criterion(y_train_pred_even, y_even_batch)\n",
        "        loss_loss = criterion(y_train_pred_loss, y_loss_batch)\n",
        "\n",
        "        tmp = torch.softmax(cls_main + cls_even + cls_loss, dim=1)\n",
        "        cls_to_draw = tmp\n",
        "\n",
        "        tmpMain = torch.softmax(cls_main_main + cls_even_main + cls_loss_main, dim=1)\n",
        "        cls_to_draw_main = tmpMain\n",
        "\n",
        "        tmpLoss = torch.softmax(cls_main_loss + cls_even_loss + cls_loss_loss, dim=1)\n",
        "        cls_to_draw_loss = tmpLoss\n",
        "\n",
        "        tmpAll = torch.softmax(cls_main_all + cls_even_all + cls_loss_all, dim=1)\n",
        "        cls_to_draw_all = tmpAll\n",
        "\n",
        "        loss_result = criterionCls(tmp, y_result_batch.squeeze().long())\n",
        "\n",
        "        loss_result_main = criterionClsMain(tmpMain, y_result_main_batch.squeeze().long())\n",
        "        loss_result_loss = criterionClsLoss(tmpLoss, y_result_loss_batch.squeeze().long())\n",
        "\n",
        "        loss_result_all = criterionClsLoss(tmpAll, (y_result_main_batch + y_result_loss_batch).squeeze().long())\n",
        "        # print(\"loss_result_main:\", loss_result_main)\n",
        "        loss = loss_main + loss_even + loss_loss + loss_result + loss_result_main + loss_result_loss\n",
        "\n",
        "        # print(\"Epoch \", t, \"MSE: \", loss.item())\n",
        "        # hist[t] = loss.item()\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "    print(\"Epoch \", t, \"MSE: \", loss.item(), \"cls_main + cls_even + cls_loss: \", tmp)\n",
        "    # print(\"y_result_main_batch: \", y_result_main_batch)\n",
        "    if t % 30 == 0:\n",
        "        # 保存模型的状态字典\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_20_home_away_weights_all.pth')\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_20_home_away_weights_all_bk.pth')\n",
        "\n",
        "training_time = time.time()-start_time\n",
        "print(\"Training time: {}\".format(training_time))\n",
        "\n",
        "\n",
        "predict_main = pd.DataFrame(scalerMain.inverse_transform(y_train_pred_main.detach().cpu().numpy()))\n",
        "# original_main = pd.DataFrame(scalerMain.inverse_transform(train_tensors['y_main'].detach().numpy()))\n",
        "\n",
        "predict_even = pd.DataFrame(scalerEven.inverse_transform(y_train_pred_even.detach().cpu().numpy()))\n",
        "# original_even = pd.DataFrame(scalerEven.inverse_transform(train_tensors['y_even'].detach().numpy()))\n",
        "\n",
        "predict_loss = pd.DataFrame(scalerLoss.inverse_transform(y_train_pred_loss.detach().cpu().numpy()))\n",
        "\n",
        "predict_cls = pd.DataFrame(scalerLoss.inverse_transform(cls_to_draw.detach().cpu().numpy()))\n",
        "# original_loss = pd.DataFrame(scalerLoss.inverse_transform(train_tensors['y_loss'].detach().numpy()))\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# ax = sns.lineplot(x = original_main.index, y = original_main[0], label=\"Data Main\", color='royalblue')\n",
        "ax = sns.lineplot(x = predict_main.index, y = predict_main[0], label=\"Training Prediction Main(LSTM)\", color='tomato')\n",
        "\n",
        "# ax = sns.lineplot(x = original_even.index, y = original_even[0], label=\"Data Even\", color='red')\n",
        "ax = sns.lineplot(x = predict_even.index, y = predict_even[0], label=\"Training Prediction Even(LSTM)\", color='#FF6347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_loss.index, y = predict_loss[0], label=\"Training Prediction Loss(LSTM)\", color='#F31347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_cls.index, y = predict_cls[0], label=\"Training Prediction Class(LSTM)\", color='#a31347')\n",
        "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
        "ax.set_xlabel(\"Days\", size = 14)\n",
        "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
        "ax.set_xticklabels('', size=10)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "ax = sns.lineplot(data=hist, color='royalblue')\n",
        "ax.set_xlabel(\"Epoch\", size = 14)\n",
        "ax.set_ylabel(\"Loss\", size = 14)\n",
        "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(16)\n",
        "\n",
        "# 保存模型的状态字典\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_20_home_away_weights_all.pth')\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_20_home_away_weights_all_bk.pth')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LBpTHpJCn3oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yknGxJjxXmCq"
      },
      "source": [
        "## 基于transformer修改的模型"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fOkX61_yrPDr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLvyxFm2XmCr"
      },
      "outputs": [],
      "source": [
        "#@title 训练模型\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_dim = 1\n",
        "hidden_dim = 32\n",
        "num_layers = 2\n",
        "output_dim = 1\n",
        "output_cls_dim = 3\n",
        "num_epochs = 300\n",
        "batch_size = 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def split_data(dataMain, dataEven, dataLoss, dataResult, lookback, batch_size = 20):\n",
        "    data_raw_main = dataMain.to_numpy()\n",
        "    data_raw_even = dataEven.to_numpy()\n",
        "    data_raw_loss = dataLoss.to_numpy()\n",
        "    data_raw_result = dataResult.to_numpy()\n",
        "    data_main = []\n",
        "    data_even = []\n",
        "    data_loss = []\n",
        "    data_result = []\n",
        "\n",
        "    # create all possible sequences of length lookback\n",
        "    for index in range(len(data_raw_main) - lookback):\n",
        "        data_main.append(data_raw_main[index: index + lookback])\n",
        "    for index in range(len(data_raw_even) - lookback):\n",
        "        data_even.append(data_raw_even[index: index + lookback])\n",
        "    for index in range(len(data_raw_loss) - lookback):\n",
        "        data_loss.append(data_raw_loss[index: index + lookback])\n",
        "    for index in range(len(data_raw_result) - lookback):\n",
        "        data_result.append(data_raw_result[index: index + lookback])\n",
        "\n",
        "    data_main = np.array(data_main)\n",
        "    data_even = np.array(data_even)\n",
        "    data_loss = np.array(data_loss)\n",
        "    data_result = np.array(data_result)\n",
        "\n",
        "    test_set_size = int(np.round(0.2 * data_main.shape[0]))\n",
        "    train_set_size = data_main.shape[0]\n",
        "\n",
        "    # 将训练数据切割成多个批次\n",
        "    num_batches = train_set_size // batch_size\n",
        "    print(\"num_batches: \", num_batches, \" data_main.shape[0]: \", data_main.shape[0])\n",
        "\n",
        "\n",
        "    x_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, :-1, :], num_batches)\n",
        "    }\n",
        "    y_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, -1, :], num_batches)\n",
        "    }\n",
        "\n",
        "    x_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, :-1, :], num_batches)\n",
        "    }\n",
        "    y_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, -1, :], num_batches)\n",
        "    }\n",
        "    # print(\"x_train['main']: \", x_train['main'][1])\n",
        "    return [x_train, y_train, x_test, y_test]\n",
        "\n",
        "\n",
        "lookback = 10 # choose sequence length\n",
        "x_train, y_train, x_test, y_test = split_data(dataMain, dataEven, dataLoss, dataResult, lookback, 495)\n",
        "# x_train_batches, y_train_batches, x_test, y_test = split_data(price, lookback=20, batch_size=32)\n",
        "\n",
        "# # 打印批次的数量和每个批次的形状\n",
        "# print('Number of training batches:', len(x_train_batches))\n",
        "# if len(x_train_batches) > 0:\n",
        "#     print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "#     print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "# print('x_test.shape = ', x_test.shape)\n",
        "# print('y_test.shape = ', y_test.shape)\n",
        "\n",
        "# 打印批次的数量和每个批次的形状\n",
        "print('Number of training batches:', len(x_train['main']))\n",
        "if len(x_train['main']) > 0:\n",
        "    # print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "    # print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "    print('x_train.shape = ',x_train['main'][0].shape)\n",
        "    print('y_train.shape = ',y_train['main'][0].shape)\n",
        "    print('x_test.shape = ',x_test['main'][0].shape)\n",
        "    print('y_test.shape = ',y_test['main'][0].shape)\n",
        "    print('x_result.shape = ',y_train['result'][0].shape)\n",
        "\n",
        "# print(\"x_train: \", x_train)\n",
        "# print(\"y_train: \", y_train)\n",
        "\n",
        "# x_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train_batches]\n",
        "# y_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train_batches]\n",
        "import torch\n",
        "\n",
        "# 转换为字典\n",
        "train_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['result']]\n",
        "}\n",
        "\n",
        "test_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['result']]\n",
        "}\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, nhead):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.fcCls = nn.Linear(hidden_dim, output_cls_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # 嵌入层\n",
        "        x = x.permute(1, 0, 2)  # 转换为 (seq_len, batch_size, hidden_dim)\n",
        "\n",
        "        out = self.transformer_encoder(x)  # Transformer 编码器\n",
        "        out1 = self.fc(out[-1, :, :])  # 取最后一个时间步的输出\n",
        "\n",
        "        # 通过全连接层\n",
        "        outCls = self.fcCls(out[-1, :, :])\n",
        "        # 应用 softmax 激活\n",
        "        outCls = torch.softmax(outCls, dim=1)\n",
        "\n",
        "        return out1, outCls\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = TransformerModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers, nhead=6).to(device)\n",
        "\n",
        "model_path = '/content/drive/MyDrive/ft_model_transformer_weights.pth'\n",
        "\n",
        "# 判断模型文件是否存在\n",
        "if os.path.exists(model_path):\n",
        "    # 创建模型实例\n",
        "    # model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
        "\n",
        "    # 加载模型参数\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # 移动模型到指定设备\n",
        "    model.to(device)\n",
        "    print(\"Model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Model file does not exist.\")\n",
        "\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterionCls = nn.CrossEntropyLoss()\n",
        "# optimizerCls = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "from re import X\n",
        "import time\n",
        "\n",
        "hist = np.zeros(num_epochs)\n",
        "start_time = time.time()\n",
        "lstm = []\n",
        "\n",
        "y_train_pred_main = None\n",
        "y_train_pred_even = None\n",
        "y_train_pred_loss = None\n",
        "cls_to_draw = None\n",
        "\n",
        "for t in range(num_epochs):\n",
        "\n",
        "    for x_main_batch, x_even_batch, x_loss_batch, y_main_batch, y_even_batch, y_loss_batch, y_result_batch in zip(train_tensors['x_main'], train_tensors['x_even'], train_tensors['x_loss'], train_tensors['y_main'], train_tensors['y_even'], train_tensors['y_loss'], train_tensors['y_result']):\n",
        "\n",
        "        x_main_batch = x_main_batch.to(device)\n",
        "        x_even_batch = x_even_batch.to(device)\n",
        "        x_loss_batch = x_loss_batch.to(device)\n",
        "        y_main_batch = y_main_batch.to(device)\n",
        "        y_even_batch = y_even_batch.to(device)\n",
        "        y_loss_batch = y_loss_batch.to(device)\n",
        "        y_result_batch = y_result_batch.to(device)\n",
        "        # print(\"ok: \")\n",
        "        # 替换前10个元素的内容为第11个元素的内容\n",
        "        if len(x_main_batch) > 10:  # 确保有足够的元素\n",
        "            x_main_batch[:10] = x_main_batch[10:11]\n",
        "        # print(\"x_main_batch: \", x_main_batch)\n",
        "        y_train_pred_main, cls_main = model(x_main_batch)\n",
        "        y_train_pred_even, cls_even = model(x_even_batch)\n",
        "        y_train_pred_loss, cls_loss = model(x_loss_batch)\n",
        "\n",
        "        loss_main = criterion(y_train_pred_main, y_main_batch)\n",
        "        loss_even = criterion(y_train_pred_even, y_even_batch)\n",
        "        loss_loss = criterion(y_train_pred_loss, y_loss_batch)\n",
        "\n",
        "        tmp = torch.softmax(cls_main + cls_even + cls_loss, dim=1)\n",
        "        cls_to_draw = tmp\n",
        "\n",
        "        loss_result = criterionCls(tmp, y_result_batch.squeeze().long())\n",
        "        loss = loss_main + loss_even + loss_loss + loss_result\n",
        "\n",
        "        # print(\"Epoch \", t, \"MSE: \", loss.item())\n",
        "        hist[t] = loss.item()\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "    print(\"Epoch \", t, \"MSE: \", loss.item(), \"cls_main + cls_even + cls_loss: \", tmp)\n",
        "\n",
        "training_time = time.time()-start_time\n",
        "print(\"Training time: {}\".format(training_time))\n",
        "\n",
        "\n",
        "predict_main = pd.DataFrame(scalerMain.inverse_transform(y_train_pred_main.detach().cpu().numpy()))\n",
        "# original_main = pd.DataFrame(scalerMain.inverse_transform(train_tensors['y_main'].detach().numpy()))\n",
        "\n",
        "predict_even = pd.DataFrame(scalerEven.inverse_transform(y_train_pred_even.detach().cpu().numpy()))\n",
        "# original_even = pd.DataFrame(scalerEven.inverse_transform(train_tensors['y_even'].detach().numpy()))\n",
        "\n",
        "predict_loss = pd.DataFrame(scalerLoss.inverse_transform(y_train_pred_loss.detach().cpu().numpy()))\n",
        "\n",
        "predict_cls = pd.DataFrame(scalerLoss.inverse_transform(cls_to_draw.detach().cpu().numpy()))\n",
        "# original_loss = pd.DataFrame(scalerLoss.inverse_transform(train_tensors['y_loss'].detach().numpy()))\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# ax = sns.lineplot(x = original_main.index, y = original_main[0], label=\"Data Main\", color='royalblue')\n",
        "ax = sns.lineplot(x = predict_main.index, y = predict_main[0], label=\"Training Prediction Main(LSTM)\", color='tomato')\n",
        "\n",
        "# ax = sns.lineplot(x = original_even.index, y = original_even[0], label=\"Data Even\", color='red')\n",
        "ax = sns.lineplot(x = predict_even.index, y = predict_even[0], label=\"Training Prediction Even(LSTM)\", color='#FF6347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_loss.index, y = predict_loss[0], label=\"Training Prediction Loss(LSTM)\", color='#F31347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_cls.index, y = predict_cls[0], label=\"Training Prediction Class(LSTM)\", color='#a31347')\n",
        "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
        "ax.set_xlabel(\"Days\", size = 14)\n",
        "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
        "ax.set_xticklabels('', size=10)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "ax = sns.lineplot(data=hist, color='royalblue')\n",
        "ax.set_xlabel(\"Epoch\", size = 14)\n",
        "ax.set_ylabel(\"Loss\", size = 14)\n",
        "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(16)\n",
        "\n",
        "# 保存模型的状态字典\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_transformer_weights.pth')\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_transformer_weights_bk.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESFPpJkHgBxL",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title 绘制概率\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 类别标签\n",
        "categories = ['Class 0', 'Class 1', 'Class 2']\n",
        "\n",
        "# 绘制每一行的柱形图\n",
        "for i in range(cls_to_draw.size(0)):\n",
        "    plt.figure()\n",
        "    plt.bar(categories, cls_to_draw[i].detach().numpy(), color=['blue', 'orange', 'green'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel('Probability')\n",
        "    plt.title(f'Class Probability Distribution for Sample {i + 1}')\n",
        "    plt.show()\n",
        "    if i > 5:\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL21FWQuXmCr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpNB_XiZfuRT"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# # 类别标签\n",
        "# categories = ['Class 0', 'Class 1', 'Class 2']\n",
        "# num_samples = cls_to_draw.size(0)\n",
        "\n",
        "# # 设置柱形图的宽度\n",
        "# bar_width = 0.2\n",
        "# x = np.arange(num_samples)\n",
        "\n",
        "# # 绘制柱形图\n",
        "# for i in range(cls_to_draw.size(1)):  # 遍历每个类别\n",
        "#     plt.bar(x + i * bar_width, cls_to_draw[:, i].detach().numpy(), width=bar_width, label=categories[i])\n",
        "\n",
        "# # 设置标签和标题\n",
        "# plt.xlabel('Samples')\n",
        "# plt.ylabel('Probability')\n",
        "# plt.title('Class Probability Distribution')\n",
        "# plt.xticks(x + bar_width, [f'Sample {i + 1}' for i in range(num_samples)])\n",
        "# plt.ylim(0, 1)\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTQEnT_LMqBu"
      },
      "source": [
        "## 验证"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuEsCMi1XmCs"
      },
      "outputs": [],
      "source": [
        "# !mkdir /content/drive/MyDrive/ftTrainDataDetail\n",
        "!mkdir /content/drive/MyDrive/ftTrainDataTest\n",
        "# !rm -rf /content/drive/MyDrive/ftTrainDataDetail/*\n",
        "# !unzip -o /home/tmw/shared/ftTrainData.zip -d /content/drive/MyDrive/ftTrainDataDetail\n",
        "!unzip -o /home/tmw/shared/ftTrainDataTest.zip -d /content/drive/MyDrive/ftTrainDataTest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UijTuZJhp_y"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/output'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "#@title 获取数据\n",
        "\n",
        "# filepath = '/kaggle/input/stock-time-series-20050101-to-20171231/AMZN_2006-01-01_to_2018-01-01.csv'\n",
        "# filepath = '/kaggle/input/amazon.csv'\n",
        "import pandas as pd\n",
        "\n",
        "# file_paths = ['/content/drive/MyDrive/ftTrainDataTest/mw_1244904.csv']\n",
        "file_paths_all = [os.path.join('/content/drive/MyDrive/ftTrainDataTest', f) for f in os.listdir('/content/drive/MyDrive/ftTrainDataTest') if f.endswith('.csv')]\n",
        "\n",
        "\n",
        "for path in file_paths_all:\n",
        "    # 在这里对每个文件路径进行处理\n",
        "    print(path)\n",
        "    file_paths = [path]\n",
        "\n",
        "    dataframes = []\n",
        "    tgtLength = 500\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        df = pd.read_csv(file_path, encoding='latin1')\n",
        "        df = df[::-1]  # 反转 DataFrame 的顺序\n",
        "\n",
        "        # 检查文件大小\n",
        "        if len(df) < tgtLength:\n",
        "            # 计算需要重复的行数\n",
        "            num_rows_to_add = tgtLength - len(df)\n",
        "            last_row = df.iloc[-1:]  # 获取最后一行\n",
        "            # 重复最后一行\n",
        "            repeated_rows = pd.concat([last_row] * num_rows_to_add, ignore_index=True)\n",
        "            df = pd.concat([df, repeated_rows], ignore_index=True)\n",
        "            # 创建补充的行，填充为零\n",
        "            # zero_rows = pd.DataFrame(0, index=range(num_rows_to_add), columns=df.columns)\n",
        "            # df = pd.concat([df, zero_rows], ignore_index=True)\n",
        "        else:\n",
        "            df = df.iloc[:tgtLength]\n",
        "\n",
        "        dataframes.append(df)\n",
        "\n",
        "    # 合并所有数据\n",
        "    data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "\n",
        "    # 保存为 CSV 文件\n",
        "    data.to_csv('hello_output.csv', index=False)\n",
        "\n",
        "    #@title 数据去除特殊符号\n",
        "\n",
        "    dataMain = data[['Main']]\n",
        "    dataEven = data[['Even']]\n",
        "    dataLoss = data[['Loss']]\n",
        "    dataResult = data[['Result']]\n",
        "    dataResultHome = data[['ResultHome']]\n",
        "    dataResultAway = data[['ResultAway']]\n",
        "\n",
        "    dataMain['Main'] = dataMain['Main'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "    dataEven['Even'] = dataEven['Even'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "    dataLoss['Loss'] = dataLoss['Loss'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "\n",
        "    #@title 数据归一化\n",
        "\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "    scalerMain = MinMaxScaler(feature_range=(-1, 1))\n",
        "    scalerEven = MinMaxScaler(feature_range=(-1, 1))\n",
        "    scalerLoss = MinMaxScaler(feature_range=(-1, 1))\n",
        "    scalerResult = MinMaxScaler(feature_range=(-1, 1))\n",
        "    dataMain['Main'] = scalerMain.fit_transform(dataMain['Main'].values.reshape(-1,1))\n",
        "    dataEven['Even'] = scalerEven.fit_transform(dataEven['Even'].values.reshape(-1,1))\n",
        "    dataLoss['Loss'] = scalerLoss.fit_transform(dataLoss['Loss'].values.reshape(-1,1))\n",
        "    dataResult['Result'] = dataResult['Result']\n",
        "    dataResultHome['ResultHome'] = dataResultHome['ResultHome']\n",
        "    dataResultAway['ResultAway'] = dataResultAway['ResultAway']\n",
        "    print(dataMain['Main'] .shape)\n",
        "\n",
        "    #@title 获取准备验证的数据\n",
        "\n",
        "    import math, time\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    input_dim = 1\n",
        "    hidden_dim = 32\n",
        "    num_layers = 2\n",
        "    output_dim = 1\n",
        "    output_cls_dim = 3\n",
        "    num_epochs = 100\n",
        "    batch_size = 2\n",
        "\n",
        "\n",
        "    def split_data(dataMain, dataEven, dataLoss, dataResult, lookback, batch_size = 20):\n",
        "        data_raw_main = dataMain.to_numpy()\n",
        "        data_raw_even = dataEven.to_numpy()\n",
        "        data_raw_loss = dataLoss.to_numpy()\n",
        "        data_raw_result = dataResult.to_numpy()\n",
        "        data_main = []\n",
        "        data_even = []\n",
        "        data_loss = []\n",
        "        data_result = []\n",
        "\n",
        "        # create all possible sequences of length lookback\n",
        "        for index in range(len(data_raw_main) - lookback):\n",
        "            data_main.append(data_raw_main[index: index + lookback])\n",
        "        for index in range(len(data_raw_even) - lookback):\n",
        "            data_even.append(data_raw_even[index: index + lookback])\n",
        "        for index in range(len(data_raw_loss) - lookback):\n",
        "            data_loss.append(data_raw_loss[index: index + lookback])\n",
        "        for index in range(len(data_raw_result) - lookback):\n",
        "            data_result.append(data_raw_result[index: index + lookback])\n",
        "\n",
        "        data_main = np.array(data_main)\n",
        "        data_even = np.array(data_even)\n",
        "        data_loss = np.array(data_loss)\n",
        "        data_result = np.array(data_result)\n",
        "\n",
        "        test_set_size = int(np.round(0.2 * data_main.shape[0]))\n",
        "        train_set_size = data_main.shape[0]\n",
        "\n",
        "        # 将训练数据切割成多个批次\n",
        "        num_batches = train_set_size // batch_size\n",
        "        print(\"num_batches: \", num_batches, \" data_main.shape[0]: \", data_main.shape[0])\n",
        "\n",
        "\n",
        "        x_train = {\n",
        "            'main': np.array_split(data_main[:train_set_size, :-1, :], num_batches),\n",
        "            'even': np.array_split(data_even[:train_set_size, :-1, :], num_batches),\n",
        "            'loss': np.array_split(data_loss[:train_set_size, :-1, :], num_batches),\n",
        "            'result': np.array_split(data_result[:train_set_size, :-1, :], num_batches)\n",
        "        }\n",
        "        y_train = {\n",
        "            'main': np.array_split(data_main[:train_set_size, -1, :], num_batches),\n",
        "            'even': np.array_split(data_even[:train_set_size, -1, :], num_batches),\n",
        "            'loss': np.array_split(data_loss[:train_set_size, -1, :], num_batches),\n",
        "            'result': np.array_split(data_result[:train_set_size, -1, :], num_batches)\n",
        "        }\n",
        "\n",
        "        x_test = {\n",
        "            'main': np.array_split(data_main[train_set_size:, :-1, :], num_batches),\n",
        "            'even': np.array_split(data_even[train_set_size:, :-1, :], num_batches),\n",
        "            'loss': np.array_split(data_loss[train_set_size:, :-1, :], num_batches),\n",
        "            'result': np.array_split(data_result[train_set_size:, :-1, :], num_batches)\n",
        "        }\n",
        "        y_test = {\n",
        "            'main': np.array_split(data_main[train_set_size:, -1, :], num_batches),\n",
        "            'even': np.array_split(data_even[train_set_size:, -1, :], num_batches),\n",
        "            'loss': np.array_split(data_loss[train_set_size:, -1, :], num_batches),\n",
        "            'result': np.array_split(data_result[train_set_size:, -1, :], num_batches)\n",
        "        }\n",
        "        # print(\"x_train['main']: \", x_train['main'][1])\n",
        "        return [x_train, y_train, x_test, y_test]\n",
        "\n",
        "\n",
        "    lookback = 10 # choose sequence length\n",
        "    x_train, y_train, x_test, y_test = split_data(dataMain, dataEven, dataLoss, dataResult, lookback, 490)\n",
        "\n",
        "    # 打印批次的数量和每个批次的形状\n",
        "    print('Number of training batches:', len(x_train['main']))\n",
        "    if len(x_train['main']) > 0:\n",
        "        # print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "        # print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "        print('x_train.shape = ',x_train['main'][0].shape)\n",
        "        print('y_train.shape = ',y_train['main'][0].shape)\n",
        "        print('x_test.shape = ',x_test['main'][0].shape)\n",
        "        print('y_test.shape = ',y_test['main'][0].shape)\n",
        "        print('x_result.shape = ',y_train['result'][0].shape)\n",
        "\n",
        "    # print(\"x_train: \", x_train)\n",
        "    # print(\"y_train: \", y_train)\n",
        "\n",
        "    # x_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train_batches]\n",
        "    # y_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train_batches]\n",
        "    import torch\n",
        "\n",
        "    # 转换为字典\n",
        "    train_tensors = {\n",
        "        'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['main']],\n",
        "        'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['even']],\n",
        "        'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['loss']],\n",
        "        'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['result']],\n",
        "        'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['main']],\n",
        "        'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['even']],\n",
        "        'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['loss']],\n",
        "        'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['result']]\n",
        "    }\n",
        "\n",
        "    test_tensors = {\n",
        "        'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['main']],\n",
        "        'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['even']],\n",
        "        'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['loss']],\n",
        "        'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['result']],\n",
        "        'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['main']],\n",
        "        'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['even']],\n",
        "        'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['loss']],\n",
        "        'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['result']]\n",
        "    }\n",
        "\n",
        "    for x_main_batch, x_even_batch, x_loss_batch, y_main_batch, y_even_batch, y_loss_batch, y_result_batch in zip(train_tensors['x_main'], train_tensors['x_even'], train_tensors['x_loss'], train_tensors['y_main'], train_tensors['y_even'], train_tensors['y_loss'], train_tensors['y_result']):\n",
        "\n",
        "        # 将数据移动到设备\n",
        "        x_main_batch = x_main_batch.to(device)\n",
        "        x_even_batch = x_even_batch.to(device)\n",
        "        x_loss_batch = x_loss_batch.to(device)\n",
        "        y_main_batch = y_main_batch.to(device)\n",
        "        y_even_batch = y_even_batch.to(device)\n",
        "        y_loss_batch = y_loss_batch.to(device)\n",
        "        y_result_batch = y_result_batch.to(device)\n",
        "        # print(\"ok: \")\n",
        "        # 替换前10个元素的内容为第11个元素的内容\n",
        "        if len(x_main_batch) > 10:  # 确保有足够的元素\n",
        "            x_main_batch[:10] = x_main_batch[10:11]\n",
        "        # print(\"x_main_batch: \", x_main_batch)\n",
        "        model.eval()\n",
        "        # y_test_main_pred, cls_main = model(x_main_batch)\n",
        "        # y_test_even_pred, cls_even = model(x_even_batch)\n",
        "        # y_test_loss_pred, cls_loss = model(x_loss_batch)\n",
        "\n",
        "        y_test_main_pred, cls_main, cls_main_main, cls_main_loss = model(x_main_batch)\n",
        "        y_test_even_pred, cls_even, cls_even_main, cls_even_loss = model(x_even_batch)\n",
        "        y_test_loss_pred, cls_loss, cls_loss_main, cls_loss_loss = model(x_loss_batch)\n",
        "        tmp = torch.softmax(cls_main + cls_even + cls_loss, dim=1)\n",
        "        cls_to_draw = tmp\n",
        "\n",
        "        tmpMain = torch.softmax(cls_main_main + cls_even_main + cls_loss_main, dim=1)\n",
        "        cls_to_draw_main = tmpMain\n",
        "        tmpLoss = torch.softmax(cls_main_loss + cls_even_loss + cls_loss_loss, dim=1)\n",
        "        cls_to_draw_loss = tmpLoss\n",
        "\n",
        "    predict_main = pd.DataFrame(scalerMain.inverse_transform(y_test_main_pred.detach().cpu().numpy()))\n",
        "    predict_even = pd.DataFrame(scalerEven.inverse_transform(y_test_even_pred.detach().cpu().numpy()))\n",
        "    predict_loss = pd.DataFrame(scalerLoss.inverse_transform(y_test_loss_pred.detach().cpu().numpy()))\n",
        "\n",
        "    import seaborn as sns\n",
        "    sns.set_style(\"darkgrid\")\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    ax = sns.lineplot(x = predict_main.index, y = predict_main[0], label=\"Training Prediction Main(LSTM)\", color='tomato')\n",
        "    ax = sns.lineplot(x = predict_even.index, y = predict_even[0], label=\"Training Prediction Even(LSTM)\", color='#FF6347')\n",
        "    ax = sns.lineplot(x = predict_loss.index, y = predict_loss[0], label=\"Training Prediction Loss(LSTM)\", color='#F31347')\n",
        "\n",
        "    ax.set_title('Stock price', size = 14, fontweight='bold')\n",
        "    ax.set_xlabel(\"Days\", size = 14)\n",
        "    ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
        "    ax.set_xticklabels('', size=10)\n",
        "\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    ax = sns.lineplot(data=hist, color='royalblue')\n",
        "    ax.set_xlabel(\"Epoch\", size = 14)\n",
        "    ax.set_ylabel(\"Loss\", size = 14)\n",
        "    ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
        "    fig.set_figheight(6)\n",
        "    fig.set_figwidth(16)\n",
        "\n",
        "    import torch\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "    # 类别标签\n",
        "    categories = ['Class 0', 'Class 1', 'Class 2']\n",
        "\n",
        "    # 绘制每一行的柱形图\n",
        "    for i in range(cls_to_draw.size(0)):\n",
        "        print(dataResult['Result'])\n",
        "        print(path)\n",
        "        plt.figure()\n",
        "        plt.bar(categories, cls_to_draw[i].detach().cpu().numpy(), color=['blue', 'orange', 'green'])\n",
        "        plt.ylim(0, 1)\n",
        "        plt.ylabel('Probability')\n",
        "        plt.title(f'Class Probability Distribution for Sample {i + 1}')\n",
        "        plt.show()\n",
        "        if i > 3:\n",
        "            break\n",
        "\n",
        "\n",
        "    # 类别标签\n",
        "    categories = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "\n",
        "    # 绘制每一行的柱形图\n",
        "    for i in range(cls_to_draw_main.size(0)):\n",
        "        print(dataResultHome['ResultHome'])\n",
        "        print(path)\n",
        "        plt.figure()\n",
        "        plt.bar(categories, cls_to_draw_main[i].detach().cpu().numpy(), color=['blue', 'orange', 'green', 'orange', 'green', 'orange', 'green', 'orange', 'green', 'orange'])\n",
        "        plt.ylim(0, 1)\n",
        "        plt.ylabel('Probability')\n",
        "        plt.title(f'Class Probability Distribution for Sample {i + 1}')\n",
        "        plt.show()\n",
        "        if i > 3:\n",
        "            break\n",
        "\n",
        "    print(\"=========================================================\")\n",
        "    print(\"=========================================================\")\n",
        "    print(\"=========================================================\")\n",
        "    print(\"=========================================================\")\n",
        "\n",
        "    # 类别标签\n",
        "    categories = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "\n",
        "    # 绘制每一行的柱形图\n",
        "    for i in range(cls_to_draw_loss.size(0)):\n",
        "        print(dataResultAway['ResultAway'])\n",
        "        print(path)\n",
        "        plt.figure()\n",
        "        plt.bar(categories, cls_to_draw_loss[i].detach().cpu().numpy(), color=['blue', 'orange', 'green', 'orange', 'green', 'orange', 'green', 'orange', 'green', 'orange'])\n",
        "        plt.ylim(0, 1)\n",
        "        plt.ylabel('Probability')\n",
        "        plt.title(f'Class Probability Distribution for Sample {i + 1}')\n",
        "        plt.show()\n",
        "        if i > 3:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5m7Jdn5rqAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLEZWHXwESPV"
      },
      "outputs": [],
      "source": [
        "#@title 绘制图像\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 类别标签\n",
        "categories = ['Class 0', 'Class 1', 'Class 2']\n",
        "\n",
        "# 绘制每一行的柱形图\n",
        "for i in range(cls_to_draw.size(0)):\n",
        "    plt.figure()\n",
        "    plt.bar(categories, cls_to_draw[i].detach().cpu().numpy(), color=['blue', 'orange', 'green'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel('Probability')\n",
        "    plt.title(f'Class Probability Distribution for Sample {i + 1}')\n",
        "    plt.show()\n",
        "    if i > 5:\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6pm3DN8D5-F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 类别标签\n",
        "categories = ['Class 0', 'Class 1', 'Class 2']\n",
        "\n",
        "# 绘制每一行的柱形图\n",
        "for i in range(cls_to_draw.size(0)):\n",
        "    plt.figure()\n",
        "    plt.bar(categories, cls_to_draw[i].detach().numpy(), color=['blue', 'orange', 'green'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel('Probability')\n",
        "    plt.title(f'Class Probability Distribution for Sample {i + 1}')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePRLskNmhp_y"
      },
      "outputs": [],
      "source": [
        "# shift train predictions for plotting\n",
        "trainPredictPlot = np.empty_like(price)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[lookback:len(y_train_pred)+lookback, :] = y_train_pred\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = np.empty_like(price)\n",
        "testPredictPlot[:, :] = np.nan\n",
        "testPredictPlot[len(y_train_pred)+lookback-1:len(price)-1, :] = y_test_pred\n",
        "\n",
        "original = scaler.inverse_transform(price['Close'].values.reshape(-1,1))\n",
        "\n",
        "predictions = np.append(trainPredictPlot, testPredictPlot, axis=1)\n",
        "predictions = np.append(predictions, original, axis=1)\n",
        "result = pd.DataFrame(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tH05FABhp_z"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[0],\n",
        "                    mode='lines',\n",
        "                    name='Train prediction')))\n",
        "fig.add_trace(go.Scatter(x=result.index, y=result[1],\n",
        "                    mode='lines',\n",
        "                    name='Test prediction'))\n",
        "fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[2],\n",
        "                    mode='lines',\n",
        "                    name='Actual Value')))\n",
        "fig.update_layout(\n",
        "    xaxis=dict(\n",
        "        showline=True,\n",
        "        showgrid=True,\n",
        "        showticklabels=False,\n",
        "        linecolor='white',\n",
        "        linewidth=2\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        title_text='Close (USD)',\n",
        "        titlefont=dict(\n",
        "            family='Rockwell',\n",
        "            size=12,\n",
        "            color='white',\n",
        "        ),\n",
        "        showline=True,\n",
        "        showgrid=True,\n",
        "        showticklabels=True,\n",
        "        linecolor='white',\n",
        "        linewidth=2,\n",
        "        ticks='outside',\n",
        "        tickfont=dict(\n",
        "            family='Rockwell',\n",
        "            size=12,\n",
        "            color='white',\n",
        "        ),\n",
        "    ),\n",
        "    showlegend=True,\n",
        "    template = 'plotly_dark'\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "annotations = []\n",
        "annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n",
        "                              xanchor='left', yanchor='bottom',\n",
        "                              text='Results (LSTM)',\n",
        "                              font=dict(family='Rockwell',\n",
        "                                        size=26,\n",
        "                                        color='white'),\n",
        "                              showarrow=False))\n",
        "fig.update_layout(annotations=annotations)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibf2gv4Qhp_z"
      },
      "outputs": [],
      "source": [
        "!pip install chart-studio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXr3BZs6hp_z"
      },
      "outputs": [],
      "source": [
        "# import chart_studio.plotly as py\n",
        "# import chart_studio\n",
        "\n",
        "# chart_studio.tools.set_credentials_file(username='rodolfo_saldanha', api_key='zWJIVWJs23wfiAp516Mh')\n",
        "# py.iplot(fig, filename='stock_prediction_lstm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjpe8i-Php_z"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
        "        out, (hn) = self.gru(x, (h0.detach()))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfWFqY2nhp_0"
      },
      "outputs": [],
      "source": [
        "model = GRU(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhuqmiqMhp_0"
      },
      "outputs": [],
      "source": [
        "hist = np.zeros(num_epochs)\n",
        "start_time = time.time()\n",
        "gru = []\n",
        "\n",
        "for t in range(num_epochs):\n",
        "    y_train_pred = model(x_train)\n",
        "\n",
        "    loss = criterion(y_train_pred, y_train_gru)\n",
        "    print(\"Epoch \", t, \"MSE: \", loss.item())\n",
        "    hist[t] = loss.item()\n",
        "\n",
        "    optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "training_time = time.time()-start_time\n",
        "print(\"Training time: {}\".format(training_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeuZe2BRhp_0"
      },
      "outputs": [],
      "source": [
        "predict = pd.DataFrame(scaler.inverse_transform(y_train_pred.detach().numpy()))\n",
        "original = pd.DataFrame(scaler.inverse_transform(y_train_gru.detach().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf5fFYRWhp_0"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "ax = sns.lineplot(x = original.index, y = original[0], label=\"Data\", color='royalblue')\n",
        "ax = sns.lineplot(x = predict.index, y = predict[0], label=\"Training Prediction (GRU)\", color='tomato')\n",
        "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
        "ax.set_xlabel(\"Days\", size = 14)\n",
        "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
        "ax.set_xticklabels('', size=10)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "ax = sns.lineplot(data=hist, color='royalblue')\n",
        "ax.set_xlabel(\"Epoch\", size = 14)\n",
        "ax.set_ylabel(\"Loss\", size = 14)\n",
        "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVjVWgzThp_1"
      },
      "outputs": [],
      "source": [
        "import math, time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# make predictions\n",
        "y_test_pred = model(x_test)\n",
        "\n",
        "# invert predictions\n",
        "y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())\n",
        "y_train = scaler.inverse_transform(y_train_gru.detach().numpy())\n",
        "y_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy())\n",
        "y_test = scaler.inverse_transform(y_test_gru.detach().numpy())\n",
        "\n",
        "# calculate root mean squared error\n",
        "trainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0]))\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "testScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0]))\n",
        "print('Test Score: %.2f RMSE' % (testScore))\n",
        "gru.append(trainScore)\n",
        "gru.append(testScore)\n",
        "gru.append(training_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0SJoPGthp_1"
      },
      "outputs": [],
      "source": [
        "# shift train predictions for plotting\n",
        "trainPredictPlot = np.empty_like(price)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[lookback:len(y_train_pred)+lookback, :] = y_train_pred\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = np.empty_like(price)\n",
        "testPredictPlot[:, :] = np.nan\n",
        "testPredictPlot[len(y_train_pred)+lookback-1:len(price)-1, :] = y_test_pred\n",
        "\n",
        "original = scaler.inverse_transform(price['Close'].values.reshape(-1,1))\n",
        "\n",
        "predictions = np.append(trainPredictPlot, testPredictPlot, axis=1)\n",
        "predictions = np.append(predictions, original, axis=1)\n",
        "result = pd.DataFrame(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHNND6KThp_1"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[0],\n",
        "                    mode='lines',\n",
        "                    name='Train prediction')))\n",
        "fig.add_trace(go.Scatter(x=result.index, y=result[1],\n",
        "                    mode='lines',\n",
        "                    name='Test prediction'))\n",
        "fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[2],\n",
        "                    mode='lines',\n",
        "                    name='Actual Value')))\n",
        "fig.update_layout(\n",
        "    xaxis=dict(\n",
        "        showline=True,\n",
        "        showgrid=True,\n",
        "        showticklabels=False,\n",
        "        linecolor='white',\n",
        "        linewidth=2\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        title_text='Close (USD)',\n",
        "        titlefont=dict(\n",
        "            family='Rockwell',\n",
        "            size=12,\n",
        "            color='white',\n",
        "        ),\n",
        "        showline=True,\n",
        "        showgrid=True,\n",
        "        showticklabels=True,\n",
        "        linecolor='white',\n",
        "        linewidth=2,\n",
        "        ticks='outside',\n",
        "        tickfont=dict(\n",
        "            family='Rockwell',\n",
        "            size=12,\n",
        "            color='white',\n",
        "        ),\n",
        "    ),\n",
        "    showlegend=True,\n",
        "    template = 'plotly_dark'\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "annotations = []\n",
        "annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n",
        "                              xanchor='left', yanchor='bottom',\n",
        "                              text='Results (GRU)',\n",
        "                              font=dict(family='Rockwell',\n",
        "                                        size=26,\n",
        "                                        color='white'),\n",
        "                              showarrow=False))\n",
        "fig.update_layout(annotations=annotations)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrpcQQ68hp_1"
      },
      "outputs": [],
      "source": [
        "lstm = pd.DataFrame(lstm, columns=['LSTM'])\n",
        "gru = pd.DataFrame(gru, columns=['GRU'])\n",
        "result = pd.concat([lstm, gru], axis=1, join='inner')\n",
        "result.index = ['Train RMSE', 'Test RMSE', 'Train Time']\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k92uuQ4hp_2"
      },
      "outputs": [],
      "source": [
        "py.iplot(fig, filename='stock_prediction_gru')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}