{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title 链接Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        ""
      ],
      "metadata": {
        "id": "A6anRp8jeuTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 获取数据"
      ],
      "metadata": {
        "id": "7KPXkrGlTxOV"
      }
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "CbHXI5I4hp_o"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "#@title 拼接数据\n",
        "\n",
        "# filepath = '/kaggle/input/stock-time-series-20050101-to-20171231/AMZN_2006-01-01_to_2018-01-01.csv'\n",
        "# filepath = '/kaggle/input/amazon.csv'\n",
        "import pandas as pd\n",
        "\n",
        "# 获取/content/hello目录下所有的CSV文件\n",
        "file_paths = [os.path.join('/content/drive/MyDrive/ftTrainData', f) for f in os.listdir('/content/drive/MyDrive/ftTrainData') if f.endswith('.csv')]\n",
        "\n",
        "dataframes = []\n",
        "tgtLength = 500\n",
        "\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path, encoding='latin1')\n",
        "    df = df[::-1]  # 反转 DataFrame 的顺序\n",
        "\n",
        "    if len(df) == 0:\n",
        "        continue\n",
        "    # 检查文件大小\n",
        "    if len(df) < tgtLength:\n",
        "        # 计算需要重复的行数\n",
        "        num_rows_to_add = tgtLength - len(df)\n",
        "        last_row = df.iloc[-1:]  # 获取最后一行\n",
        "        # 重复最后一行\n",
        "        repeated_rows = pd.concat([last_row] * num_rows_to_add, ignore_index=True)\n",
        "        df = pd.concat([df, repeated_rows], ignore_index=True)\n",
        "        # 创建补充的行，填充为零\n",
        "        # zero_rows = pd.DataFrame(0, index=range(num_rows_to_add), columns=df.columns)\n",
        "        # df = pd.concat([df, zero_rows], ignore_index=True)\n",
        "    else:\n",
        "        df = df.iloc[:tgtLength]\n",
        "\n",
        "    dataframes.append(df)\n",
        "\n",
        "# 合并所有数据\n",
        "data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "\n",
        "# 保存为 CSV 文件\n",
        "data.to_csv('hello.csv', index=False)\n",
        "\n",
        "#@title 数据去除特殊符号\n",
        "\n",
        "dataMain = data[['Main']]\n",
        "dataEven = data[['Even']]\n",
        "dataLoss = data[['Loss']]\n",
        "dataResult = data[['Result']]\n",
        "\n",
        "dataMain['Main'] = dataMain['Main'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "dataEven['Even'] = dataEven['Even'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "dataLoss['Loss'] = dataLoss['Loss'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "\n",
        "#@title 数据归一化\n",
        "\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# scalerMain = MinMaxScaler(feature_range=(-1, 1))\n",
        "# scalerEven = MinMaxScaler(feature_range=(-1, 1))\n",
        "# scalerLoss = MinMaxScaler(feature_range=(-1, 1))\n",
        "# scalerResult = MinMaxScaler(feature_range=(-1, 1))\n",
        "# dataMain['Main'] = scalerMain.fit_transform(dataMain['Main'].values.reshape(-1,1))\n",
        "# dataEven['Even'] = scalerEven.fit_transform(dataEven['Even'].values.reshape(-1,1))\n",
        "# dataLoss['Loss'] = scalerLoss.fit_transform(dataLoss['Loss'].values.reshape(-1,1))\n",
        "# dataResult['Result'] = dataResult['Result']\n",
        "# print(dataMain['Main'] .shape)\n",
        "# dataMain['Main']\n",
        "\n",
        "def scale_in_chunks(data, chunk_size=500):\n",
        "    scalers = []\n",
        "    scaled_data = []\n",
        "\n",
        "    # 将数据分成若干块\n",
        "    data_chunks = np.array_split(data, np.arange(chunk_size, len(data), chunk_size))\n",
        "\n",
        "    for chunk in data_chunks:\n",
        "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "        scaled_chunk = scaler.fit_transform(chunk.values.reshape(-1, 1))\n",
        "        scalers.append(scaler)  # 如果需要在后续反向转换时使用\n",
        "        scaled_data.append(scaled_chunk)\n",
        "\n",
        "    return np.vstack(scaled_data), scalers\n",
        "\n",
        "# 对每个数据集进行分块缩放\n",
        "dataMain['Main'], scalers_main = scale_in_chunks(dataMain['Main'])\n",
        "dataEven['Even'], scalers_even = scale_in_chunks(dataEven['Even'])\n",
        "dataLoss['Loss'], scalers_loss = scale_in_chunks(dataLoss['Loss'])\n",
        "dataMain['Main']\n",
        "# dataResult['Result'] 保持原样"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8ehyV_uihp_s"
      },
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# sns.set_style(\"darkgrid\")\n",
        "# plt.figure(figsize = (15,9))\n",
        "# plt.plot(data[['Main']])\n",
        "# plt.plot(data[['Even']])\n",
        "# plt.plot(data[['Loss']])\n",
        "# plt.xticks(range(0,data.shape[0],500),data['Date'].loc[::500],rotation=45)\n",
        "# plt.title(\"Amazon Stock Price\",fontsize=18, fontweight='bold')\n",
        "# plt.xlabel('Date',fontsize=18)\n",
        "# plt.ylabel('Close Price (USD)',fontsize=18)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rrjH2cFEhp_s"
      },
      "cell_type": "markdown",
      "source": [
        "## 数据处理"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "fx4S1tcNhp_x"
      },
      "cell_type": "code",
      "source": [
        "#@title 训练模型\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_dim = 1\n",
        "hidden_dim = 32\n",
        "num_layers = 2\n",
        "output_dim = 1\n",
        "output_cls_dim = 3\n",
        "num_epochs = 100\n",
        "batch_size = 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def split_data(dataMain, dataEven, dataLoss, dataResult, lookback, batch_size = 20):\n",
        "    data_raw_main = dataMain.to_numpy()\n",
        "    data_raw_even = dataEven.to_numpy()\n",
        "    data_raw_loss = dataLoss.to_numpy()\n",
        "    data_raw_result = dataResult.to_numpy()\n",
        "    data_main = []\n",
        "    data_even = []\n",
        "    data_loss = []\n",
        "    data_result = []\n",
        "\n",
        "    # create all possible sequences of length lookback\n",
        "    for index in range(len(data_raw_main) - lookback):\n",
        "        data_main.append(data_raw_main[index: index + lookback])\n",
        "    for index in range(len(data_raw_even) - lookback):\n",
        "        data_even.append(data_raw_even[index: index + lookback])\n",
        "    for index in range(len(data_raw_loss) - lookback):\n",
        "        data_loss.append(data_raw_loss[index: index + lookback])\n",
        "    for index in range(len(data_raw_result) - lookback):\n",
        "        data_result.append(data_raw_result[index: index + lookback])\n",
        "\n",
        "    data_main = np.array(data_main)\n",
        "    data_even = np.array(data_even)\n",
        "    data_loss = np.array(data_loss)\n",
        "    data_result = np.array(data_result)\n",
        "\n",
        "    test_set_size = int(np.round(0.2 * data_main.shape[0]))\n",
        "    train_set_size = data_main.shape[0]\n",
        "\n",
        "    # 将训练数据切割成多个批次\n",
        "    num_batches = train_set_size // batch_size\n",
        "    print(\"num_batches: \", num_batches, \" data_main.shape[0]: \", data_main.shape[0])\n",
        "\n",
        "\n",
        "    x_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, :-1, :], num_batches)\n",
        "    }\n",
        "    y_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, -1, :], num_batches)\n",
        "    }\n",
        "\n",
        "    x_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, :-1, :], num_batches)\n",
        "    }\n",
        "    y_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, -1, :], num_batches)\n",
        "    }\n",
        "    # print(\"x_train['main']: \", x_train['main'][1])\n",
        "    return [x_train, y_train, x_test, y_test]\n",
        "\n",
        "\n",
        "lookback = 10 # choose sequence length\n",
        "x_train, y_train, x_test, y_test = split_data(dataMain, dataEven, dataLoss, dataResult, lookback, 495)\n",
        "# x_train_batches, y_train_batches, x_test, y_test = split_data(price, lookback=20, batch_size=32)\n",
        "\n",
        "# # 打印批次的数量和每个批次的形状\n",
        "# print('Number of training batches:', len(x_train_batches))\n",
        "# if len(x_train_batches) > 0:\n",
        "#     print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "#     print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "# print('x_test.shape = ', x_test.shape)\n",
        "# print('y_test.shape = ', y_test.shape)\n",
        "\n",
        "# 打印批次的数量和每个批次的形状\n",
        "print('Number of training batches:', len(x_train['main']))\n",
        "if len(x_train['main']) > 0:\n",
        "    # print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "    # print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "    print('x_train.shape = ',x_train['main'][0].shape)\n",
        "    print('y_train.shape = ',y_train['main'][0].shape)\n",
        "    print('x_test.shape = ',x_test['main'][0].shape)\n",
        "    print('y_test.shape = ',y_test['main'][0].shape)\n",
        "    print('x_result.shape = ',y_train['result'][0].shape)\n",
        "\n",
        "# print(\"x_train: \", x_train)\n",
        "# print(\"y_train: \", y_train)\n",
        "\n",
        "# x_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train_batches]\n",
        "# y_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train_batches]\n",
        "import torch\n",
        "\n",
        "# 转换为字典\n",
        "train_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['result']]\n",
        "}\n",
        "\n",
        "test_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['result']]\n",
        "}\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.fcCls = nn.Linear(hidden_dim, output_cls_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "        out1 = self.fc(out[:, -1, :])\n",
        "\n",
        "        # 通过全连接层\n",
        "        outCls = self.fcCls(out[:, -1, :])\n",
        "        # 应用 softmax 激活\n",
        "        outCls = torch.softmax(outCls, dim=1)\n",
        "\n",
        "        return out1, outCls\n",
        "\n",
        "\n",
        "\n",
        "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterionCls = nn.CrossEntropyLoss()\n",
        "# optimizerCls = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "from re import X\n",
        "import time\n",
        "\n",
        "hist = np.zeros(num_epochs)\n",
        "start_time = time.time()\n",
        "lstm = []\n",
        "\n",
        "y_train_pred_main = None\n",
        "y_train_pred_even = None\n",
        "y_train_pred_loss = None\n",
        "cls_to_draw = None\n",
        "\n",
        "for t in range(num_epochs):\n",
        "\n",
        "    for x_main_batch, x_even_batch, x_loss_batch, y_main_batch, y_even_batch, y_loss_batch, y_result_batch in zip(train_tensors['x_main'], train_tensors['x_even'], train_tensors['x_loss'], train_tensors['y_main'], train_tensors['y_even'], train_tensors['y_loss'], train_tensors['y_result']):\n",
        "\n",
        "        x_main_batch = x_main_batch.to(device)\n",
        "        x_even_batch = x_even_batch.to(device)\n",
        "        x_loss_batch = x_loss_batch.to(device)\n",
        "        y_main_batch = y_main_batch.to(device)\n",
        "        y_even_batch = y_even_batch.to(device)\n",
        "        y_loss_batch = y_loss_batch.to(device)\n",
        "        y_result_batch = y_result_batch.to(device)\n",
        "        # print(\"ok: \")\n",
        "        # 替换前10个元素的内容为第11个元素的内容\n",
        "        if len(x_main_batch) > 10:  # 确保有足够的元素\n",
        "            x_main_batch[:10] = x_main_batch[10:11]\n",
        "        # print(\"x_main_batch: \", x_main_batch)\n",
        "        y_train_pred_main, cls_main = model(x_main_batch)\n",
        "        y_train_pred_even, cls_even = model(x_even_batch)\n",
        "        y_train_pred_loss, cls_loss = model(x_loss_batch)\n",
        "\n",
        "        loss_main = criterion(y_train_pred_main, y_main_batch)\n",
        "        loss_even = criterion(y_train_pred_even, y_even_batch)\n",
        "        loss_loss = criterion(y_train_pred_loss, y_loss_batch)\n",
        "\n",
        "        tmp = torch.softmax(cls_main + cls_even + cls_loss, dim=1)\n",
        "        cls_to_draw = tmp\n",
        "\n",
        "        loss_result = criterionCls(tmp, y_result_batch.squeeze().long())\n",
        "        loss = loss_main + loss_even + loss_loss + loss_result\n",
        "\n",
        "        # print(\"Epoch \", t, \"MSE: \", loss.item())\n",
        "        hist[t] = loss.item()\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "    print(\"Epoch \", t, \"MSE: \", loss.item(), \"cls_main + cls_even + cls_loss: \", tmp)\n",
        "\n",
        "training_time = time.time()-start_time\n",
        "print(\"Training time: {}\".format(training_time))\n",
        "\n",
        "\n",
        "predict_main = pd.DataFrame(scalerMain.inverse_transform(y_train_pred_main.detach().numpy()))\n",
        "# original_main = pd.DataFrame(scalerMain.inverse_transform(train_tensors['y_main'].detach().numpy()))\n",
        "\n",
        "predict_even = pd.DataFrame(scalerEven.inverse_transform(y_train_pred_even.detach().numpy()))\n",
        "# original_even = pd.DataFrame(scalerEven.inverse_transform(train_tensors['y_even'].detach().numpy()))\n",
        "\n",
        "predict_loss = pd.DataFrame(scalerLoss.inverse_transform(y_train_pred_loss.detach().numpy()))\n",
        "\n",
        "predict_cls = pd.DataFrame(scalerLoss.inverse_transform(cls_to_draw.detach().numpy()))\n",
        "# original_loss = pd.DataFrame(scalerLoss.inverse_transform(train_tensors['y_loss'].detach().numpy()))\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# ax = sns.lineplot(x = original_main.index, y = original_main[0], label=\"Data Main\", color='royalblue')\n",
        "ax = sns.lineplot(x = predict_main.index, y = predict_main[0], label=\"Training Prediction Main(LSTM)\", color='tomato')\n",
        "\n",
        "# ax = sns.lineplot(x = original_even.index, y = original_even[0], label=\"Data Even\", color='red')\n",
        "ax = sns.lineplot(x = predict_even.index, y = predict_even[0], label=\"Training Prediction Even(LSTM)\", color='#FF6347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_loss.index, y = predict_loss[0], label=\"Training Prediction Loss(LSTM)\", color='#F31347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_cls.index, y = predict_cls[0], label=\"Training Prediction Class(LSTM)\", color='#a31347')\n",
        "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
        "ax.set_xlabel(\"Days\", size = 14)\n",
        "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
        "ax.set_xticklabels('', size=10)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "ax = sns.lineplot(data=hist, color='royalblue')\n",
        "ax.set_xlabel(\"Epoch\", size = 14)\n",
        "ax.set_ylabel(\"Loss\", size = 14)\n",
        "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 绘制概率\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 类别标签\n",
        "categories = ['Class 0', 'Class 1', 'Class 2']\n",
        "\n",
        "# 绘制每一行的柱形图\n",
        "for i in range(cls_to_draw.size(0)):\n",
        "    plt.figure()\n",
        "    plt.bar(categories, cls_to_draw[i].detach().numpy(), color=['blue', 'orange', 'green'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel('Probability')\n",
        "    plt.title(f'Class Probability Distribution for Sample {i + 1}')\n",
        "    plt.show()\n",
        "    if i > 5:\n",
        "        break\n",
        "\n"
      ],
      "metadata": {
        "id": "ESFPpJkHgBxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# # 类别标签\n",
        "# categories = ['Class 0', 'Class 1', 'Class 2']\n",
        "# num_samples = cls_to_draw.size(0)\n",
        "\n",
        "# # 设置柱形图的宽度\n",
        "# bar_width = 0.2\n",
        "# x = np.arange(num_samples)\n",
        "\n",
        "# # 绘制柱形图\n",
        "# for i in range(cls_to_draw.size(1)):  # 遍历每个类别\n",
        "#     plt.bar(x + i * bar_width, cls_to_draw[:, i].detach().numpy(), width=bar_width, label=categories[i])\n",
        "\n",
        "# # 设置标签和标题\n",
        "# plt.xlabel('Samples')\n",
        "# plt.ylabel('Probability')\n",
        "# plt.title('Class Probability Distribution')\n",
        "# plt.xticks(x + bar_width, [f'Sample {i + 1}' for i in range(num_samples)])\n",
        "# plt.ylim(0, 1)\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "BpNB_XiZfuRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 验证"
      ],
      "metadata": {
        "id": "dTQEnT_LMqBu"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_UijTuZJhp_y"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/output'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "#@title 获取数据\n",
        "\n",
        "# filepath = '/kaggle/input/stock-time-series-20050101-to-20171231/AMZN_2006-01-01_to_2018-01-01.csv'\n",
        "# filepath = '/kaggle/input/amazon.csv'\n",
        "import pandas as pd\n",
        "\n",
        "file_paths = [f'/kaggle/output/amazon_copy_{i + 1}.csv' for i in range(1)]\n",
        "dataframes = []\n",
        "tgtLength = 500\n",
        "\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path)\n",
        "    df = df[::-1]  # 反转 DataFrame 的顺序\n",
        "\n",
        "    # 检查文件大小\n",
        "    if len(df) < tgtLength:\n",
        "        # 计算需要重复的行数\n",
        "        num_rows_to_add = tgtLength - len(df)\n",
        "        last_row = df.iloc[-1:]  # 获取最后一行\n",
        "        # 重复最后一行\n",
        "        repeated_rows = pd.concat([last_row] * num_rows_to_add, ignore_index=True)\n",
        "        df = pd.concat([df, repeated_rows], ignore_index=True)\n",
        "        # 创建补充的行，填充为零\n",
        "        # zero_rows = pd.DataFrame(0, index=range(num_rows_to_add), columns=df.columns)\n",
        "        # df = pd.concat([df, zero_rows], ignore_index=True)\n",
        "    else:\n",
        "        df = df.iloc[:tgtLength]\n",
        "\n",
        "    dataframes.append(df)\n",
        "\n",
        "# 合并所有数据\n",
        "data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "\n",
        "# 保存为 CSV 文件\n",
        "data.to_csv('hello_output.csv', index=False)\n",
        "\n",
        "#@title 数据去除特殊符号\n",
        "\n",
        "dataMain = data[['Main']]\n",
        "dataEven = data[['Even']]\n",
        "dataLoss = data[['Loss']]\n",
        "dataResult = data[['Result']]\n",
        "\n",
        "dataMain['Main'] = dataMain['Main'].str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "dataEven['Even'] = dataEven['Even'].str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "dataLoss['Loss'] = dataLoss['Loss'].str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "\n",
        "#@title 数据归一化\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scalerMain = MinMaxScaler(feature_range=(-1, 1))\n",
        "scalerEven = MinMaxScaler(feature_range=(-1, 1))\n",
        "scalerLoss = MinMaxScaler(feature_range=(-1, 1))\n",
        "scalerResult = MinMaxScaler(feature_range=(-1, 1))\n",
        "dataMain['Main'] = scalerMain.fit_transform(dataMain['Main'].values.reshape(-1,1))\n",
        "dataEven['Even'] = scalerEven.fit_transform(dataEven['Even'].values.reshape(-1,1))\n",
        "dataLoss['Loss'] = scalerLoss.fit_transform(dataLoss['Loss'].values.reshape(-1,1))\n",
        "dataResult['Result'] = dataResult['Result']\n",
        "print(dataMain['Main'] .shape)\n",
        "\n",
        "#@title 获取准备验证的数据\n",
        "\n",
        "import math, time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "input_dim = 1\n",
        "hidden_dim = 32\n",
        "num_layers = 2\n",
        "output_dim = 1\n",
        "output_cls_dim = 3\n",
        "num_epochs = 100\n",
        "batch_size = 2\n",
        "\n",
        "\n",
        "def split_data(dataMain, dataEven, dataLoss, dataResult, lookback, batch_size = 20):\n",
        "    data_raw_main = dataMain.to_numpy()\n",
        "    data_raw_even = dataEven.to_numpy()\n",
        "    data_raw_loss = dataLoss.to_numpy()\n",
        "    data_raw_result = dataResult.to_numpy()\n",
        "    data_main = []\n",
        "    data_even = []\n",
        "    data_loss = []\n",
        "    data_result = []\n",
        "\n",
        "    # create all possible sequences of length lookback\n",
        "    for index in range(len(data_raw_main) - lookback):\n",
        "        data_main.append(data_raw_main[index: index + lookback])\n",
        "    for index in range(len(data_raw_even) - lookback):\n",
        "        data_even.append(data_raw_even[index: index + lookback])\n",
        "    for index in range(len(data_raw_loss) - lookback):\n",
        "        data_loss.append(data_raw_loss[index: index + lookback])\n",
        "    for index in range(len(data_raw_result) - lookback):\n",
        "        data_result.append(data_raw_result[index: index + lookback])\n",
        "\n",
        "    data_main = np.array(data_main)\n",
        "    data_even = np.array(data_even)\n",
        "    data_loss = np.array(data_loss)\n",
        "    data_result = np.array(data_result)\n",
        "\n",
        "    test_set_size = int(np.round(0.2 * data_main.shape[0]))\n",
        "    train_set_size = data_main.shape[0]\n",
        "\n",
        "    # 将训练数据切割成多个批次\n",
        "    num_batches = train_set_size // batch_size\n",
        "    print(\"num_batches: \", num_batches, \" data_main.shape[0]: \", data_main.shape[0])\n",
        "\n",
        "\n",
        "    x_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, :-1, :], num_batches)\n",
        "    }\n",
        "    y_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, -1, :], num_batches)\n",
        "    }\n",
        "\n",
        "    x_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, :-1, :], num_batches)\n",
        "    }\n",
        "    y_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, -1, :], num_batches)\n",
        "    }\n",
        "    # print(\"x_train['main']: \", x_train['main'][1])\n",
        "    return [x_train, y_train, x_test, y_test]\n",
        "\n",
        "\n",
        "lookback = 10 # choose sequence length\n",
        "x_train, y_train, x_test, y_test = split_data(dataMain, dataEven, dataLoss, dataResult, lookback, 490)\n",
        "\n",
        "# 打印批次的数量和每个批次的形状\n",
        "print('Number of training batches:', len(x_train['main']))\n",
        "if len(x_train['main']) > 0:\n",
        "    # print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "    # print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "    print('x_train.shape = ',x_train['main'][0].shape)\n",
        "    print('y_train.shape = ',y_train['main'][0].shape)\n",
        "    print('x_test.shape = ',x_test['main'][0].shape)\n",
        "    print('y_test.shape = ',y_test['main'][0].shape)\n",
        "    print('x_result.shape = ',y_train['result'][0].shape)\n",
        "\n",
        "# print(\"x_train: \", x_train)\n",
        "# print(\"y_train: \", y_train)\n",
        "\n",
        "# x_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train_batches]\n",
        "# y_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train_batches]\n",
        "import torch\n",
        "\n",
        "# 转换为字典\n",
        "train_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['result']]\n",
        "}\n",
        "\n",
        "test_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['result']]\n",
        "}\n",
        "\n",
        "for x_main_batch, x_even_batch, x_loss_batch, y_main_batch, y_even_batch, y_loss_batch, y_result_batch in zip(train_tensors['x_main'], train_tensors['x_even'], train_tensors['x_loss'], train_tensors['y_main'], train_tensors['y_even'], train_tensors['y_loss'], train_tensors['y_result']):\n",
        "    # print(\"ok: \")\n",
        "    # 替换前10个元素的内容为第11个元素的内容\n",
        "    if len(x_main_batch) > 10:  # 确保有足够的元素\n",
        "        x_main_batch[:10] = x_main_batch[10:11]\n",
        "    # print(\"x_main_batch: \", x_main_batch)\n",
        "    y_test_main_pred, cls_main = model(x_main_batch)\n",
        "    y_test_even_pred, cls_even = model(x_even_batch)\n",
        "    y_test_loss_pred, cls_loss = model(x_loss_batch)\n",
        "    tmp = torch.softmax(cls_main + cls_even + cls_loss, dim=1)\n",
        "    cls_to_draw = tmp\n",
        "# # make predictions\n",
        "# y_test_pred = model(x_test_tensor)\n",
        "# lstm = []\n",
        "# # invert predictions\n",
        "# y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())\n",
        "# # y_train = scaler.inverse_transform(y_train_lstm.detach().numpy())\n",
        "# y_test_main_pred = scalerMain.inverse_transform(y_test_main_pred.detach().numpy())\n",
        "# # y_test = scaler.inverse_transform(y_test_lstm_tensor.detach().numpy())\n",
        "\n",
        "# # calculate root mean squared error\n",
        "# trainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0]))\n",
        "# print('Train Score: %.2f RMSE' % (trainScore))\n",
        "# testScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0]))\n",
        "# print('Test Score: %.2f RMSE' % (testScore))\n",
        "# lstm.append(trainScore)\n",
        "# lstm.append(testScore)\n",
        "# lstm.append(training_time)\n",
        "\n",
        "predict_main = pd.DataFrame(scalerMain.inverse_transform(y_test_main_pred.detach().numpy()))\n",
        "# original_main = pd.DataFrame(scalerMain.inverse_transform(train_tensors['y_main'].detach().numpy()))\n",
        "\n",
        "predict_even = pd.DataFrame(scalerEven.inverse_transform(y_test_even_pred.detach().numpy()))\n",
        "# # original_even = pd.DataFrame(scalerEven.inverse_transform(train_tensors['y_even'].detach().numpy()))\n",
        "\n",
        "predict_loss = pd.DataFrame(scalerLoss.inverse_transform(y_test_loss_pred.detach().numpy()))\n",
        "\n",
        "# predict_cls = pd.DataFrame(scalerLoss.inverse_transform(cls_to_draw.detach().numpy()))\n",
        "# original_loss = pd.DataFrame(scalerLoss.inverse_transform(train_tensors['y_loss'].detach().numpy()))\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# ax = sns.lineplot(x = original_main.index, y = original_main[0], label=\"Data Main\", color='royalblue')\n",
        "ax = sns.lineplot(x = predict_main.index, y = predict_main[0], label=\"Training Prediction Main(LSTM)\", color='tomato')\n",
        "\n",
        "# ax = sns.lineplot(x = original_even.index, y = original_even[0], label=\"Data Even\", color='red')\n",
        "ax = sns.lineplot(x = predict_even.index, y = predict_even[0], label=\"Training Prediction Even(LSTM)\", color='#FF6347')\n",
        "\n",
        "# # ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_loss.index, y = predict_loss[0], label=\"Training Prediction Loss(LSTM)\", color='#F31347')\n",
        "\n",
        "# # ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "# ax = sns.lineplot(x = predict_cls.index, y = predict_cls[0], label=\"Training Prediction Class(LSTM)\", color='#a31347')\n",
        "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
        "ax.set_xlabel(\"Days\", size = 14)\n",
        "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
        "ax.set_xticklabels('', size=10)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "ax = sns.lineplot(data=hist, color='royalblue')\n",
        "ax.set_xlabel(\"Epoch\", size = 14)\n",
        "ax.set_ylabel(\"Loss\", size = 14)\n",
        "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 绘制图像\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 类别标签\n",
        "categories = ['Class 0', 'Class 1', 'Class 2']\n",
        "\n",
        "# 绘制每一行的柱形图\n",
        "for i in range(cls_to_draw.size(0)):\n",
        "    plt.figure()\n",
        "    plt.bar(categories, cls_to_draw[i].detach().numpy(), color=['blue', 'orange', 'green'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel('Probability')\n",
        "    plt.title(f'Class Probability Distribution for Sample {i + 1}')\n",
        "    plt.show()\n",
        "    if i > 5:\n",
        "        break\n",
        "\n"
      ],
      "metadata": {
        "id": "KLEZWHXwESPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 类别标签\n",
        "categories = ['Class 0', 'Class 1', 'Class 2']\n",
        "\n",
        "# 绘制每一行的柱形图\n",
        "for i in range(cls_to_draw.size(0)):\n",
        "    plt.figure()\n",
        "    plt.bar(categories, cls_to_draw[i].detach().numpy(), color=['blue', 'orange', 'green'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel('Probability')\n",
        "    plt.title(f'Class Probability Distribution for Sample {i + 1}')\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "B6pm3DN8D5-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ePRLskNmhp_y"
      },
      "cell_type": "code",
      "source": [
        "# shift train predictions for plotting\n",
        "trainPredictPlot = np.empty_like(price)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[lookback:len(y_train_pred)+lookback, :] = y_train_pred\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = np.empty_like(price)\n",
        "testPredictPlot[:, :] = np.nan\n",
        "testPredictPlot[len(y_train_pred)+lookback-1:len(price)-1, :] = y_test_pred\n",
        "\n",
        "original = scaler.inverse_transform(price['Close'].values.reshape(-1,1))\n",
        "\n",
        "predictions = np.append(trainPredictPlot, testPredictPlot, axis=1)\n",
        "predictions = np.append(predictions, original, axis=1)\n",
        "result = pd.DataFrame(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-tH05FABhp_z"
      },
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[0],\n",
        "                    mode='lines',\n",
        "                    name='Train prediction')))\n",
        "fig.add_trace(go.Scatter(x=result.index, y=result[1],\n",
        "                    mode='lines',\n",
        "                    name='Test prediction'))\n",
        "fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[2],\n",
        "                    mode='lines',\n",
        "                    name='Actual Value')))\n",
        "fig.update_layout(\n",
        "    xaxis=dict(\n",
        "        showline=True,\n",
        "        showgrid=True,\n",
        "        showticklabels=False,\n",
        "        linecolor='white',\n",
        "        linewidth=2\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        title_text='Close (USD)',\n",
        "        titlefont=dict(\n",
        "            family='Rockwell',\n",
        "            size=12,\n",
        "            color='white',\n",
        "        ),\n",
        "        showline=True,\n",
        "        showgrid=True,\n",
        "        showticklabels=True,\n",
        "        linecolor='white',\n",
        "        linewidth=2,\n",
        "        ticks='outside',\n",
        "        tickfont=dict(\n",
        "            family='Rockwell',\n",
        "            size=12,\n",
        "            color='white',\n",
        "        ),\n",
        "    ),\n",
        "    showlegend=True,\n",
        "    template = 'plotly_dark'\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "annotations = []\n",
        "annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n",
        "                              xanchor='left', yanchor='bottom',\n",
        "                              text='Results (LSTM)',\n",
        "                              font=dict(family='Rockwell',\n",
        "                                        size=26,\n",
        "                                        color='white'),\n",
        "                              showarrow=False))\n",
        "fig.update_layout(annotations=annotations)\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "id": "Ibf2gv4Qhp_z"
      },
      "cell_type": "code",
      "source": [
        "!pip install chart-studio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "LXr3BZs6hp_z"
      },
      "cell_type": "code",
      "source": [
        "# import chart_studio.plotly as py\n",
        "# import chart_studio\n",
        "\n",
        "# chart_studio.tools.set_credentials_file(username='rodolfo_saldanha', api_key='zWJIVWJs23wfiAp516Mh')\n",
        "# py.iplot(fig, filename='stock_prediction_lstm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "rjpe8i-Php_z"
      },
      "cell_type": "code",
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
        "        out, (hn) = self.gru(x, (h0.detach()))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "nfWFqY2nhp_0"
      },
      "cell_type": "code",
      "source": [
        "model = GRU(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "AhuqmiqMhp_0"
      },
      "cell_type": "code",
      "source": [
        "hist = np.zeros(num_epochs)\n",
        "start_time = time.time()\n",
        "gru = []\n",
        "\n",
        "for t in range(num_epochs):\n",
        "    y_train_pred = model(x_train)\n",
        "\n",
        "    loss = criterion(y_train_pred, y_train_gru)\n",
        "    print(\"Epoch \", t, \"MSE: \", loss.item())\n",
        "    hist[t] = loss.item()\n",
        "\n",
        "    optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "training_time = time.time()-start_time\n",
        "print(\"Training time: {}\".format(training_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "QeuZe2BRhp_0"
      },
      "cell_type": "code",
      "source": [
        "predict = pd.DataFrame(scaler.inverse_transform(y_train_pred.detach().numpy()))\n",
        "original = pd.DataFrame(scaler.inverse_transform(y_train_gru.detach().numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lf5fFYRWhp_0"
      },
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "ax = sns.lineplot(x = original.index, y = original[0], label=\"Data\", color='royalblue')\n",
        "ax = sns.lineplot(x = predict.index, y = predict[0], label=\"Training Prediction (GRU)\", color='tomato')\n",
        "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
        "ax.set_xlabel(\"Days\", size = 14)\n",
        "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
        "ax.set_xticklabels('', size=10)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "ax = sns.lineplot(data=hist, color='royalblue')\n",
        "ax.set_xlabel(\"Epoch\", size = 14)\n",
        "ax.set_ylabel(\"Loss\", size = 14)\n",
        "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "CVjVWgzThp_1"
      },
      "cell_type": "code",
      "source": [
        "import math, time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# make predictions\n",
        "y_test_pred = model(x_test)\n",
        "\n",
        "# invert predictions\n",
        "y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())\n",
        "y_train = scaler.inverse_transform(y_train_gru.detach().numpy())\n",
        "y_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy())\n",
        "y_test = scaler.inverse_transform(y_test_gru.detach().numpy())\n",
        "\n",
        "# calculate root mean squared error\n",
        "trainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0]))\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "testScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0]))\n",
        "print('Test Score: %.2f RMSE' % (testScore))\n",
        "gru.append(trainScore)\n",
        "gru.append(testScore)\n",
        "gru.append(training_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "T0SJoPGthp_1"
      },
      "cell_type": "code",
      "source": [
        "# shift train predictions for plotting\n",
        "trainPredictPlot = np.empty_like(price)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[lookback:len(y_train_pred)+lookback, :] = y_train_pred\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = np.empty_like(price)\n",
        "testPredictPlot[:, :] = np.nan\n",
        "testPredictPlot[len(y_train_pred)+lookback-1:len(price)-1, :] = y_test_pred\n",
        "\n",
        "original = scaler.inverse_transform(price['Close'].values.reshape(-1,1))\n",
        "\n",
        "predictions = np.append(trainPredictPlot, testPredictPlot, axis=1)\n",
        "predictions = np.append(predictions, original, axis=1)\n",
        "result = pd.DataFrame(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lHNND6KThp_1"
      },
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[0],\n",
        "                    mode='lines',\n",
        "                    name='Train prediction')))\n",
        "fig.add_trace(go.Scatter(x=result.index, y=result[1],\n",
        "                    mode='lines',\n",
        "                    name='Test prediction'))\n",
        "fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[2],\n",
        "                    mode='lines',\n",
        "                    name='Actual Value')))\n",
        "fig.update_layout(\n",
        "    xaxis=dict(\n",
        "        showline=True,\n",
        "        showgrid=True,\n",
        "        showticklabels=False,\n",
        "        linecolor='white',\n",
        "        linewidth=2\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        title_text='Close (USD)',\n",
        "        titlefont=dict(\n",
        "            family='Rockwell',\n",
        "            size=12,\n",
        "            color='white',\n",
        "        ),\n",
        "        showline=True,\n",
        "        showgrid=True,\n",
        "        showticklabels=True,\n",
        "        linecolor='white',\n",
        "        linewidth=2,\n",
        "        ticks='outside',\n",
        "        tickfont=dict(\n",
        "            family='Rockwell',\n",
        "            size=12,\n",
        "            color='white',\n",
        "        ),\n",
        "    ),\n",
        "    showlegend=True,\n",
        "    template = 'plotly_dark'\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "annotations = []\n",
        "annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n",
        "                              xanchor='left', yanchor='bottom',\n",
        "                              text='Results (GRU)',\n",
        "                              font=dict(family='Rockwell',\n",
        "                                        size=26,\n",
        "                                        color='white'),\n",
        "                              showarrow=False))\n",
        "fig.update_layout(annotations=annotations)\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "lrpcQQ68hp_1"
      },
      "cell_type": "code",
      "source": [
        "lstm = pd.DataFrame(lstm, columns=['LSTM'])\n",
        "gru = pd.DataFrame(gru, columns=['GRU'])\n",
        "result = pd.concat([lstm, gru], axis=1, join='inner')\n",
        "result.index = ['Train RMSE', 'Test RMSE', 'Train Time']\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "5k92uuQ4hp_2"
      },
      "cell_type": "code",
      "source": [
        "py.iplot(fig, filename='stock_prediction_gru')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}