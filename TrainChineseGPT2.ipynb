{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title 链接Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KlWbyYyiuLFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "0uDH1wmnp9AM"
      },
      "outputs": [],
      "source": [
        "#@title 自定义GPT2模型\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "from transformers import BertTokenizer, GPT2LMHeadModel\n",
        "from torch import nn\n",
        "\n",
        "# from utils.utils import get_project_rootpath\n",
        "import os\n",
        "\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GPT2, self).__init__()\n",
        "\n",
        "        # self.gpt = GPT2LMHeadModel.from_pretrained(os.path.join(get_project_rootpath(), \"gpt2-chinese-cluecorpussmall\"))\n",
        "\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "\n",
        "\n",
        "    def forward(self, batch_inputs):\n",
        "        outputs = self.gpt(input_ids=batch_inputs)\n",
        "        return outputs\n",
        "\n",
        "    @property\n",
        "    def config(self):\n",
        "        # 返回模型的配置\n",
        "        return self.gpt.config\n",
        "\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        # Provide the device attribute for the model\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    def to(self, device):\n",
        "        # Move the model and its parameters to the specified device\n",
        "        self.gpt.to(device)\n",
        "        return self\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 数据"
      ],
      "metadata": {
        "id": "l995WrFwAwef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 数据加载\n",
        "import json\n",
        "import torch\n",
        "import torch.utils.data as Data\n",
        "from torch import nn, optim\n",
        "import numpy as np\n",
        "\n",
        "# 将文本数据转换为模型输入的数字编码\n",
        "def make_data(file_path, tokenizer):\n",
        "    # 读取文件内容\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    train_datas = []  # 初始化列表存储处理后的数据\n",
        "    for line in lines:\n",
        "        line = line.strip()  # 去除每行的前后空白字符\n",
        "        # 将文本行中的制表符（\\t）替换为[SEP]，并在行尾加上[SEP]\n",
        "        train_data = [i if i != '\\t' else \"[SEP]\" for i in line] + ['[SEP]']\n",
        "        # 使用tokenizer将文本数据编码为数字序列\n",
        "        # print(\"train_data: \", train_data)\n",
        "        train_num_data = tokenizer.encode(train_data)\n",
        "        train_num_data = train_num_data[:-1]  # 去掉最后一个标记（通常是[SEP]）\n",
        "        train_datas.append(train_num_data)  # 将编码后的数据添加到列表中\n",
        "\n",
        "    return train_datas  # 返回所有处理后的数据\n",
        "\n",
        "\n",
        "# 自定义数据集类，继承自torch.utils.data.Dataset\n",
        "class MyDataSet(Data.Dataset):\n",
        "    def __init__(self, datas, vocab2id):\n",
        "        self.datas = datas  # 保存数据\n",
        "        self.vocab2id = vocab2id  # 保存词汇到ID的映射\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        data = self.datas[item]  # 获取指定索引的样本数据\n",
        "        # 划分数据为输入和输出\n",
        "        decoder_input = data[:-1]  # 输入数据，去掉最后一个标记\n",
        "        decoder_output = data[1:]  # 输出数据，从第二个标记开始\n",
        "\n",
        "        # print(\"decoder_input: \", decoder_input, \" decoder_output: \", decoder_output)\n",
        "\n",
        "        # 计算输入和输出的长度\n",
        "        decoder_input_len = len(decoder_input)\n",
        "        decoder_output_len = len(decoder_output)\n",
        "\n",
        "        # 返回样本的输入和输出以及它们的长度\n",
        "        return {\"decoder_input\": decoder_input, \"decoder_input_len\": decoder_input_len,\n",
        "                \"decoder_output\": decoder_output, \"decoder_output_len\": decoder_output_len}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.datas)  # 返回数据集的总长度\n",
        "\n",
        "    def padding_batch(self, batch):\n",
        "        # 获取批次中每个样本的输入和输出长度\n",
        "        decoder_input_lens = [d[\"decoder_input_len\"] for d in batch]\n",
        "        decoder_output_lens = [d[\"decoder_output_len\"] for d in batch]\n",
        "\n",
        "        # 找到输入和输出的最大长度\n",
        "        decoder_input_maxlen = max(decoder_input_lens)\n",
        "        decoder_output_maxlen = max(decoder_output_lens)\n",
        "\n",
        "        # 对每个样本进行填充，使其长度一致\n",
        "        for d in batch:\n",
        "            d[\"decoder_input\"].extend([self.vocab2id[\"[PAD]\"]] * (decoder_input_maxlen - d[\"decoder_input_len\"]))\n",
        "            d[\"decoder_output\"].extend([self.vocab2id[\"[PAD]\"]] * (decoder_output_maxlen - d[\"decoder_output_len\"]))\n",
        "            # print(\"decoder_inputsdecoder_inputs: \", d[\"decoder_input\"], \"decoder_outputdecoder_output:\", d[\"decoder_output\"])\n",
        "\n",
        "        # 将填充后的输入和输出转换为张量\n",
        "        decoder_inputs = torch.tensor([d[\"decoder_input\"] for d in batch], dtype=torch.long)\n",
        "        decoder_outputs = torch.tensor([d[\"decoder_output\"] for d in batch], dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "        return decoder_inputs, decoder_outputs  # 返回填充后的输入和输出张量\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kFQfesu0qgZA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 自定义数据\n",
        "\n",
        "%%writefile selfTxt.txt\n",
        "谢谢你所做的一切\n",
        "你开心就好\n",
        "开心\n",
        "\n",
        "你们宿舍都是这么厉害的人吗\n",
        "是的\n",
        "又高又厉害\n",
        "\n",
        "今天好点了吗？\n",
        "一天比一天严重\n",
        "吃药不管用，去打一针。别拖着\n",
        "\n",
        "是的。下辈子想做只萤火虫\n",
        "可是萤火虫太容易被抓了还是改一个吧\n",
        "不，我只想奋不顾身扑火\n",
        "\n",
        "加油，三月动起来，五月笑起来\n",
        "正解你为什么就那么厉害呢\n",
        "哈哈，没办法，智商就是这么高\n",
        "\n",
        "好身材，秀出来\n",
        "哈哈哈其实我是胖的\n",
        "谢谢\n"
      ],
      "metadata": {
        "id": "ciz7s5bP2GvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 训练"
      ],
      "metadata": {
        "id": "bxy1gjd1A0am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title AverageMeter\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # 初始化函数，设置各变量初始值为None，并调用reset函数进行重置\n",
        "        self.count = None  # 记录数据的数量\n",
        "        self.sum = None  # 记录数据的总和\n",
        "        self.avg = None  # 记录当前的平均值\n",
        "        self.val = None  # 记录当前值\n",
        "        self.reset()  # 重置所有变量\n",
        "\n",
        "    def reset(self):\n",
        "        # 重置所有变量的值为初始状态\n",
        "        self.val = 0  # 当前值设为0\n",
        "        self.avg = 0  # 平均值设为0\n",
        "        self.sum = 0  # 总和设为0\n",
        "        self.count = 0  # 数据的数量设为0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        # 更新函数，用于更新当前值、总和、数量及重新计算平均值\n",
        "        self.val = val  # 更新当前值为传入的值\n",
        "        self.sum += val * n  # 根据传入的数量n，将总和增加val * n\n",
        "        self.count += n  # 更新数据的数量\n",
        "        self.avg = self.sum / self.count  # 计算新的平均值\n"
      ],
      "metadata": {
        "id": "dHYB8ERstu-p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 训练过程\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "import sys\n",
        "from torch import nn, optim\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# 添加上级目录到系统路径中\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# 定义训练参数的类\n",
        "class TrainArgs:\n",
        "    def __init__(self):\n",
        "        self.device = \"cpu\"  # 训练设备（默认为CPU）\n",
        "        self.batch_size = 4  # 批次大小\n",
        "        self.epochs = 1  # 训练轮数\n",
        "        self.print_every = 10  # 每隔多少步打印一次信息\n",
        "        self.clip = 1  # 梯度裁剪的阈值\n",
        "        # 训练数据文件路径\n",
        "        self.train_file_path = \"/content/drive/MyDrive/train.txt\"\n",
        "        self.save_path = \"GPT2.pt\"  # 模型保存路径\n",
        "        self.lr = 1e-4  # 学习率\n",
        "\n",
        "# 实例化 TrainArgs 类\n",
        "train_args = TrainArgs()\n",
        "\n",
        "# 设置训练参数（这些设置在 TrainArgs 类中已经定义）\n",
        "train_args.device = \"cpu\"\n",
        "train_args.batch_size = 1\n",
        "train_args.epochs = 10\n",
        "train_args.print_every = 10\n",
        "train_args.clip = 1\n",
        "# train_args.train_file_path = \"/content/drive/MyDrive/train.txt\"\n",
        "train_args.train_file_path = \"/content/selfTxt.txt\"\n",
        "train_args.save_path = \"GPT2.pt\"\n",
        "train_args.lr = 1e-4\n",
        "\n",
        "# 计算训练时间的函数\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time  # 计算总时间\n",
        "    elapsed_mins = int(elapsed_time / 60)  # 转换为分钟\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))  # 剩余秒数\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "# 单步训练函数\n",
        "def train_step(model, data_loader, epoch, optimizer, criterion, clip=1, print_every=None):\n",
        "    model.train()  # 设置模型为训练模式\n",
        "\n",
        "    if print_every is None:\n",
        "        print_every = 1  # 如果未指定打印频率，则默认每步打印一次\n",
        "\n",
        "    epoch_loss = 0  # 初始化本轮的总损失\n",
        "    losses = AverageMeter()  # 用于记录损失的类实例\n",
        "    temp_time = time.time()  # 记录当前时间\n",
        "\n",
        "    # 遍历数据加载器中的每个批次\n",
        "    for step, (dec_inputs, dec_outputs) in enumerate(data_loader):\n",
        "        '''\n",
        "        dec_inputs: [batch_size, tgt_len]\n",
        "        dec_outputs: [batch_size, tgt_len]\n",
        "        '''\n",
        "        # print(\"dec_inputs: \", dec_inputs, \"dec_outputs: \", dec_outputs)\n",
        "        optimizer.zero_grad()  # 清除之前的梯度\n",
        "        dec_inputs, dec_outputs = dec_inputs.to(device), dec_outputs.to(device)  # 将数据移动到设备上\n",
        "\n",
        "        # 使用模型进行前向传播，输出：[batch_size * tgt_len, tgt_vocab_size]\n",
        "        outputs = model(dec_inputs)\n",
        "        outputs = outputs.logits  # 获取模型的输出\n",
        "        outputs = outputs.view(-1, outputs.size(-1))  # 调整输出的维度\n",
        "        loss = criterion(outputs, dec_outputs.view(-1))  # 计算损失\n",
        "        epoch_loss += loss.item()  # 累加损失\n",
        "        losses.update(loss.item(), batch_size)  # 更新损失记录\n",
        "\n",
        "        loss.backward()  # 反向传播计算梯度\n",
        "\n",
        "        # 梯度裁剪，防止梯度爆炸\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()  # 更新模型参数\n",
        "\n",
        "        # 打印训练进度\n",
        "        if print_every and (step + 1) % print_every == 0:\n",
        "            minutes, seconds = epoch_time(temp_time, time.time())\n",
        "            print('Epoch: [{0}][{1}/{2}] '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  'Elapsed {minutes:s}min {seconds:s}s '\n",
        "                  .format(epoch, step + 1, len(data_loader),\n",
        "                          minutes=minutes.__str__(),\n",
        "                          seconds=seconds.__str__(),\n",
        "                          loss=losses))\n",
        "            temp_time = time.time()  # 重置计时器\n",
        "\n",
        "    return epoch_loss / len(data_loader)  # 返回每轮的平均损失\n",
        "\n",
        "# 训练函数\n",
        "def train(model, dataloader, train_args):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0).to(device)  # 定义损失函数\n",
        "    lr = train_args.lr  # 学习率\n",
        "    CLIP = train_args.clip  # 梯度裁剪的阈值\n",
        "    print_every = train_args.print_every  # 打印频率\n",
        "    save_path = train_args.save_path  # 模型保存路径\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)  # 定义优化器\n",
        "\n",
        "    for epoch in range(train_args.epochs):  # 遍历每个训练轮次\n",
        "        start_time = time.time()  # 记录轮次开始时间\n",
        "        train_loss = train_step(model, dataloader, epoch, optimizer, criterion, CLIP, print_every=print_every)  # 进行训练\n",
        "        end_time = time.time()  # 记录轮次结束时间\n",
        "\n",
        "        torch.save(model.state_dict(), save_path)  # 保存模型参数\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)  # 计算本轮时间\n",
        "        print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f}')  # 打印训练损失\n",
        "\n",
        "# 打印模型参数的总数和可训练参数的数量\n",
        "def print_num_parameters(model):\n",
        "    # 计算总参数数\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'{total_params:,} total parameters.')\n",
        "    # 计算可训练参数数\n",
        "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'{total_trainable_params:,} training parameters.')\n",
        "\n",
        "# 主函数\n",
        "if __name__ == '__main__':\n",
        "    device = train_args.device  # 获取设备\n",
        "    # 初始化分词器\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "\n",
        "    epochs = train_args.epochs  # 训练轮次\n",
        "    batch_size = train_args.batch_size  # 批次大小\n",
        "\n",
        "    train_file_path = train_args.train_file_path  # 训练数据文件路径\n",
        "    datas = make_data(train_file_path, tokenizer)  # 处理数据\n",
        "    dataset = MyDataSet(datas, tokenizer.vocab)  # 创建数据集实例\n",
        "    dataloader = Data.DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.padding_batch)  # 创建数据加载器\n",
        "\n",
        "    model = GPT2().to(device)  # 初始化模型，并将其移动到设备上\n",
        "    train(model, dataloader, train_args)  # 开始训练\n",
        "\n"
      ],
      "metadata": {
        "id": "3z-4JRFdqpGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 训练参数参考\n",
        "import argparse\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "# from utils.utils import get_project_rootpath\n",
        "import os\n",
        "\n",
        "# checkpoints_dir = os.path.join(get_project_rootpath(), \"model_checkpoints\")\n",
        "\n",
        "\n",
        "def train_parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"训练参数配置\")\n",
        "    parser.add_argument(\"--device\", type=str, default=\"cuda\", help=\"batch size\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"batch size\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=1, help=\"epochs\")\n",
        "    parser.add_argument(\"--print_every\", type=int, default=10, help=\"print every\")\n",
        "    parser.add_argument(\"--clip\", type=int, default=1, help=\"clip\")\n",
        "\n",
        "\n",
        "    parser.add_argument(\"--train_file_path\", type=str, default=os.path.join(\"\",\"/content/drive/MyDrive/train.txt\"),\n",
        "                        help=\"train_file_path\")\n",
        "\n",
        "    parser.add_argument('--save_path', type=str, default=os.path.join(\"\", \"GPT2.pt\"),\n",
        "                        help='decay step')\n",
        "    parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')\n",
        "\n",
        "\n",
        "    return parser.parse_args()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tnZHu30-tTkA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 验证\n",
        "\n"
      ],
      "metadata": {
        "id": "11XcDuBv7d10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_project_rootpath():\n",
        "    \"\"\"\n",
        "    获取项目根目录。此函数的能力体现在，不论当前module被import到任何位置，都可以正确获取项目根目录\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    path = os.path.realpath(os.curdir)\n",
        "    while True:\n",
        "        # PyCharm项目中，'.idea'是必然存在的，且名称唯一\n",
        "        if '.idea' in os.listdir(path):\n",
        "            return path\n",
        "        path = os.path.dirname(path)\n"
      ],
      "metadata": {
        "id": "Hc8p3ARG7RVU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title validate result\n",
        "\n",
        "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
        "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "model = GPT2().to(device)\n",
        "# 加载模型权重\n",
        "model.load_state_dict(torch.load('GPT2.pt'))\n",
        "\n",
        "# 设置模型为评估模式\n",
        "model.eval()\n",
        "\n",
        "# 创建文本生成管道\n",
        "text_generator = TextGenerationPipeline(model.gpt, tokenizer)\n",
        "\n",
        "# 使用模型进行文本生成\n",
        "result = text_generator(\"今天好点了吗？\", max_length=100, do_sample=True)\n",
        "print(result)\n",
        "\n",
        "# 使用模型进行文本生成\n",
        "result = text_generator(\"好身材，秀出来\", max_length=100, do_sample=True)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "gPTIR3PC5dy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title validate result\n",
        "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
        "import torch\n",
        "\n",
        "# 加载tokenizer和模型\n",
        "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\").to('cpu')\n",
        "\n",
        "# 加载训练后的模型权重\n",
        "# model.load_state_dict(torch.load('GPT2.pt', map_location=torch.device('cpu')))\n",
        "\n",
        "# 设置模型为评估模式\n",
        "model.eval()\n",
        "\n",
        "# 创建文本生成管道\n",
        "text_generator = TextGenerationPipeline(model, tokenizer)\n",
        "\n",
        "# 对话互动\n",
        "history = []\n",
        "\n",
        "while True:\n",
        "    # 用户输入\n",
        "    user_input = input(\"你: \")\n",
        "\n",
        "    # 将用户输入加入到对话历史中\n",
        "    history.append(user_input)\n",
        "\n",
        "    # 构造输入给模型\n",
        "    input_text = \" \".join(history)\n",
        "\n",
        "    # 使用模型生成响应\n",
        "    response = text_generator(input_text, max_length=1000, do_sample=True, top_k=50, top_p=0.95)[0]['generated_text']\n",
        "\n",
        "    # 提取模型生成的响应\n",
        "    generated_text = response[len(input_text):].strip()\n",
        "\n",
        "    # 打印模型的响应\n",
        "    print(\"AI:\", generated_text)\n",
        "\n",
        "    # 将模型的响应加入到对话历史中\n",
        "    history.append(generated_text)\n",
        "\n",
        "    # 结束对话条件（可选）\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"再见\", \"拜拜\"]:\n",
        "        print(\"AI: 再见！\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "pV-VzsInnwN8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}