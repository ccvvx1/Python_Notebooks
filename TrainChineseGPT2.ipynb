{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KlWbyYyiuLFI"
   },
   "outputs": [],
   "source": [
    "#@title 链接Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0uDH1wmnp9AM"
   },
   "outputs": [],
   "source": [
    "#@title 自定义GPT2模型\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from transformers import BertTokenizer, GPT2LMHeadModel,GPT2Config\n",
    "from torch import nn\n",
    "\n",
    "# from utils.utils import get_project_rootpath\n",
    "import os\n",
    "\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GPT2, self).__init__()\n",
    "\n",
    "        # self.gpt = GPT2LMHeadModel.from_pretrained(os.path.join(get_project_rootpath(), \"gpt2-chinese-cluecorpussmall\"))\n",
    "\n",
    "        # self.gpt = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
    "        config = GPT2Config()\n",
    "        print(config)\n",
    "        config.n_embd = 30\n",
    "        config.n_head = 6\n",
    "        config.n_layer = 6\n",
    "        self.gpt = GPT2LMHeadModel(config)\n",
    "\n",
    "\n",
    "    def forward(self, batch_inputs):\n",
    "        outputs = self.gpt(input_ids=batch_inputs)\n",
    "        return outputs\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        # 返回模型的配置\n",
    "        return self.gpt.config\n",
    "\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        # Provide the device attribute for the model\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def to(self, device):\n",
    "        # Move the model and its parameters to the specified device\n",
    "        self.gpt.to(device)\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l995WrFwAwef"
   },
   "source": [
    "## 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kFQfesu0qgZA"
   },
   "outputs": [],
   "source": [
    "#@title 数据加载\n",
    "import json\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "\n",
    "# 将文本数据转换为模型输入的数字编码\n",
    "def make_data(file_path, tokenizer):\n",
    "    # 读取文件内容\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    train_datas = []  # 初始化列表存储处理后的数据\n",
    "    for line in lines:\n",
    "        line = line.strip()  # 去除每行的前后空白字符\n",
    "        # 将文本行中的制表符（\\t）替换为[SEP]，并在行尾加上[SEP]\n",
    "        train_data = [i if i != '\\t' else \"[SEP]\" for i in line] + ['[SEP]']\n",
    "        # 使用tokenizer将文本数据编码为数字序列\n",
    "        # print(\"train_data: \", train_data)\n",
    "        train_num_data = tokenizer.encode(train_data)\n",
    "        train_num_data = train_num_data[:-1]  # 去掉最后一个标记（通常是[SEP]）\n",
    "        train_datas.append(train_num_data)  # 将编码后的数据添加到列表中\n",
    "\n",
    "    return train_datas  # 返回所有处理后的数据\n",
    "\n",
    "\n",
    "# 自定义数据集类，继承自torch.utils.data.Dataset\n",
    "class MyDataSet(Data.Dataset):\n",
    "    def __init__(self, datas, vocab2id):\n",
    "        self.datas = datas  # 保存数据\n",
    "        self.vocab2id = vocab2id  # 保存词汇到ID的映射\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = self.datas[item]  # 获取指定索引的样本数据\n",
    "        # 划分数据为输入和输出\n",
    "        decoder_input = data[:-1]  # 输入数据，去掉最后一个标记\n",
    "        decoder_output = data[1:]  # 输出数据，从第二个标记开始\n",
    "\n",
    "        # print(\"decoder_input: \", decoder_input, \" decoder_output: \", decoder_output)\n",
    "\n",
    "        # 计算输入和输出的长度\n",
    "        decoder_input_len = len(decoder_input)\n",
    "        decoder_output_len = len(decoder_output)\n",
    "\n",
    "        # 返回样本的输入和输出以及它们的长度\n",
    "        return {\"decoder_input\": decoder_input, \"decoder_input_len\": decoder_input_len,\n",
    "                \"decoder_output\": decoder_output, \"decoder_output_len\": decoder_output_len}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datas)  # 返回数据集的总长度\n",
    "\n",
    "    def padding_batch(self, batch):\n",
    "        # 获取批次中每个样本的输入和输出长度\n",
    "        decoder_input_lens = [d[\"decoder_input_len\"] for d in batch]\n",
    "        decoder_output_lens = [d[\"decoder_output_len\"] for d in batch]\n",
    "\n",
    "        # 找到输入和输出的最大长度\n",
    "        decoder_input_maxlen = max(decoder_input_lens)\n",
    "        decoder_output_maxlen = max(decoder_output_lens)\n",
    "\n",
    "        # 对每个样本进行填充，使其长度一致\n",
    "        for d in batch:\n",
    "            d[\"decoder_input\"].extend([self.vocab2id[\"[PAD]\"]] * (decoder_input_maxlen - d[\"decoder_input_len\"]))\n",
    "            d[\"decoder_output\"].extend([self.vocab2id[\"[PAD]\"]] * (decoder_output_maxlen - d[\"decoder_output_len\"]))\n",
    "            # print(\"decoder_inputsdecoder_inputs: \", d[\"decoder_input\"], \"decoder_outputdecoder_output:\", d[\"decoder_output\"])\n",
    "\n",
    "        # 将填充后的输入和输出转换为张量\n",
    "        decoder_inputs = torch.tensor([d[\"decoder_input\"] for d in batch], dtype=torch.long)\n",
    "        decoder_outputs = torch.tensor([d[\"decoder_output\"] for d in batch], dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "        return decoder_inputs, decoder_outputs  # 返回填充后的输入和输出张量\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ciz7s5bP2GvE",
    "outputId": "aa3e8fdd-333d-4f79-fa25-cae7f999c5f5"
   },
   "outputs": [],
   "source": [
    "#@title 自定义数据\n",
    "\n",
    "%%writefile selfTxt.txt\n",
    "谢谢你所做的一切\n",
    "你开心就好\n",
    "开心\n",
    "\n",
    "你们宿舍都是这么厉害的人吗\n",
    "是的\n",
    "又高又厉害\n",
    "\n",
    "今天好点了吗？\n",
    "一天比一天严重\n",
    "吃药不管用，去打一针。别拖着\n",
    "\n",
    "是的。下辈子想做只萤火虫\n",
    "可是萤火虫太容易被抓了还是改一个吧\n",
    "不，我只想奋不顾身扑火\n",
    "\n",
    "加油，三月动起来，五月笑起来\n",
    "正解你为什么就那么厉害呢\n",
    "哈哈，没办法，智商就是这么高\n",
    "\n",
    "好身材，秀出来\n",
    "哈哈哈其实我是胖的\n",
    "谢谢\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxy1gjd1A0am"
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dHYB8ERstu-p"
   },
   "outputs": [],
   "source": [
    "#@title AverageMeter\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # 初始化函数，设置各变量初始值为None，并调用reset函数进行重置\n",
    "        self.count = None  # 记录数据的数量\n",
    "        self.sum = None  # 记录数据的总和\n",
    "        self.avg = None  # 记录当前的平均值\n",
    "        self.val = None  # 记录当前值\n",
    "        self.reset()  # 重置所有变量\n",
    "\n",
    "    def reset(self):\n",
    "        # 重置所有变量的值为初始状态\n",
    "        self.val = 0  # 当前值设为0\n",
    "        self.avg = 0  # 平均值设为0\n",
    "        self.sum = 0  # 总和设为0\n",
    "        self.count = 0  # 数据的数量设为0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        # 更新函数，用于更新当前值、总和、数量及重新计算平均值\n",
    "        self.val = val  # 更新当前值为传入的值\n",
    "        self.sum += val * n  # 根据传入的数量n，将总和增加val * n\n",
    "        self.count += n  # 更新数据的数量\n",
    "        self.avg = self.sum / self.count  # 计算新的平均值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3z-4JRFdqpGo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmw/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 1.06 MiB is free. Including non-PyTorch memory, this process has 1.94 GiB memory in use. Of the allocated memory 1.76 GiB is allocated by PyTorch, and 135.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 147\u001b[0m\n\u001b[1;32m    144\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m Data\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, collate_fn\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mpadding_batch)  \u001b[38;5;66;03m# 创建数据加载器\u001b[39;00m\n\u001b[1;32m    146\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2()\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# 初始化模型，并将其移动到设备上\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_args\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 110\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, train_args)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_args\u001b[38;5;241m.\u001b[39mepochs):  \u001b[38;5;66;03m# 遍历每个训练轮次\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# 记录轮次开始时间\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_every\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 进行训练\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# 记录轮次结束时间\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), save_path)  \u001b[38;5;66;03m# 保存模型参数\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 83\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, data_loader, epoch, optimizer, criterion, clip, print_every)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# 梯度裁剪，防止梯度爆炸\u001b[39;00m\n\u001b[1;32m     81\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n\u001b[0;32m---> 83\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 更新模型参数\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# 打印训练进度\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_every \u001b[38;5;129;01mand\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m print_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/adam.py:216\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    213\u001b[0m     state_steps: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     adam(\n\u001b[1;32m    227\u001b[0m         params_with_grad,\n\u001b[1;32m    228\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    247\u001b[0m     )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/optim/adam.py:160\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    156\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    157\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    158\u001b[0m )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    166\u001b[0m         p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    167\u001b[0m     )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 1.95 GiB of which 1.06 MiB is free. Including non-PyTorch memory, this process has 1.94 GiB memory in use. Of the allocated memory 1.76 GiB is allocated by PyTorch, and 135.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "#@title 训练过程\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import sys\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 添加上级目录到系统路径中\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# 定义训练参数的类\n",
    "class TrainArgs:\n",
    "    def __init__(self):\n",
    "        self.device = \"cpu\"  # 训练设备（默认为CPU）\n",
    "        self.batch_size = 4  # 批次大小\n",
    "        self.epochs = 1  # 训练轮数\n",
    "        self.print_every = 10  # 每隔多少步打印一次信息\n",
    "        self.clip = 1  # 梯度裁剪的阈值\n",
    "        # 训练数据文件路径\n",
    "        self.train_file_path = \"/content/drive/MyDrive/train.txt\"\n",
    "        self.save_path = \"GPT2.pt\"  # 模型保存路径\n",
    "        self.lr = 1e-4  # 学习率\n",
    "\n",
    "# 实例化 TrainArgs 类\n",
    "train_args = TrainArgs()\n",
    "\n",
    "# 设置训练参数（这些设置在 TrainArgs 类中已经定义）\n",
    "train_args.device = \"cuda\"\n",
    "train_args.batch_size = 1\n",
    "train_args.epochs = 10\n",
    "train_args.print_every = 10\n",
    "train_args.clip = 1\n",
    "# train_args.train_file_path = \"/content/drive/MyDrive/train.txt\"\n",
    "train_args.train_file_path = \"selfTxt.txt\"\n",
    "train_args.save_path = \"GPT2.pt\"\n",
    "train_args.lr = 1e-4\n",
    "\n",
    "# 计算训练时间的函数\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time  # 计算总时间\n",
    "    elapsed_mins = int(elapsed_time / 60)  # 转换为分钟\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))  # 剩余秒数\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "# 单步训练函数\n",
    "def train_step(model, data_loader, epoch, optimizer, criterion, clip=1, print_every=None):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "\n",
    "    if print_every is None:\n",
    "        print_every = 1  # 如果未指定打印频率，则默认每步打印一次\n",
    "\n",
    "    epoch_loss = 0  # 初始化本轮的总损失\n",
    "    losses = AverageMeter()  # 用于记录损失的类实例\n",
    "    temp_time = time.time()  # 记录当前时间\n",
    "\n",
    "    # 遍历数据加载器中的每个批次\n",
    "    for step, (dec_inputs, dec_outputs) in enumerate(data_loader):\n",
    "        '''\n",
    "        dec_inputs: [batch_size, tgt_len]\n",
    "        dec_outputs: [batch_size, tgt_len]\n",
    "        '''\n",
    "        # print(\"dec_inputs: \", dec_inputs, \"dec_outputs: \", dec_outputs)\n",
    "        optimizer.zero_grad()  # 清除之前的梯度\n",
    "        dec_inputs, dec_outputs = dec_inputs.to(device), dec_outputs.to(device)  # 将数据移动到设备上\n",
    "\n",
    "        # 使用模型进行前向传播，输出：[batch_size * tgt_len, tgt_vocab_size]\n",
    "        outputs = model(dec_inputs)\n",
    "        outputs = outputs.logits  # 获取模型的输出\n",
    "        outputs = outputs.view(-1, outputs.size(-1))  # 调整输出的维度\n",
    "        loss = criterion(outputs, dec_outputs.view(-1))  # 计算损失\n",
    "        epoch_loss += loss.item()  # 累加损失\n",
    "        losses.update(loss.item(), batch_size)  # 更新损失记录\n",
    "\n",
    "        loss.backward()  # 反向传播计算梯度\n",
    "\n",
    "        # 梯度裁剪，防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()  # 更新模型参数\n",
    "\n",
    "        # 打印训练进度\n",
    "        if print_every and (step + 1) % print_every == 0:\n",
    "            minutes, seconds = epoch_time(temp_time, time.time())\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Elapsed {minutes:s}min {seconds:s}s '\n",
    "                  .format(epoch, step + 1, len(data_loader),\n",
    "                          minutes=minutes.__str__(),\n",
    "                          seconds=seconds.__str__(),\n",
    "                          loss=losses))\n",
    "            temp_time = time.time()  # 重置计时器\n",
    "\n",
    "    return epoch_loss / len(data_loader)  # 返回每轮的平均损失\n",
    "\n",
    "# 训练函数\n",
    "def train(model, dataloader, train_args):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0).to(device)  # 定义损失函数\n",
    "    lr = train_args.lr  # 学习率\n",
    "    CLIP = train_args.clip  # 梯度裁剪的阈值\n",
    "    print_every = train_args.print_every  # 打印频率\n",
    "    save_path = train_args.save_path  # 模型保存路径\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)  # 定义优化器\n",
    "\n",
    "    for epoch in range(train_args.epochs):  # 遍历每个训练轮次\n",
    "        start_time = time.time()  # 记录轮次开始时间\n",
    "        train_loss = train_step(model, dataloader, epoch, optimizer, criterion, CLIP, print_every=print_every)  # 进行训练\n",
    "        end_time = time.time()  # 记录轮次结束时间\n",
    "\n",
    "        torch.save(model.state_dict(), save_path)  # 保存模型参数\n",
    "\n",
    "        torch.save(model.gpt.state_dict(), f'simpleGPT2.pt')  # 保存模型参数\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)  # 计算本轮时间\n",
    "        print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}')  # 打印训练损失\n",
    "\n",
    "# 打印模型参数的总数和可训练参数的数量\n",
    "def print_num_parameters(model):\n",
    "    # 计算总参数数\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'{total_params:,} total parameters.')\n",
    "    # 计算可训练参数数\n",
    "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'{total_trainable_params:,} training parameters.')\n",
    "\n",
    "# 主函数\n",
    "if __name__ == '__main__':\n",
    "    device = train_args.device  # 获取设备\n",
    "    # 初始化分词器\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
    "    # torch.save(tokenizer.state_dict(), f'tokenizer.pt')  # 保存模型参数# 保存分词器\n",
    "    tokenizer.save_pretrained('./tokenizer')  # 保存到指定目录\n",
    "\n",
    "    epochs = train_args.epochs  # 训练轮次\n",
    "    batch_size = train_args.batch_size  # 批次大小\n",
    "\n",
    "    train_file_path = train_args.train_file_path  # 训练数据文件路径\n",
    "    datas = make_data(train_file_path, tokenizer)  # 处理数据\n",
    "    dataset = MyDataSet(datas, tokenizer.vocab)  # 创建数据集实例\n",
    "    dataloader = Data.DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.padding_batch)  # 创建数据加载器\n",
    "\n",
    "    model = GPT2().to(device)  # 初始化模型，并将其移动到设备上\n",
    "    train(model, dataloader, train_args)  # 开始训练\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "tnZHu30-tTkA"
   },
   "outputs": [],
   "source": [
    "#@title 训练参数参考\n",
    "import argparse\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "# from utils.utils import get_project_rootpath\n",
    "import os\n",
    "\n",
    "# checkpoints_dir = os.path.join(get_project_rootpath(), \"model_checkpoints\")\n",
    "\n",
    "\n",
    "def train_parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"训练参数配置\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda\", help=\"batch size\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"batch size\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1, help=\"epochs\")\n",
    "    parser.add_argument(\"--print_every\", type=int, default=10, help=\"print every\")\n",
    "    parser.add_argument(\"--clip\", type=int, default=1, help=\"clip\")\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--train_file_path\", type=str, default=os.path.join(\"\",\"/content/drive/MyDrive/train.txt\"),\n",
    "                        help=\"train_file_path\")\n",
    "\n",
    "    parser.add_argument('--save_path', type=str, default=os.path.join(\"\", \"GPT2.pt\"),\n",
    "                        help='decay step')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')\n",
    "\n",
    "\n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11XcDuBv7d10"
   },
   "source": [
    "## 验证\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hc8p3ARG7RVU"
   },
   "outputs": [],
   "source": [
    "def get_project_rootpath():\n",
    "    \"\"\"\n",
    "    获取项目根目录。此函数的能力体现在，不论当前module被import到任何位置，都可以正确获取项目根目录\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    path = os.path.realpath(os.curdir)\n",
    "    while True:\n",
    "        # PyCharm项目中，'.idea'是必然存在的，且名称唯一\n",
    "        if '.idea' in os.listdir(path):\n",
    "            return path\n",
    "        path = os.path.dirname(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPTIR3PC5dy1"
   },
   "outputs": [],
   "source": [
    "#@title validate result\n",
    "\n",
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
    "model = GPT2().to(device)\n",
    "# 加载模型权重\n",
    "model.load_state_dict(torch.load('GPT2.pt'))\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 创建文本生成管道\n",
    "text_generator = TextGenerationPipeline(model.gpt, tokenizer)\n",
    "\n",
    "# 使用模型进行文本生成\n",
    "result = text_generator(\"今天好点了吗？\", max_length=100, do_sample=True)\n",
    "print(result)\n",
    "\n",
    "# 使用模型进行文本生成\n",
    "result = text_generator(\"好身材，秀出来\", max_length=100, do_sample=True)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pV-VzsInnwN8"
   },
   "outputs": [],
   "source": [
    "#@title validate result\n",
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "import torch\n",
    "\n",
    "# 加载tokenizer和模型\n",
    "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\").to('cpu')\n",
    "\n",
    "# 加载训练后的模型权重\n",
    "# model.load_state_dict(torch.load('GPT2.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 创建文本生成管道\n",
    "text_generator = TextGenerationPipeline(model, tokenizer)\n",
    "\n",
    "# 对话互动\n",
    "history = []\n",
    "\n",
    "while True:\n",
    "    # 用户输入\n",
    "    user_input = input(\"你: \")\n",
    "\n",
    "    # 将用户输入加入到对话历史中\n",
    "    history.append(user_input)\n",
    "\n",
    "    # 构造输入给模型\n",
    "    input_text = \" \".join(history)\n",
    "\n",
    "    # 使用模型生成响应\n",
    "    response = text_generator(input_text, max_length=1000, do_sample=True, top_k=50, top_p=0.95)[0]['generated_text']\n",
    "\n",
    "    # 提取模型生成的响应\n",
    "    generated_text = response[len(input_text):].strip()\n",
    "\n",
    "    # 打印模型的响应\n",
    "    print(\"AI:\", generated_text)\n",
    "\n",
    "    # 将模型的响应加入到对话历史中\n",
    "    history.append(generated_text)\n",
    "\n",
    "    # 结束对话条件（可选）\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"再见\", \"拜拜\"]:\n",
    "        print(\"AI: 再见！\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKxI-lKAufs4"
   },
   "source": [
    "## 安卓加载pt模型文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6s4H02fujHO"
   },
   "outputs": [],
   "source": [
    "#@title 简单加载pt模型文件\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, BertTokenizer\n",
    "\n",
    "# 1. 定义或加载 GPT-2 模型架构\n",
    "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
    "\n",
    "# 2. 加载保存的模型权重（如果有的话）\n",
    "model.load_state_dict(torch.load('simpleGPT2.pt'))\n",
    "\n",
    "# 3. 设置模型为推理模式\n",
    "model.eval()\n",
    "\n",
    "# 4. 进行推理示例\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/content/tokenizer\")\n",
    "input_text = \"你好\"\n",
    "input_ids = tokenizer(input_text, return_tensors='pt')['input_ids']\n",
    "\n",
    "# 推理\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "# 获取生成的 token ID 序列\n",
    "generated_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# 结果解码\n",
    "decoded_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 打印解码后的文本\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azXQELIl9l9m"
   },
   "outputs": [],
   "source": [
    "!pip install onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jC7PEe5U8PdF"
   },
   "outputs": [],
   "source": [
    "#@title 转成安卓加载\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# 定义并加载 GPT-2 模型架构\n",
    "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
    "\n",
    "# 将模型转换为 ONNX 格式\n",
    "model.eval()\n",
    "dummy_input = torch.randint(0, 1000, (1, 10))  # 修改为适当的输入大小\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"model.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=14,  # 使用较高的 opset 版本\n",
    "    input_names=['input_ids'],\n",
    "    output_names=['output']\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4MS57dyKuaK"
   },
   "outputs": [],
   "source": [
    "!cp model.onnx /content/drive/MyDrive/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
