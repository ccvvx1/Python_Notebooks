{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KlWbyYyiuLFI"
      },
      "outputs": [],
      "source": [
        "#@title 链接Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uDH1wmnp9AM"
      },
      "outputs": [],
      "source": [
        "#@title 自定义GPT2模型\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "from transformers import BertTokenizer, GPT2LMHeadModel,GPT2Config\n",
        "from torch import nn\n",
        "\n",
        "# from utils.utils import get_project_rootpath\n",
        "import os\n",
        "\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GPT2, self).__init__()\n",
        "\n",
        "        # self.gpt = GPT2LMHeadModel.from_pretrained(os.path.join(get_project_rootpath(), \"gpt2-chinese-cluecorpussmall\"))\n",
        "\n",
        "        # self.gpt = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "        config = GPT2Config()\n",
        "\n",
        "        config.n_embd = 500\n",
        "        config.n_head = 10\n",
        "        config.n_layer = 10\n",
        "        print(config)\n",
        "        self.gpt = GPT2LMHeadModel(config)\n",
        "\n",
        "\n",
        "    def forward(self, batch_inputs):\n",
        "        outputs = self.gpt(input_ids=batch_inputs)\n",
        "        return outputs\n",
        "\n",
        "    @property\n",
        "    def config(self):\n",
        "        # 返回模型的配置\n",
        "        return self.gpt.config\n",
        "\n",
        "\n",
        "    @property\n",
        "    def device(self):\n",
        "        # Provide the device attribute for the model\n",
        "        return next(self.parameters()).device\n",
        "\n",
        "    def to(self, device):\n",
        "        # Move the model and its parameters to the specified device\n",
        "        self.gpt.to(device)\n",
        "        return self\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l995WrFwAwef"
      },
      "source": [
        "## 数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFQfesu0qgZA"
      },
      "outputs": [],
      "source": [
        "#@title 数据加载\n",
        "import json\n",
        "import torch\n",
        "import torch.utils.data as Data\n",
        "from torch import nn, optim\n",
        "import numpy as np\n",
        "\n",
        "# 将文本数据转换为模型输入的数字编码\n",
        "def make_data(file_path, tokenizer):\n",
        "    # 读取文件内容\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    train_datas = []  # 初始化列表存储处理后的数据\n",
        "    for line in lines:\n",
        "        line = line.strip()  # 去除每行的前后空白字符\n",
        "        # 将文本行中的制表符（\\t）替换为[SEP]，并在行尾加上[SEP]\n",
        "        train_data = [i if i != '\\t' else \"[SEP]\" for i in line] + ['[SEP]']\n",
        "        # 使用tokenizer将文本数据编码为数字序列\n",
        "        # print(\"train_data: \", train_data)\n",
        "        train_num_data = tokenizer.encode(train_data)\n",
        "        train_num_data = train_num_data[:-1]  # 去掉最后一个标记（通常是[SEP]）\n",
        "        train_datas.append(train_num_data)  # 将编码后的数据添加到列表中\n",
        "\n",
        "    return train_datas  # 返回所有处理后的数据\n",
        "\n",
        "\n",
        "# 自定义数据集类，继承自torch.utils.data.Dataset\n",
        "class MyDataSet(Data.Dataset):\n",
        "    def __init__(self, datas, vocab2id):\n",
        "        self.datas = datas  # 保存数据\n",
        "        self.vocab2id = vocab2id  # 保存词汇到ID的映射\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        data = self.datas[item]  # 获取指定索引的样本数据\n",
        "        # 划分数据为输入和输出\n",
        "        decoder_input = data[:-1]  # 输入数据，去掉最后一个标记\n",
        "        decoder_output = data[1:]  # 输出数据，从第二个标记开始\n",
        "\n",
        "        # print(\"decoder_input: \", decoder_input, \" decoder_output: \", decoder_output)\n",
        "\n",
        "        # 计算输入和输出的长度\n",
        "        decoder_input_len = len(decoder_input)\n",
        "        decoder_output_len = len(decoder_output)\n",
        "\n",
        "        # 返回样本的输入和输出以及它们的长度\n",
        "        return {\"decoder_input\": decoder_input, \"decoder_input_len\": decoder_input_len,\n",
        "                \"decoder_output\": decoder_output, \"decoder_output_len\": decoder_output_len}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.datas)  # 返回数据集的总长度\n",
        "\n",
        "    def padding_batch(self, batch):\n",
        "        # 获取批次中每个样本的输入和输出长度\n",
        "        decoder_input_lens = [d[\"decoder_input_len\"] for d in batch]\n",
        "        decoder_output_lens = [d[\"decoder_output_len\"] for d in batch]\n",
        "\n",
        "        # 找到输入和输出的最大长度\n",
        "        decoder_input_maxlen = max(decoder_input_lens)\n",
        "        decoder_output_maxlen = max(decoder_output_lens)\n",
        "\n",
        "        # 对每个样本进行填充，使其长度一致\n",
        "        for d in batch:\n",
        "            d[\"decoder_input\"].extend([self.vocab2id[\"[PAD]\"]] * (decoder_input_maxlen - d[\"decoder_input_len\"]))\n",
        "            d[\"decoder_output\"].extend([self.vocab2id[\"[PAD]\"]] * (decoder_output_maxlen - d[\"decoder_output_len\"]))\n",
        "            # print(\"decoder_inputsdecoder_inputs: \", d[\"decoder_input\"], \"decoder_outputdecoder_output:\", d[\"decoder_output\"])\n",
        "\n",
        "        # 将填充后的输入和输出转换为张量\n",
        "        decoder_inputs = torch.tensor([d[\"decoder_input\"] for d in batch], dtype=torch.long)\n",
        "        decoder_outputs = torch.tensor([d[\"decoder_output\"] for d in batch], dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "        return decoder_inputs, decoder_outputs  # 返回填充后的输入和输出张量\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciz7s5bP2GvE"
      },
      "outputs": [],
      "source": [
        "#@title 自定义数据\n",
        "\n",
        "%%writefile selfTxt.txt\n",
        "谢谢你所做的一切\n",
        "你开心就好\n",
        "开心\n",
        "\n",
        "你们宿舍都是这么厉害的人吗\n",
        "是的\n",
        "又高又厉害\n",
        "\n",
        "今天好点了吗？\n",
        "一天比一天严重\n",
        "吃药不管用，去打一针。别拖着\n",
        "\n",
        "是的。下辈子想做只萤火虫\n",
        "可是萤火虫太容易被抓了还是改一个吧\n",
        "不，我只想奋不顾身扑火\n",
        "\n",
        "加油，三月动起来，五月笑起来\n",
        "正解你为什么就那么厉害呢\n",
        "哈哈，没办法，智商就是这么高\n",
        "\n",
        "好身材，秀出来\n",
        "哈哈哈其实我是胖的\n",
        "谢谢\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxy1gjd1A0am"
      },
      "source": [
        "## 训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHYB8ERstu-p"
      },
      "outputs": [],
      "source": [
        "#@title AverageMeter\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # 初始化函数，设置各变量初始值为None，并调用reset函数进行重置\n",
        "        self.count = None  # 记录数据的数量\n",
        "        self.sum = None  # 记录数据的总和\n",
        "        self.avg = None  # 记录当前的平均值\n",
        "        self.val = None  # 记录当前值\n",
        "        self.reset()  # 重置所有变量\n",
        "\n",
        "    def reset(self):\n",
        "        # 重置所有变量的值为初始状态\n",
        "        self.val = 0  # 当前值设为0\n",
        "        self.avg = 0  # 平均值设为0\n",
        "        self.sum = 0  # 总和设为0\n",
        "        self.count = 0  # 数据的数量设为0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        # 更新函数，用于更新当前值、总和、数量及重新计算平均值\n",
        "        self.val = val  # 更新当前值为传入的值\n",
        "        self.sum += val * n  # 根据传入的数量n，将总和增加val * n\n",
        "        self.count += n  # 更新数据的数量\n",
        "        self.avg = self.sum / self.count  # 计算新的平均值\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z-4JRFdqpGo"
      },
      "outputs": [],
      "source": [
        "#@title 训练过程\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "import sys\n",
        "from torch import nn, optim\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# 添加上级目录到系统路径中\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# 定义训练参数的类\n",
        "class TrainArgs:\n",
        "    def __init__(self):\n",
        "        self.device = \"cpu\"  # 训练设备（默认为CPU）\n",
        "        self.batch_size = 4  # 批次大小\n",
        "        self.epochs = 1  # 训练轮数\n",
        "        self.print_every = 10  # 每隔多少步打印一次信息\n",
        "        self.clip = 1  # 梯度裁剪的阈值\n",
        "        # 训练数据文件路径\n",
        "        self.train_file_path = \"/content/drive/MyDrive/train.txt\"\n",
        "        self.save_path = \"GPT2.pt\"  # 模型保存路径\n",
        "        self.lr = 1e-4  # 学习率\n",
        "\n",
        "# 实例化 TrainArgs 类\n",
        "train_args = TrainArgs()\n",
        "\n",
        "# 设置训练参数（这些设置在 TrainArgs 类中已经定义）\n",
        "train_args.device = \"cuda\"\n",
        "train_args.batch_size = 1\n",
        "train_args.epochs = 10\n",
        "train_args.print_every = 10\n",
        "train_args.clip = 1\n",
        "# train_args.train_file_path = \"/content/drive/MyDrive/train.txt\"\n",
        "train_args.train_file_path = \"selfTxt.txt\"\n",
        "train_args.train_file_path = \"/home/tmw/shared/train.txt\"\n",
        "\n",
        "train_args.save_path = \"GPT2.pt\"\n",
        "train_args.lr = 1e-4\n",
        "\n",
        "\n",
        "# 计算训练时间的函数\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time  # 计算总时间\n",
        "    elapsed_mins = int(elapsed_time / 60)  # 转换为分钟\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))  # 剩余秒数\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "# 单步训练函数\n",
        "def train_step(model, data_loader, epoch, optimizer, criterion, clip=1, print_every=None):\n",
        "    model.load_state_dict(torch.load('GPT2.pt'))\n",
        "    model.train()  # 设置模型为训练模式\n",
        "\n",
        "    if print_every is None:\n",
        "        print_every = 1  # 如果未指定打印频率，则默认每步打印一次\n",
        "\n",
        "    epoch_loss = 0  # 初始化本轮的总损失\n",
        "    losses = AverageMeter()  # 用于记录损失的类实例\n",
        "    temp_time = time.time()  # 记录当前时间\n",
        "\n",
        "    # 遍历数据加载器中的每个批次\n",
        "    for step, (dec_inputs, dec_outputs) in enumerate(data_loader):\n",
        "        '''\n",
        "        dec_inputs: [batch_size, tgt_len]\n",
        "        dec_outputs: [batch_size, tgt_len]\n",
        "        '''\n",
        "        # print(\"dec_inputs: \", dec_inputs, \"dec_outputs: \", dec_outputs)\n",
        "        optimizer.zero_grad()  # 清除之前的梯度\n",
        "        dec_inputs, dec_outputs = dec_inputs.to(device), dec_outputs.to(device)  # 将数据移动到设备上\n",
        "\n",
        "        # 使用模型进行前向传播，输出：[batch_size * tgt_len, tgt_vocab_size]\n",
        "        outputs = model(dec_inputs)\n",
        "        outputs = outputs.logits  # 获取模型的输出\n",
        "        outputs = outputs.view(-1, outputs.size(-1))  # 调整输出的维度\n",
        "        loss = criterion(outputs, dec_outputs.view(-1))  # 计算损失\n",
        "        epoch_loss += loss.item()  # 累加损失\n",
        "        losses.update(loss.item(), batch_size)  # 更新损失记录\n",
        "\n",
        "        loss.backward()  # 反向传播计算梯度\n",
        "\n",
        "        # 梯度裁剪，防止梯度爆炸\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()  # 更新模型参数\n",
        "\n",
        "        # 打印训练进度\n",
        "        if print_every and (step + 1) % print_every == 0:\n",
        "            minutes, seconds = epoch_time(temp_time, time.time())\n",
        "            print('Epoch: [{0}][{1}/{2}] '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  'Elapsed {minutes:s}min {seconds:s}s '\n",
        "                  .format(epoch, step + 1, len(data_loader),\n",
        "                          minutes=minutes.__str__(),\n",
        "                          seconds=seconds.__str__(),\n",
        "                          loss=losses))\n",
        "            temp_time = time.time()  # 重置计时器\n",
        "\n",
        "        if step %1000 == 0:\n",
        "            save_path = train_args.save_path\n",
        "            torch.save(model.state_dict(), save_path)  # 保存模型参数\n",
        "            torch.save(model.gpt.state_dict(), f'simpleGPT2.pt')  # 保存模型参数\n",
        "\n",
        "    return epoch_loss / len(data_loader)  # 返回每轮的平均损失\n",
        "\n",
        "# 训练函数\n",
        "def train(model, dataloader, train_args):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0).to(device)  # 定义损失函数\n",
        "    lr = train_args.lr  # 学习率\n",
        "    CLIP = train_args.clip  # 梯度裁剪的阈值\n",
        "    print_every = train_args.print_every  # 打印频率\n",
        "    save_path = train_args.save_path  # 模型保存路径\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)  # 定义优化器\n",
        "\n",
        "    for epoch in range(train_args.epochs):  # 遍历每个训练轮次\n",
        "        start_time = time.time()  # 记录轮次开始时间\n",
        "        train_loss = train_step(model, dataloader, epoch, optimizer, criterion, CLIP, print_every=print_every)  # 进行训练\n",
        "        end_time = time.time()  # 记录轮次结束时间\n",
        "\n",
        "        torch.save(model.state_dict(), save_path)  # 保存模型参数\n",
        "\n",
        "        torch.save(model.gpt.state_dict(), f'simpleGPT2.pt')  # 保存模型参数\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)  # 计算本轮时间\n",
        "        print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f}')  # 打印训练损失\n",
        "\n",
        "# 打印模型参数的总数和可训练参数的数量\n",
        "def print_num_parameters(model):\n",
        "    # 计算总参数数\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'{total_params:,} total parameters.')\n",
        "    # 计算可训练参数数\n",
        "    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'{total_trainable_params:,} training parameters.')\n",
        "\n",
        "# 主函数\n",
        "if __name__ == '__main__':\n",
        "    device = train_args.device  # 获取设备\n",
        "    # 初始化分词器\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "    # torch.save(tokenizer.state_dict(), f'tokenizer.pt')  # 保存模型参数# 保存分词器\n",
        "    tokenizer.save_pretrained('./tokenizer')  # 保存到指定目录\n",
        "\n",
        "    epochs = train_args.epochs  # 训练轮次\n",
        "    batch_size = train_args.batch_size  # 批次大小\n",
        "\n",
        "    train_file_path = train_args.train_file_path  # 训练数据文件路径\n",
        "    datas = make_data(train_file_path, tokenizer)  # 处理数据\n",
        "    dataset = MyDataSet(datas, tokenizer.vocab)  # 创建数据集实例\n",
        "    dataloader = Data.DataLoader(dataset, batch_size=batch_size, collate_fn=dataset.padding_batch)  # 创建数据加载器\n",
        "\n",
        "    model = GPT2().to(device)  # 初始化模型，并将其移动到设备上\n",
        "    train(model, dataloader, train_args)  # 开始训练\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tnZHu30-tTkA"
      },
      "outputs": [],
      "source": [
        "#@title 训练参数参考\n",
        "import argparse\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "# from utils.utils import get_project_rootpath\n",
        "import os\n",
        "\n",
        "# checkpoints_dir = os.path.join(get_project_rootpath(), \"model_checkpoints\")\n",
        "\n",
        "\n",
        "def train_parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"训练参数配置\")\n",
        "    parser.add_argument(\"--device\", type=str, default=\"cuda\", help=\"batch size\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=4, help=\"batch size\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=1, help=\"epochs\")\n",
        "    parser.add_argument(\"--print_every\", type=int, default=10, help=\"print every\")\n",
        "    parser.add_argument(\"--clip\", type=int, default=1, help=\"clip\")\n",
        "\n",
        "\n",
        "    parser.add_argument(\"--train_file_path\", type=str, default=os.path.join(\"\",\"/content/drive/MyDrive/train.txt\"),\n",
        "                        help=\"train_file_path\")\n",
        "\n",
        "    parser.add_argument('--save_path', type=str, default=os.path.join(\"\", \"GPT2.pt\"),\n",
        "                        help='decay step')\n",
        "    parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')\n",
        "\n",
        "\n",
        "    return parser.parse_args()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11XcDuBv7d10"
      },
      "source": [
        "## 验证\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc8p3ARG7RVU"
      },
      "outputs": [],
      "source": [
        "def get_project_rootpath():\n",
        "    \"\"\"\n",
        "    获取项目根目录。此函数的能力体现在，不论当前module被import到任何位置，都可以正确获取项目根目录\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    path = os.path.realpath(os.curdir)\n",
        "    while True:\n",
        "        # PyCharm项目中，'.idea'是必然存在的，且名称唯一\n",
        "        if '.idea' in os.listdir(path):\n",
        "            return path\n",
        "        path = os.path.dirname(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPTIR3PC5dy1"
      },
      "outputs": [],
      "source": [
        "#@title validate result\n",
        "\n",
        "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
        "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "model = GPT2().to(device)\n",
        "# 加载模型权重\n",
        "model.load_state_dict(torch.load('GPT2.pt'))\n",
        "\n",
        "# 设置模型为评估模式\n",
        "model.eval()\n",
        "\n",
        "# 创建文本生成管道\n",
        "text_generator = TextGenerationPipeline(model.gpt, tokenizer)\n",
        "\n",
        "# 使用模型进行文本生成\n",
        "result = text_generator(\"今天好点了吗？\", max_length=100, do_sample=True)\n",
        "print(result)\n",
        "\n",
        "# 使用模型进行文本生成\n",
        "result = text_generator(\"好身材，秀出来\", max_length=100, do_sample=True)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pV-VzsInnwN8"
      },
      "outputs": [],
      "source": [
        "#@title validate result\n",
        "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
        "import torch\n",
        "\n",
        "# 加载tokenizer和模型\n",
        "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\").to('cpu')\n",
        "\n",
        "# 加载训练后的模型权重\n",
        "# model.load_state_dict(torch.load('GPT2.pt', map_location=torch.device('cpu')))\n",
        "\n",
        "# 设置模型为评估模式\n",
        "model.eval()\n",
        "\n",
        "# 创建文本生成管道\n",
        "text_generator = TextGenerationPipeline(model, tokenizer)\n",
        "\n",
        "# 对话互动\n",
        "history = []\n",
        "\n",
        "while True:\n",
        "    # 用户输入\n",
        "    user_input = input(\"你: \")\n",
        "\n",
        "    # 将用户输入加入到对话历史中\n",
        "    history.append(user_input)\n",
        "\n",
        "    # 构造输入给模型\n",
        "    input_text = \" \".join(history)\n",
        "\n",
        "    # 使用模型生成响应\n",
        "    response = text_generator(input_text, max_length=1000, do_sample=True, top_k=50, top_p=0.95)[0]['generated_text']\n",
        "\n",
        "    # 提取模型生成的响应\n",
        "    generated_text = response[len(input_text):].strip()\n",
        "\n",
        "    # 打印模型的响应\n",
        "    print(\"AI:\", generated_text)\n",
        "\n",
        "    # 将模型的响应加入到对话历史中\n",
        "    history.append(generated_text)\n",
        "\n",
        "    # 结束对话条件（可选）\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"再见\", \"拜拜\"]:\n",
        "        print(\"AI: 再见！\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKxI-lKAufs4"
      },
      "source": [
        "## 安卓加载pt模型文件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6s4H02fujHO"
      },
      "outputs": [],
      "source": [
        "#@title 简单加载pt模型文件\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, BertTokenizer\n",
        "\n",
        "# 1. 定义或加载 GPT-2 模型架构\n",
        "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "\n",
        "# 2. 加载保存的模型权重（如果有的话）\n",
        "model.load_state_dict(torch.load('simpleGPT2.pt'))\n",
        "\n",
        "# 3. 设置模型为推理模式\n",
        "model.eval()\n",
        "\n",
        "# 4. 进行推理示例\n",
        "tokenizer = BertTokenizer.from_pretrained(\"/content/tokenizer\")\n",
        "input_text = \"你好\"\n",
        "input_ids = tokenizer(input_text, return_tensors='pt')['input_ids']\n",
        "\n",
        "# 推理\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "\n",
        "# 获取生成的 token ID 序列\n",
        "generated_ids = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "# 结果解码\n",
        "decoded_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# 打印解码后的文本\n",
        "print(decoded_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azXQELIl9l9m"
      },
      "outputs": [],
      "source": [
        "!pip install onnx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jC7PEe5U8PdF"
      },
      "outputs": [],
      "source": [
        "#@title 转成安卓加载\n",
        "\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# 定义并加载 GPT-2 模型架构\n",
        "model = GPT2LMHeadModel.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "\n",
        "# 将模型转换为 ONNX 格式\n",
        "model.eval()\n",
        "dummy_input = torch.randint(0, 1000, (1, 10))  # 修改为适当的输入大小\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"model.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=14,  # 使用较高的 opset 版本\n",
        "    input_names=['input_ids'],\n",
        "    output_names=['output']\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4MS57dyKuaK"
      },
      "outputs": [],
      "source": [
        "!cp model.onnx /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# 加载 ONNX 模型\n",
        "onnx_model_path = \"model.onnx\"\n",
        "session = ort.InferenceSession(onnx_model_path)\n",
        "\n",
        "# 定义测试输入\n",
        "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "input_text = \"你好\"\n",
        "\n",
        "# 将输入文本编码为 token ids，并填充到 max_length=10\n",
        "input_ids = tokenizer(\n",
        "    input_text,\n",
        "    return_tensors='pt',        # 返回 PyTorch 张量\n",
        "    padding='max_length',       # 填充到最大长度\n",
        "    max_length=10,              # 设置最大长度为 10\n",
        "    truncation=True             # 如果输入文本超过长度限制，则截断\n",
        ")['input_ids']\n",
        "\n",
        "print(\"Padded input_ids:\", input_ids)\n",
        "\n",
        "# 将 PyTorch 张量转换为 NumPy 数组\n",
        "input_ids_np = input_ids.numpy()\n",
        "\n",
        "# 准备输入字典（ONNX 期望的输入是 NumPy 数组）\n",
        "inputs = {\"input_ids\": input_ids_np}\n",
        "print(\"inputs:\", inputs)\n",
        "\n",
        "# 运行推理\n",
        "outputs = session.run(None, inputs)\n",
        "\n",
        "# 打印输出\n",
        "print(\"ONNX model output:\", outputs)\n",
        "\n",
        "# 如果需要处理输出，例如获取生成的 token，解码文本等：\n",
        "generated_ids = torch.argmax(torch.tensor(outputs[0]), dim=-1)\n",
        "decoded_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated text:\", decoded_text)"
      ],
      "metadata": {
        "id": "18WeleCVRNPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title P采样\n",
        "\n",
        "import onnxruntime as ort\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# 加载 ONNX 模型\n",
        "onnx_model_path = \"model.onnx\"\n",
        "session = ort.InferenceSession(onnx_model_path)\n",
        "\n",
        "# 定义测试输入\n",
        "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "input_text = \"你好\"\n",
        "\n",
        "# 将输入文本编码为 token ids，并填充到 max_length=10\n",
        "input_ids = tokenizer(\n",
        "    input_text,\n",
        "    return_tensors='pt',        # 返回 PyTorch 张量\n",
        "    padding='max_length',       # 填充到最大长度\n",
        "    max_length=10,              # 设置最大长度为 10\n",
        "    truncation=True             # 如果输入文本超过长度限制，则截断\n",
        ")['input_ids']\n",
        "\n",
        "print(\"Padded input_ids:\", input_ids)\n",
        "\n",
        "# 将 PyTorch 张量转换为 NumPy 数组\n",
        "input_ids_np = input_ids.numpy()\n",
        "\n",
        "# 准备输入字典（ONNX 期望的输入是 NumPy 数组）\n",
        "inputs = {\"input_ids\": input_ids_np}\n",
        "print(\"inputs:\", inputs)\n",
        "\n",
        "# 生成文本的最大长度\n",
        "max_length = 100  # 可以根据需求调整长度\n",
        "\n",
        "# 初始化生成的 token ids\n",
        "generated_ids = input_ids_np[0].tolist()  # 使用单个句子的 token ids\n",
        "\n",
        "for _ in range(max_length - input_ids.shape[1]):  # 减去初始长度\n",
        "    # 将生成的 token ids 转为 NumPy 数组，并确保形状正确\n",
        "    input_ids_np = np.array(generated_ids[-10:]).reshape(1, -1)  # 保持二维数组的形状\n",
        "\n",
        "    # 运行推理\n",
        "    outputs = session.run(None, {\"input_ids\": input_ids_np})\n",
        "\n",
        "    # 处理 logits\n",
        "    logits = torch.tensor(outputs[0])  # 输出通常是 logits，假设第一个输出是 logits\n",
        "    probs = torch.softmax(logits, dim=-1)  # 使用 softmax 转化为概率分布\n",
        "    temperature = 1.0  # 温度系数可以调节采样随机性，越小越接近贪婪搜索\n",
        "    probs = torch.pow(probs, 1.0 / temperature)  # 调整温度\n",
        "    probs = probs / torch.sum(probs, dim=-1, keepdim=True)  # 重新归一化概率\n",
        "\n",
        "    # 从最后一个 token 的概率分布中采样\n",
        "    last_token_probs = probs[:, -1, :]  # 只使用最后一个时间步的概率\n",
        "    sampled_token_ids = torch.multinomial(last_token_probs, num_samples=1)  # 采样出下一个 token\n",
        "\n",
        "    # 添加采样的 token id 到生成的序列中\n",
        "    sampled_token_id = sampled_token_ids.numpy().flatten()[0]  # 提取单个 token id\n",
        "    generated_ids.append(sampled_token_id)\n",
        "\n",
        "    # 如果生成的 token 是结束符号，可以提前停止\n",
        "    if sampled_token_id == tokenizer.eos_token_id:\n",
        "        break\n",
        "\n",
        "# 将生成的 token ids 转化为文本\n",
        "decoded_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated text:\", decoded_text)\n"
      ],
      "metadata": {
        "id": "2g_wQjQkR6jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 可以调试的p参数\n",
        "\n",
        "\n",
        "import onnxruntime as ort\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# Load ONNX model\n",
        "onnx_model_path = \"model.onnx\"\n",
        "session = ort.InferenceSession(onnx_model_path)\n",
        "\n",
        "# Define test input\n",
        "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "input_text = \"你好\"\n",
        "\n",
        "# Encode input text into token ids with padding\n",
        "input_ids = tokenizer(\n",
        "    input_text,\n",
        "    return_tensors='pt',        # Return PyTorch tensors\n",
        "    padding='max_length',       # Pad to maximum length\n",
        "    max_length=10,              # Set max length to 10\n",
        "    truncation=True             # Truncate if input text exceeds length limit\n",
        ")['input_ids']\n",
        "\n",
        "print(\"Padded input_ids:\", input_ids)\n",
        "\n",
        "# Convert PyTorch tensor to NumPy array\n",
        "input_ids_np = input_ids.numpy()\n",
        "\n",
        "# Prepare input dictionary for ONNX model\n",
        "inputs = {\"input_ids\": input_ids_np}\n",
        "print(\"inputs:\", inputs)\n",
        "\n",
        "# Set generation parameters\n",
        "max_length = 100  # Maximum length of the generated text\n",
        "p = 0.9  # Top-p value\n",
        "temperature = 1.0  # Temperature for sampling\n",
        "\n",
        "# Initialize generated token ids\n",
        "generated_ids = input_ids_np[0].tolist()  # Use single sentence token ids\n",
        "\n",
        "for _ in range(max_length - input_ids.shape[1]):  # Subtract initial length\n",
        "    # Ensure correct shape for input_ids_np\n",
        "    input_ids_np = np.array(generated_ids[-10:]).reshape(1, -1)  # Maintain 2D shape\n",
        "\n",
        "    # Run inference\n",
        "    outputs = session.run(None, {\"input_ids\": input_ids_np})\n",
        "\n",
        "    # Process logits\n",
        "    logits = torch.tensor(outputs[0])  # Assuming the first output is logits\n",
        "    probs = torch.softmax(logits, dim=-1)  # Convert logits to probability distribution\n",
        "    probs = torch.pow(probs, 1.0 / temperature)  # Adjust temperature\n",
        "    probs = probs / torch.sum(probs, dim=-1, keepdim=True)  # Re-normalize probabilities\n",
        "\n",
        "    # Calculate top-p sampling\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)\n",
        "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "    sorted_indices_to_keep = cumulative_probs <= p\n",
        "    top_p_probs = torch.where(sorted_indices_to_keep, sorted_probs, torch.tensor(0.0))\n",
        "\n",
        "    # Ensure no zero probabilities\n",
        "    top_p_probs = torch.clamp(top_p_probs, min=1e-8)  # Set a small positive value if probabilities are zero\n",
        "    top_p_probs = top_p_probs / torch.sum(top_p_probs, dim=-1, keepdim=True)  # Normalize\n",
        "\n",
        "    # Sample from top-p probability distribution\n",
        "    last_token_probs = top_p_probs[:, -1, :]\n",
        "    sampled_token_ids = torch.multinomial(last_token_probs, num_samples=1)  # Sample next token\n",
        "\n",
        "    # Convert sampled token id to integer and append\n",
        "    sampled_token_id = sampled_token_ids.numpy().flatten()[0]  # Extract single token id\n",
        "    generated_ids.append(sampled_token_id)\n",
        "\n",
        "    # If the generated token is the end of sequence token, stop early\n",
        "    if sampled_token_id == tokenizer.eos_token_id:\n",
        "        break\n",
        "\n",
        "# Decode generated token ids into text\n",
        "decoded_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated text:\", decoded_text)\n"
      ],
      "metadata": {
        "id": "mHPF-yczcrlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title k采样\n",
        "\n",
        "import onnxruntime as ort\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# 加载 ONNX 模型\n",
        "onnx_model_path = \"model.onnx\"\n",
        "session = ort.InferenceSession(onnx_model_path)\n",
        "\n",
        "# 定义测试输入\n",
        "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "input_text = \"你好\"\n",
        "\n",
        "# 将输入文本编码为 token ids，并填充到 max_length=10\n",
        "input_ids = tokenizer(\n",
        "    input_text,\n",
        "    return_tensors='pt',        # 返回 PyTorch 张量\n",
        "    padding='max_length',       # 填充到最大长度\n",
        "    max_length=10,              # 设置最大长度为 10\n",
        "    truncation=True             # 如果输入文本超过长度限制，则截断\n",
        ")['input_ids']\n",
        "\n",
        "print(\"Padded input_ids:\", input_ids)\n",
        "\n",
        "# 将 PyTorch 张量转换为 NumPy 数组\n",
        "input_ids_np = input_ids.numpy()\n",
        "\n",
        "# 准备输入字典（ONNX 期望的输入是 NumPy 数组）\n",
        "inputs = {\"input_ids\": input_ids_np}\n",
        "print(\"inputs:\", inputs)\n",
        "\n",
        "# 生成文本的最大长度\n",
        "max_length = 200  # 可以根据需求调整长度\n",
        "top_k = 50       # Top-K 参数\n",
        "temperature = 1.0  # 温度系数\n",
        "\n",
        "# 初始化生成的 token ids\n",
        "generated_ids = input_ids_np[0].tolist()  # 使用单个句子的 token ids\n",
        "\n",
        "def top_k_sampling(logits, k=50):\n",
        "    \"\"\"执行 Top-K 采样\"\"\"\n",
        "    # 对 logits 应用 softmax 转化为概率分布\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # 获取 top-k 的 token 及其概率\n",
        "    topk_probs, topk_indices = torch.topk(probs, k=k, dim=-1)\n",
        "\n",
        "    # 计算 top-k 的概率分布\n",
        "    topk_probs = topk_probs / torch.sum(topk_probs, dim=-1, keepdim=True)\n",
        "\n",
        "    # 从 top-k 的概率分布中采样\n",
        "    sampled_token_ids = torch.multinomial(topk_probs, num_samples=1)\n",
        "\n",
        "    # 从 top-k indices 中选择采样的 token id\n",
        "    sampled_token_ids = torch.gather(topk_indices, dim=-1, index=sampled_token_ids)\n",
        "\n",
        "    return sampled_token_ids\n",
        "\n",
        "for _ in range(max_length - input_ids.shape[1]):  # 减去初始长度\n",
        "    # 将生成的 token ids 转为 NumPy 数组，并确保形状正确\n",
        "    input_ids_np = np.array(generated_ids[-10:]).reshape(1, -1)  # 保持二维数组的形状\n",
        "\n",
        "    # 运行推理\n",
        "    outputs = session.run(None, {\"input_ids\": input_ids_np})\n",
        "\n",
        "    # 处理 logits\n",
        "    logits = torch.tensor(outputs[0])  # 输出通常是 logits，假设第一个输出是 logits\n",
        "\n",
        "    if len(logits.shape) == 3:\n",
        "        logits = logits[:, -1, :]  # 只使用最后一个时间步的 logits\n",
        "    elif len(logits.shape) == 2:\n",
        "        logits = logits.unsqueeze(1)  # 在没有时间步维度的情况下，添加时间步维度\n",
        "\n",
        "    # 使用 Top-K 采样\n",
        "    sampled_token_ids = top_k_sampling(logits, k=top_k)  # 使用 Top-K 采样\n",
        "\n",
        "    # 对采样的 token id 应用温度调节\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    probs = torch.pow(probs, 1.0 / temperature)  # 调整温度\n",
        "    probs = probs / torch.sum(probs, dim=-1, keepdim=True)  # 重新归一化概率\n",
        "\n",
        "    # 从 top-k 的概率分布中重新采样\n",
        "    last_token_probs = probs[:, -1, :] if len(probs.shape) == 3 else probs\n",
        "    sampled_token_ids = torch.multinomial(last_token_probs, num_samples=1)\n",
        "\n",
        "    # 添加采样的 token id 到生成的序列中\n",
        "    sampled_token_id = sampled_token_ids.numpy().flatten()[0]  # 提取单个 token id\n",
        "    generated_ids.append(sampled_token_id)\n",
        "\n",
        "    # 如果生成的 token 是结束符号，可以提前停止\n",
        "    if sampled_token_id == tokenizer.eos_token_id:\n",
        "        break\n",
        "\n",
        "# 将生成的 token ids 转化为文本\n",
        "decoded_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated text:\", decoded_text)\n"
      ],
      "metadata": {
        "id": "V0Yz5tEzXZ89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title P采样和K采样同时使用\n",
        "\n",
        "import onnxruntime as ort\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# 加载 ONNX 模型\n",
        "onnx_model_path = \"model.onnx\"\n",
        "session = ort.InferenceSession(onnx_model_path)\n",
        "\n",
        "# 定义测试输入\n",
        "tokenizer = BertTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
        "input_text = \"你身材真好啊！\"\n",
        "\n",
        "# 将输入文本编码为 token ids，并填充到 max_length=10\n",
        "input_ids = tokenizer(\n",
        "    input_text,\n",
        "    return_tensors='pt',        # 返回 PyTorch 张量\n",
        "    padding='max_length',       # 填充到最大长度\n",
        "    max_length=10,              # 设置最大长度为 10\n",
        "    truncation=True             # 如果输入文本超过长度限制，则截断\n",
        ")['input_ids']\n",
        "\n",
        "print(\"Padded input_ids:\", input_ids)\n",
        "\n",
        "# 将 PyTorch 张量转换为 NumPy 数组\n",
        "input_ids_np = input_ids.numpy()\n",
        "\n",
        "# 准备输入字典（ONNX 期望的输入是 NumPy 数组）\n",
        "inputs = {\"input_ids\": input_ids_np}\n",
        "print(\"inputs:\", inputs)\n",
        "\n",
        "# 生成文本的最大长度\n",
        "max_length = 200  # 可以根据需求调整长度\n",
        "top_k = 50       # Top-K 参数\n",
        "temperature = 1.0  # 温度系数\n",
        "p = 0.9  # 设置 top-p 的值\n",
        "\n",
        "# 初始化生成的 token ids\n",
        "generated_ids = input_ids_np[0].tolist()  # 使用单个句子的 token ids\n",
        "\n",
        "def top_k_sampling(logits, k=50):\n",
        "    \"\"\"执行 Top-K 采样\"\"\"\n",
        "    # 对 logits 应用 softmax 转化为概率分布\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # 获取 top-k 的 token 及其概率\n",
        "    topk_probs, topk_indices = torch.topk(probs, k=k, dim=-1)\n",
        "\n",
        "    # 计算 top-k 的概率分布\n",
        "    topk_probs = topk_probs / torch.sum(topk_probs, dim=-1, keepdim=True)\n",
        "\n",
        "    return topk_probs, topk_indices\n",
        "\n",
        "for index in range(max_length - input_ids.shape[1]):  # 减去初始长度\n",
        "    # 将生成的 token ids 转为 NumPy 数组，并确保形状正确\n",
        "    input_ids_np = np.array(generated_ids[-10:]).reshape(1, -1)  # 保持二维数组的形状\n",
        "\n",
        "    # 运行推理\n",
        "    outputs = session.run(None, {\"input_ids\": input_ids_np})\n",
        "\n",
        "    # 解码 token IDs 为文本\n",
        "    decoded_text = tokenizer.decode(input_ids_np[0], skip_special_tokens=True)\n",
        "    # print(\"Decoded text:\", decoded_text)\n",
        "\n",
        "    # 检查文本是否以句号结尾\n",
        "    if decoded_text.strip().endswith(\"。\"):\n",
        "        # print(\"The text ends with a period.\")\n",
        "        if index >= max_length * 2 / 3:\n",
        "          break\n",
        "\n",
        "\n",
        "    # 处理 logits\n",
        "    logits = torch.tensor(outputs[0])  # 输出通常是 logits，假设第一个输出是 logits\n",
        "\n",
        "    if len(logits.shape) == 3:\n",
        "        logits = logits[:, -1, :]  # 只使用最后一个时间步的 logits\n",
        "    elif len(logits.shape) == 2:\n",
        "        logits = logits.unsqueeze(1)  # 在没有时间步维度的情况下，添加时间步维度\n",
        "\n",
        "    # 使用 Top-K 采样\n",
        "    topk_probs, topk_indices = top_k_sampling(logits, k=top_k)\n",
        "\n",
        "    # 使用温度调整\n",
        "    topk_probs = torch.pow(topk_probs, 1.0 / temperature)  # 调整温度\n",
        "    topk_probs = topk_probs / torch.sum(topk_probs, dim=-1, keepdim=True)  # 重新归一化概率\n",
        "\n",
        "    # 从 top-k 的概率分布中采样\n",
        "    sampled_token_ids = torch.multinomial(topk_probs, num_samples=1)\n",
        "\n",
        "    # 从 top-k indices 中选择采样的 token id\n",
        "    sampled_token_ids = torch.gather(topk_indices, dim=-1, index=sampled_token_ids)\n",
        "\n",
        "    # 添加采样的 token id 到生成的序列中\n",
        "    sampled_token_id = sampled_token_ids.numpy().flatten()[0]  # 提取单个 token id\n",
        "    generated_ids.append(sampled_token_id)\n",
        "\n",
        "    # 如果生成的 token 是结束符号，可以提前停止\n",
        "    if sampled_token_id == tokenizer.eos_token_id:\n",
        "        break\n",
        "\n",
        "# 将生成的 token ids 转化为文本\n",
        "decoded_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated text:\", decoded_text)\n"
      ],
      "metadata": {
        "id": "6ImZjqSFX48m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}