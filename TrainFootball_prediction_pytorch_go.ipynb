{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 通过go获取数据"
      ],
      "metadata": {
        "id": "jcqTIFe48bsS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nULIOhqi680r"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keygen -t rsa -b 4096 -C \"694497013@qq.com\"\n",
        "!ssh-keyscan -t rsa github.com >>~/.ssh/known_hosts\n",
        "!ssh-keyscan -t rsa gitee.com >>~/.ssh/known_hosts\n",
        "!cat ~/.ssh/id_rsa.pub"
      ],
      "metadata": {
        "id": "JeId8I1x7Ula"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone git@github.com:ccvvx1/go-test-new-part1.git"
      ],
      "metadata": {
        "id": "OVWFVaX77J-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://go.dev/dl/go1.20.7.linux-amd64.tar.gz\n",
        "!mkdir /content/go\n",
        "!tar zxvf go1.20.7.linux-amd64.tar.gz -C /content/go\n",
        "!ln -sf /content/go/go/bin/go /usr/local/bin/go\n",
        "!go env -w GO111MODULE=auto\n",
        "!go get github.com/PuerkitoBio/goquery\n",
        "!go get github.com/dgrijalva/jwt-go\n",
        "!go get github.com/garyburd/redigo/redis\n",
        "!go get github.com/go-sql-driver/mysql\n",
        "!go get github.com/gohouse/gorose\n",
        "!go get github.com/json-iterator/go\n",
        "!go get github.com/mattn/go-sqlite3\n",
        "!go get github.com/mohae/deepcopy\n",
        "!go get github.com/r3labs/sse\n",
        "!go get github.com/richardlehane/mscfb\n",
        "!go get github.com/tealeg/xlsx\n",
        "!go get github.com/xuri/efp\n",
        "!go get github.com/xuri/excelize\n",
        "!go get github.com/yuin/gopher-lua\n",
        "!go get gonum.org/v1/gonum/stat\n",
        "!go get google.golang.org/grpc\n",
        "!go get gopkg.in/yaml.v3\n",
        "!go get go.mongodb.org/mongo-driver/bson\n",
        "!apt install rar\n",
        "!apt-get install -y nodejs\n",
        "!apt-get install -y npm\n",
        "!npm install -g http-server\n",
        "!curl -Ls https://github.com/ekzhang/bore/releases/download/v0.4.0/bore-v0.4.0-x86_64-unknown-linux-musl.tar.gz | tar zx -C /usr/bin\n",
        "!curl -Lo /usr/bin/cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 && chmod +x /usr/bin/cloudflared\n",
        "%cd /content/go-test-new-part1\n",
        "!go build -o createScript.out\n",
        "!cp createScript.out ../\n",
        "%cd /content\n",
        "!mkdir ftTrainData"
      ],
      "metadata": {
        "id": "HykJizw28Znk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!go get github.com/golang/snappy\n",
        "!go get github.com/klauspost/compress/zstd\n",
        "!go get github.com/xdg-go/scram\n",
        "!go get github.com/youmark/pkcs8\n",
        "!go get golang.org/x/sync/errgroup\n",
        "%cd /content/go-test-new-part1\n",
        "!go build -o createScript.out"
      ],
      "metadata": {
        "id": "npJhUkB3-y7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cookie.txt\n",
        "b-user-id=d6afe009-bd84-e36a-a39d-7a2024814a5e; _ga_H8WNRJTRHH=GS1.1.1717167078.1.1.1717167115.0.0.0; __utmz=56961525.1717167131.2.2.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; LastUrl=; PHPSESSID=de23b19b57de40a5752ba551788ee015513e32cf; FirstURL=www.okooo.com/; FirstOKURL=https%3A//www.okooo.com/danchang/; First_Source=www.okooo.com; __utmc=56961525; pm=; _gid=GA1.2.311102218.1727369033; IMUserID=24275694; IMUserName=ok_171231822983; OkAutoUuid=d6bbc7e375729c02b9ad7c9b8a00995f; OkMsIndex=4; OKSID=de23b19b57de40a5752ba551788ee015513e32cf; M_UserName=%22ok_171231822983%22; M_UserID=24275694; M_Ukey=1e7b0d5d7374d2d016b8c67aa639585d; OkTouchAutoUuid=d6bbc7e375729c02b9ad7c9b8a00995f; OkTouchMsIndex=4; isInvitePurview=0; _ga=GA1.1.1782144738.1710862920; _ga_62RZYRYQFM=GS1.1.1727447786.1.1.1727449440.60.0.0; _c_WBKFRo=EZ30dV6cNJ9qjLWgCc7UUWiiTAaIJfywoungdWw3; _nb_ioWEgULi=; acw_tc=ac11000117274971739345404e005b48ea0a1be453501c40387144f5cbbb59; __utma=56961525.1782144738.1710862920.1727453985.1727497391.12; __utmb=56961525.3.8.1727497391; _ga_B3LCXP8H9E=GS1.1.1727487246.9.1.1727498729.60.0.0"
      ],
      "metadata": {
        "id": "ZQOKTCE68vgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "./createScript.out ft --i 1257720,1267752,1267753,1267754,1267756,1267757,1271707,1271708,1259748,1266449,1259198,1263336,1263335,1273837,1263339,1263337,1263334,1263333,1270744,\n",
        "1261289,1261287,1261288,1261286,1261797,1269308,1271077,1257717,1267759,1263025,1244904,1244905,1244906,1243361,1243363,1243364,1243362,1264340,1268350,1270073,1259751,1266448,1245293,1249367,1249362,1258586,1258587,12585\n",
        "88,1248646,1248236,1248237,1248238,1248239,1248240,1247039,1248647,1248648,1248649,1248650,1249805,1270380,1247041,1247040,1249806,1249808,1271709,1271710,1271713,1260837,1260129,1264334,1264335,1264339,1264886,1264893,12\n",
        "65442,1265448,1263332,1262258,1267758,1257718,1242440,1270382,1274082,1274083,1269309,1269311,1269312,1269313,1269315,1260130,1260131,1260133,1260134,1260135,1264333,1264336,1264337,1264338,1264341,1264342,1264344,1264885\n",
        ",1264887,1264888,1264889,1264890,1264891,1264892,1264894,1264896,1265437,1265438,1265439,1265440,1265441,1265443,1265444,1265445,1265446,1267035,1267036,"
      ],
      "metadata": {
        "id": "8YoGnS3_87WN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 训练模型"
      ],
      "metadata": {
        "id": "RM3fHdiA8f9M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6anRp8jeuTf"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title 链接Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YtGaXeHCtsnG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7c6f699-8234-4790-b88f-6ae3b2a8f5d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/ftTrainDataTest’: No such file or directory\n",
            "unzip:  cannot find or open /home/tmw/shared/ftTrainDataTest.zip, /home/tmw/shared/ftTrainDataTest.zip.zip or /home/tmw/shared/ftTrainDataTest.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "# !mkdir /content/drive/MyDrive/ftTrainDataDetail\n",
        "!mkdir /content/drive/MyDrive/ftTrainDataTest\n",
        "# !rm -rf /content/drive/MyDrive/ftTrainDataDetail/*\n",
        "# !unzip -o /home/tmw/shared/ftTrainData.zip -d /content/drive/MyDrive/ftTrainDataDetail\n",
        "!unzip -o /home/tmw/shared/ftTrainDataTest.zip -d /content/drive/MyDrive/ftTrainDataTest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KPXkrGlTxOV"
      },
      "source": [
        "## 获取数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "CbHXI5I4hp_o"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "#@title 拼接数据\n",
        "\n",
        "# filepath = '/kaggle/input/stock-time-series-20050101-to-20171231/AMZN_2006-01-01_to_2018-01-01.csv'\n",
        "# filepath = '/kaggle/input/amazon.csv'\n",
        "import pandas as pd\n",
        "\n",
        "# 获取/content/hello目录下所有的CSV文件\n",
        "file_paths = [os.path.join('/content/drive/MyDrive/ftTrainDataDetail', f) for f in os.listdir('/content/drive/MyDrive/ftTrainDataDetail') if f.endswith('.csv')]\n",
        "\n",
        "dataframes = []\n",
        "tgtLength = 500\n",
        "\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path, encoding='latin1')\n",
        "    df = df[::-1]  # 反转 DataFrame 的顺序\n",
        "\n",
        "    if len(df) == 0:\n",
        "        continue\n",
        "    # 检查文件大小\n",
        "    if len(df) < tgtLength:\n",
        "        # 计算需要重复的行数\n",
        "        num_rows_to_add = tgtLength - len(df)\n",
        "        last_row = df.iloc[-1:]  # 获取最后一行\n",
        "        # 重复最后一行\n",
        "        repeated_rows = pd.concat([last_row] * num_rows_to_add, ignore_index=True)\n",
        "        df = pd.concat([df, repeated_rows], ignore_index=True)\n",
        "        # 创建补充的行，填充为零\n",
        "        # zero_rows = pd.DataFrame(0, index=range(num_rows_to_add), columns=df.columns)\n",
        "        # df = pd.concat([df, zero_rows], ignore_index=True)\n",
        "    else:\n",
        "        df = df.iloc[:tgtLength]\n",
        "\n",
        "    dataframes.append(df)\n",
        "\n",
        "# 合并所有数据\n",
        "data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "\n",
        "# 保存为 CSV 文件\n",
        "data.to_csv('hello.csv', index=False)\n",
        "\n",
        "#@title 数据去除特殊符号\n",
        "\n",
        "dataMain = data[['Main']]\n",
        "dataEven = data[['Even']]\n",
        "dataLoss = data[['Loss']]\n",
        "dataResult = data[['Result']]\n",
        "\n",
        "dataMain['Main'] = dataMain['Main'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "dataEven['Even'] = dataEven['Even'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "dataLoss['Loss'] = dataLoss['Loss'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "\n",
        "\n",
        "# # 检查非数字字符串\n",
        "# def clean_column(column):\n",
        "#     # 移除特定字符\n",
        "#     column = column.str.replace('↑', '').str.replace('↓', '')\n",
        "#     # 检查无法转换的值\n",
        "#     invalid_values = column[~column.str.replace('.', '', regex=False).str.isnumeric()]\n",
        "#     if not invalid_values.empty:\n",
        "#         print(\"无效值:\", invalid_values)\n",
        "#     # 转换为 float\n",
        "#     return column.astype(float, errors='coerce')\n",
        "\n",
        "# # 清理并转换\n",
        "# dataMain['Main'] = clean_column(dataMain['Main'])\n",
        "# dataMain['Even'] = clean_column(dataMain['Even'])\n",
        "# dataMain['Loss'] = clean_column(dataMain['Loss'])\n",
        "\n",
        "\n",
        "#@title 数据归一化\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scalerMain = MinMaxScaler(feature_range=(-1, 1))\n",
        "scalerEven = MinMaxScaler(feature_range=(-1, 1))\n",
        "scalerLoss = MinMaxScaler(feature_range=(-1, 1))\n",
        "scalerResult = MinMaxScaler(feature_range=(-1, 1))\n",
        "dataMain['Main'] = scalerMain.fit_transform(dataMain['Main'].values.reshape(-1,1))\n",
        "dataEven['Even'] = scalerEven.fit_transform(dataEven['Even'].values.reshape(-1,1))\n",
        "dataLoss['Loss'] = scalerLoss.fit_transform(dataLoss['Loss'].values.reshape(-1,1))\n",
        "dataResult['Result'] = dataResult['Result']\n",
        "print(dataMain['Main'] .shape)\n",
        "dataMain['Main']\n",
        "\n",
        "# def scale_in_chunks(data, chunk_size=500):\n",
        "#     scalers = []\n",
        "#     scaled_data = []\n",
        "\n",
        "#     # 将数据分成若干块\n",
        "#     data_chunks = np.array_split(data, np.arange(chunk_size, len(data), chunk_size))\n",
        "\n",
        "#     for chunk in data_chunks:\n",
        "#         scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "#         scaled_chunk = scaler.fit_transform(chunk.values.reshape(-1, 1))\n",
        "#         scalers.append(scaler)  # 如果需要在后续反向转换时使用\n",
        "#         scaled_data.append(scaled_chunk)\n",
        "\n",
        "#     return np.vstack(scaled_data), scalers\n",
        "\n",
        "# # 对每个数据集进行分块缩放\n",
        "# dataMain['Main'], scalers_main = scale_in_chunks(dataMain['Main'])\n",
        "# dataEven['Even'], scalers_even = scale_in_chunks(dataEven['Even'])\n",
        "# dataLoss['Loss'], scalers_loss = scale_in_chunks(dataLoss['Loss'])\n",
        "# print(dataMain['Main'], dataEven['Even'], dataLoss['Loss'])\n",
        "# dataResult['Result'] 保持原样"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ehyV_uihp_s"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "\n",
        "# sns.set_style(\"darkgrid\")\n",
        "# plt.figure(figsize = (15,9))\n",
        "# plt.plot(data[['Main']])\n",
        "# plt.plot(data[['Even']])\n",
        "# plt.plot(data[['Loss']])\n",
        "# plt.xticks(range(0,data.shape[0],500),data['Date'].loc[::500],rotation=45)\n",
        "# plt.title(\"Amazon Stock Price\",fontsize=18, fontweight='bold')\n",
        "# plt.xlabel('Date',fontsize=18)\n",
        "# plt.ylabel('Close Price (USD)',fontsize=18)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrjH2cFEhp_s"
      },
      "source": [
        "## 数据处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fx4S1tcNhp_x"
      },
      "outputs": [],
      "source": [
        "#@title 训练模型\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_dim = 1\n",
        "hidden_dim = 32\n",
        "num_layers = 2\n",
        "output_dim = 1\n",
        "output_cls_dim = 3\n",
        "num_epochs = 100\n",
        "batch_size = 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def split_data(dataMain, dataEven, dataLoss, dataResult, lookback, batch_size = 20):\n",
        "    data_raw_main = dataMain.to_numpy()\n",
        "    data_raw_even = dataEven.to_numpy()\n",
        "    data_raw_loss = dataLoss.to_numpy()\n",
        "    data_raw_result = dataResult.to_numpy()\n",
        "    data_main = []\n",
        "    data_even = []\n",
        "    data_loss = []\n",
        "    data_result = []\n",
        "\n",
        "    # create all possible sequences of length lookback\n",
        "    for index in range(len(data_raw_main) - lookback):\n",
        "        data_main.append(data_raw_main[index: index + lookback])\n",
        "    for index in range(len(data_raw_even) - lookback):\n",
        "        data_even.append(data_raw_even[index: index + lookback])\n",
        "    for index in range(len(data_raw_loss) - lookback):\n",
        "        data_loss.append(data_raw_loss[index: index + lookback])\n",
        "    for index in range(len(data_raw_result) - lookback):\n",
        "        data_result.append(data_raw_result[index: index + lookback])\n",
        "\n",
        "    data_main = np.array(data_main)\n",
        "    data_even = np.array(data_even)\n",
        "    data_loss = np.array(data_loss)\n",
        "    data_result = np.array(data_result)\n",
        "\n",
        "    test_set_size = int(np.round(0.2 * data_main.shape[0]))\n",
        "    train_set_size = data_main.shape[0]\n",
        "\n",
        "    # 将训练数据切割成多个批次\n",
        "    num_batches = train_set_size // batch_size\n",
        "    print(\"num_batches: \", num_batches, \" data_main.shape[0]: \", data_main.shape[0])\n",
        "\n",
        "\n",
        "    x_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, :-1, :], num_batches)\n",
        "    }\n",
        "    y_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, -1, :], num_batches)\n",
        "    }\n",
        "\n",
        "    x_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, :-1, :], num_batches)\n",
        "    }\n",
        "    y_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, -1, :], num_batches)\n",
        "    }\n",
        "    # print(\"x_train['main']: \", x_train['main'][1])\n",
        "    return [x_train, y_train, x_test, y_test]\n",
        "\n",
        "\n",
        "lookback = 10 # choose sequence length\n",
        "x_train, y_train, x_test, y_test = split_data(dataMain, dataEven, dataLoss, dataResult, lookback, 495)\n",
        "# x_train_batches, y_train_batches, x_test, y_test = split_data(price, lookback=20, batch_size=32)\n",
        "\n",
        "# # 打印批次的数量和每个批次的形状\n",
        "# print('Number of training batches:', len(x_train_batches))\n",
        "# if len(x_train_batches) > 0:\n",
        "#     print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "#     print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "# print('x_test.shape = ', x_test.shape)\n",
        "# print('y_test.shape = ', y_test.shape)\n",
        "\n",
        "# 打印批次的数量和每个批次的形状\n",
        "print('Number of training batches:', len(x_train['main']))\n",
        "if len(x_train['main']) > 0:\n",
        "    # print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "    # print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "    print('x_train.shape = ',x_train['main'][0].shape)\n",
        "    print('y_train.shape = ',y_train['main'][0].shape)\n",
        "    print('x_test.shape = ',x_test['main'][0].shape)\n",
        "    print('y_test.shape = ',y_test['main'][0].shape)\n",
        "    print('x_result.shape = ',y_train['result'][0].shape)\n",
        "\n",
        "# print(\"x_train: \", x_train)\n",
        "# print(\"y_train: \", y_train)\n",
        "\n",
        "# x_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train_batches]\n",
        "# y_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train_batches]\n",
        "import torch\n",
        "\n",
        "# 转换为字典\n",
        "train_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['result']]\n",
        "}\n",
        "\n",
        "test_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['result']]\n",
        "}\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.fcCls = nn.Linear(hidden_dim, output_cls_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "        out1 = self.fc(out[:, -1, :])\n",
        "\n",
        "        # 通过全连接层\n",
        "        outCls = self.fcCls(out[:, -1, :])\n",
        "        # 应用 softmax 激活\n",
        "        outCls = torch.softmax(outCls, dim=1)\n",
        "\n",
        "        return out1, outCls\n",
        "\n",
        "\n",
        "\n",
        "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "\n",
        "model_path = '/content/drive/MyDrive/ft_model_weights.pth'\n",
        "\n",
        "# 判断模型文件是否存在\n",
        "if os.path.exists(model_path):\n",
        "    # 创建模型实例\n",
        "    # model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
        "\n",
        "    # 加载模型参数\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # 移动模型到指定设备\n",
        "    model.to(device)\n",
        "    print(\"Model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Model file does not exist.\")\n",
        "\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterionCls = nn.CrossEntropyLoss()\n",
        "# optimizerCls = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "from re import X\n",
        "import time\n",
        "\n",
        "hist = np.zeros(num_epochs)\n",
        "start_time = time.time()\n",
        "lstm = []\n",
        "\n",
        "y_train_pred_main = None\n",
        "y_train_pred_even = None\n",
        "y_train_pred_loss = None\n",
        "cls_to_draw = None\n",
        "\n",
        "for t in range(num_epochs):\n",
        "\n",
        "    for x_main_batch, x_even_batch, x_loss_batch, y_main_batch, y_even_batch, y_loss_batch, y_result_batch in zip(train_tensors['x_main'], train_tensors['x_even'], train_tensors['x_loss'], train_tensors['y_main'], train_tensors['y_even'], train_tensors['y_loss'], train_tensors['y_result']):\n",
        "\n",
        "        x_main_batch = x_main_batch.to(device)\n",
        "        x_even_batch = x_even_batch.to(device)\n",
        "        x_loss_batch = x_loss_batch.to(device)\n",
        "        y_main_batch = y_main_batch.to(device)\n",
        "        y_even_batch = y_even_batch.to(device)\n",
        "        y_loss_batch = y_loss_batch.to(device)\n",
        "        y_result_batch = y_result_batch.to(device)\n",
        "        # print(\"ok: \")\n",
        "        # 替换前10个元素的内容为第11个元素的内容\n",
        "        if len(x_main_batch) > 10:  # 确保有足够的元素\n",
        "            x_main_batch[:10] = x_main_batch[10:11]\n",
        "        # print(\"x_main_batch: \", x_main_batch)\n",
        "        y_train_pred_main, cls_main = model(x_main_batch)\n",
        "        y_train_pred_even, cls_even = model(x_even_batch)\n",
        "        y_train_pred_loss, cls_loss = model(x_loss_batch)\n",
        "\n",
        "        loss_main = criterion(y_train_pred_main, y_main_batch)\n",
        "        loss_even = criterion(y_train_pred_even, y_even_batch)\n",
        "        loss_loss = criterion(y_train_pred_loss, y_loss_batch)\n",
        "\n",
        "        tmp = torch.softmax(cls_main + cls_even + cls_loss, dim=1)\n",
        "        cls_to_draw = tmp\n",
        "\n",
        "        loss_result = criterionCls(tmp, y_result_batch.squeeze().long())\n",
        "        loss = loss_main + loss_even + loss_loss + loss_result\n",
        "\n",
        "        # print(\"Epoch \", t, \"MSE: \", loss.item())\n",
        "        hist[t] = loss.item()\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "    print(\"Epoch \", t, \"MSE: \", loss.item(), \"cls_main + cls_even + cls_loss: \", tmp)\n",
        "\n",
        "training_time = time.time()-start_time\n",
        "print(\"Training time: {}\".format(training_time))\n",
        "\n",
        "\n",
        "predict_main = pd.DataFrame(scalerMain.inverse_transform(y_train_pred_main.detach().cpu().numpy()))\n",
        "# original_main = pd.DataFrame(scalerMain.inverse_transform(train_tensors['y_main'].detach().numpy()))\n",
        "\n",
        "predict_even = pd.DataFrame(scalerEven.inverse_transform(y_train_pred_even.detach().cpu().numpy()))\n",
        "# original_even = pd.DataFrame(scalerEven.inverse_transform(train_tensors['y_even'].detach().numpy()))\n",
        "\n",
        "predict_loss = pd.DataFrame(scalerLoss.inverse_transform(y_train_pred_loss.detach().cpu().numpy()))\n",
        "\n",
        "predict_cls = pd.DataFrame(scalerLoss.inverse_transform(cls_to_draw.detach().cpu().numpy()))\n",
        "# original_loss = pd.DataFrame(scalerLoss.inverse_transform(train_tensors['y_loss'].detach().numpy()))\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# ax = sns.lineplot(x = original_main.index, y = original_main[0], label=\"Data Main\", color='royalblue')\n",
        "ax = sns.lineplot(x = predict_main.index, y = predict_main[0], label=\"Training Prediction Main(LSTM)\", color='tomato')\n",
        "\n",
        "# ax = sns.lineplot(x = original_even.index, y = original_even[0], label=\"Data Even\", color='red')\n",
        "ax = sns.lineplot(x = predict_even.index, y = predict_even[0], label=\"Training Prediction Even(LSTM)\", color='#FF6347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_loss.index, y = predict_loss[0], label=\"Training Prediction Loss(LSTM)\", color='#F31347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_cls.index, y = predict_cls[0], label=\"Training Prediction Class(LSTM)\", color='#a31347')\n",
        "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
        "ax.set_xlabel(\"Days\", size = 14)\n",
        "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
        "ax.set_xticklabels('', size=10)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "ax = sns.lineplot(data=hist, color='royalblue')\n",
        "ax.set_xlabel(\"Epoch\", size = 14)\n",
        "ax.set_ylabel(\"Loss\", size = 14)\n",
        "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(16)\n",
        "\n",
        "# 保存模型的状态字典\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_weights.pth')\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_model_weights_bk.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_nlT0bC6806"
      },
      "source": [
        "## GRU模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJB0MI-X6807"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/output'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "#@title 获取数据\n",
        "\n",
        "# filepath = '/kaggle/input/stock-time-series-20050101-to-20171231/AMZN_2006-01-01_to_2018-01-01.csv'\n",
        "# filepath = '/kaggle/input/amazon.csv'#@title 训练模型\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_dim = 1\n",
        "hidden_dim = 32\n",
        "num_layers = 2\n",
        "output_dim = 1\n",
        "output_cls_dim = 3\n",
        "num_epochs = 100\n",
        "batch_size = 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def split_data(dataMain, dataEven, dataLoss, dataResult, lookback, batch_size = 20):\n",
        "    data_raw_main = dataMain.to_numpy()\n",
        "    data_raw_even = dataEven.to_numpy()\n",
        "    data_raw_loss = dataLoss.to_numpy()\n",
        "    data_raw_result = dataResult.to_numpy()\n",
        "    data_main = []\n",
        "    data_even = []\n",
        "    data_loss = []\n",
        "    data_result = []\n",
        "\n",
        "    # create all possible sequences of length lookback\n",
        "    for index in range(len(data_raw_main) - lookback):\n",
        "        data_main.append(data_raw_main[index: index + lookback])\n",
        "    for index in range(len(data_raw_even) - lookback):\n",
        "        data_even.append(data_raw_even[index: index + lookback])\n",
        "    for index in range(len(data_raw_loss) - lookback):\n",
        "        data_loss.append(data_raw_loss[index: index + lookback])\n",
        "    for index in range(len(data_raw_result) - lookback):\n",
        "        data_result.append(data_raw_result[index: index + lookback])\n",
        "\n",
        "    data_main = np.array(data_main)\n",
        "    data_even = np.array(data_even)\n",
        "    data_loss = np.array(data_loss)\n",
        "    data_result = np.array(data_result)\n",
        "\n",
        "    test_set_size = int(np.round(0.2 * data_main.shape[0]))\n",
        "    train_set_size = data_main.shape[0]\n",
        "\n",
        "    # 将训练数据切割成多个批次\n",
        "    num_batches = train_set_size // batch_size\n",
        "    print(\"num_batches: \", num_batches, \" data_main.shape[0]: \", data_main.shape[0])\n",
        "\n",
        "\n",
        "    x_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, :-1, :], num_batches)\n",
        "    }\n",
        "    y_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, -1, :], num_batches)\n",
        "    }\n",
        "\n",
        "    x_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, :-1, :], num_batches)\n",
        "    }\n",
        "    y_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, -1, :], num_batches)\n",
        "    }\n",
        "    # print(\"x_train['main']: \", x_train['main'][1])\n",
        "    return [x_train, y_train, x_test, y_test]\n",
        "\n",
        "\n",
        "lookback = 10 # choose sequence length\n",
        "x_train, y_train, x_test, y_test = split_data(dataMain, dataEven, dataLoss, dataResult, lookback, 495)\n",
        "# x_train_batches, y_train_batches, x_test, y_test = split_data(price, lookback=20, batch_size=32)\n",
        "\n",
        "# # 打印批次的数量和每个批次的形状\n",
        "# print('Number of training batches:', len(x_train_batches))\n",
        "# if len(x_train_batches) > 0:\n",
        "#     print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "#     print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "# print('x_test.shape = ', x_test.shape)\n",
        "# print('y_test.shape = ', y_test.shape)\n",
        "\n",
        "# 打印批次的数量和每个批次的形状\n",
        "print('Number of training batches:', len(x_train['main']))\n",
        "if len(x_train['main']) > 0:\n",
        "    # print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "    # print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "    print('x_train.shape = ',x_train['main'][0].shape)\n",
        "    print('y_train.shape = ',y_train['main'][0].shape)\n",
        "    print('x_test.shape = ',x_test['main'][0].shape)\n",
        "    print('y_test.shape = ',y_test['main'][0].shape)\n",
        "    print('x_result.shape = ',y_train['result'][0].shape)\n",
        "\n",
        "# print(\"x_train: \", x_train)\n",
        "# print(\"y_train: \", y_train)\n",
        "\n",
        "# x_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train_batches]\n",
        "# y_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train_batches]\n",
        "import torch\n",
        "\n",
        "# 转换为字典\n",
        "train_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['result']]\n",
        "}\n",
        "\n",
        "test_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['result']]\n",
        "}\n",
        "\n",
        "\n",
        "# class LSTM(nn.Module):\n",
        "#     def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "#         super(LSTM, self).__init__()\n",
        "#         self.hidden_dim = hidden_dim\n",
        "#         self.num_layers = num_layers\n",
        "\n",
        "#         self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "#         self.fcCls = nn.Linear(hidden_dim, output_cls_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "#         out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "#         out1 = self.fc(out[:, -1, :])\n",
        "\n",
        "#         # 通过全连接层\n",
        "#         outCls = self.fcCls(out[:, -1, :])\n",
        "#         # 应用 softmax 激活\n",
        "#         outCls = torch.softmax(outCls, dim=1)\n",
        "\n",
        "#         return out1, outCls\n",
        "\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.fcCls = nn.Linear(hidden_dim, output_cls_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device=x.device).requires_grad_()\n",
        "        out, (hn) = self.gru(x, (h0.detach()))\n",
        "        out1 = self.fc(out[:, -1, :])\n",
        "        # 通过全连接层\n",
        "        outCls = self.fcCls(out[:, -1, :])\n",
        "        # 应用 softmax 激活\n",
        "        outCls = torch.softmax(outCls, dim=1)\n",
        "\n",
        "        return out1, outCls\n",
        "\n",
        "\n",
        "\n",
        "# model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "\n",
        "model = GRU(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).to(device)\n",
        "# criterion = torch.nn.MSELoss(reduction='mean')\n",
        "# optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "model_path = '/content/drive/MyDrive/ft_gru_model_weights.pth'\n",
        "\n",
        "# 判断模型文件是否存在\n",
        "if os.path.exists(model_path):\n",
        "    # 创建模型实例\n",
        "    # model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
        "\n",
        "    # 加载模型参数\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # 移动模型到指定设备\n",
        "    model.to(device)\n",
        "    print(\"Model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Model file does not exist.\")\n",
        "\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterionCls = nn.CrossEntropyLoss()\n",
        "# optimizerCls = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "from re import X\n",
        "import time\n",
        "\n",
        "hist = np.zeros(num_epochs)\n",
        "start_time = time.time()\n",
        "lstm = []\n",
        "\n",
        "y_train_pred_main = None\n",
        "y_train_pred_even = None\n",
        "y_train_pred_loss = None\n",
        "cls_to_draw = None\n",
        "\n",
        "for t in range(num_epochs):\n",
        "\n",
        "    for x_main_batch, x_even_batch, x_loss_batch, y_main_batch, y_even_batch, y_loss_batch, y_result_batch in zip(train_tensors['x_main'], train_tensors['x_even'], train_tensors['x_loss'], train_tensors['y_main'], train_tensors['y_even'], train_tensors['y_loss'], train_tensors['y_result']):\n",
        "\n",
        "        x_main_batch = x_main_batch.to(device)\n",
        "        x_even_batch = x_even_batch.to(device)\n",
        "        x_loss_batch = x_loss_batch.to(device)\n",
        "        y_main_batch = y_main_batch.to(device)\n",
        "        y_even_batch = y_even_batch.to(device)\n",
        "        y_loss_batch = y_loss_batch.to(device)\n",
        "        y_result_batch = y_result_batch.to(device)\n",
        "        # print(\"ok: \")\n",
        "        # 替换前10个元素的内容为第11个元素的内容\n",
        "        if len(x_main_batch) > 10:  # 确保有足够的元素\n",
        "            x_main_batch[:10] = x_main_batch[10:11]\n",
        "        # print(\"x_main_batch: \", x_main_batch)\n",
        "        y_train_pred_main, cls_main = model(x_main_batch)\n",
        "        y_train_pred_even, cls_even = model(x_even_batch)\n",
        "        y_train_pred_loss, cls_loss = model(x_loss_batch)\n",
        "\n",
        "        loss_main = criterion(y_train_pred_main, y_main_batch)\n",
        "        loss_even = criterion(y_train_pred_even, y_even_batch)\n",
        "        loss_loss = criterion(y_train_pred_loss, y_loss_batch)\n",
        "\n",
        "        tmp = torch.softmax(cls_main + cls_even + cls_loss, dim=1)\n",
        "        cls_to_draw = tmp\n",
        "\n",
        "        loss_result = criterionCls(tmp, y_result_batch.squeeze().long())\n",
        "        loss = loss_main + loss_even + loss_loss + loss_result\n",
        "\n",
        "        # print(\"Epoch \", t, \"MSE: \", loss.item())\n",
        "        hist[t] = loss.item()\n",
        "\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "    print(\"Epoch \", t, \"MSE: \", loss.item(), \"cls_main + cls_even + cls_loss: \", tmp)\n",
        "\n",
        "training_time = time.time()-start_time\n",
        "print(\"Training time: {}\".format(training_time))\n",
        "\n",
        "\n",
        "predict_main = pd.DataFrame(scalerMain.inverse_transform(y_train_pred_main.detach().cpu().numpy()))\n",
        "# original_main = pd.DataFrame(scalerMain.inverse_transform(train_tensors['y_main'].detach().numpy()))\n",
        "\n",
        "predict_even = pd.DataFrame(scalerEven.inverse_transform(y_train_pred_even.detach().cpu().numpy()))\n",
        "# original_even = pd.DataFrame(scalerEven.inverse_transform(train_tensors['y_even'].detach().numpy()))\n",
        "\n",
        "predict_loss = pd.DataFrame(scalerLoss.inverse_transform(y_train_pred_loss.detach().cpu().numpy()))\n",
        "\n",
        "predict_cls = pd.DataFrame(scalerLoss.inverse_transform(cls_to_draw.detach().cpu().numpy()))\n",
        "# original_loss = pd.DataFrame(scalerLoss.inverse_transform(train_tensors['y_loss'].detach().numpy()))\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# ax = sns.lineplot(x = original_main.index, y = original_main[0], label=\"Data Main\", color='royalblue')\n",
        "ax = sns.lineplot(x = predict_main.index, y = predict_main[0], label=\"Training Prediction Main(LSTM)\", color='tomato')\n",
        "\n",
        "# ax = sns.lineplot(x = original_even.index, y = original_even[0], label=\"Data Even\", color='red')\n",
        "ax = sns.lineplot(x = predict_even.index, y = predict_even[0], label=\"Training Prediction Even(LSTM)\", color='#FF6347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_loss.index, y = predict_loss[0], label=\"Training Prediction Loss(LSTM)\", color='#F31347')\n",
        "\n",
        "# ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_cls.index, y = predict_cls[0], label=\"Training Prediction Class(LSTM)\", color='#a31347')\n",
        "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
        "ax.set_xlabel(\"Days\", size = 14)\n",
        "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
        "ax.set_xticklabels('', size=10)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "ax = sns.lineplot(data=hist, color='royalblue')\n",
        "ax.set_xlabel(\"Epoch\", size = 14)\n",
        "ax.set_ylabel(\"Loss\", size = 14)\n",
        "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(16)\n",
        "\n",
        "# 保存模型的状态字典\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_gru_model_weights.pth')\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/ft_gru_model_weights_bk.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESFPpJkHgBxL"
      },
      "outputs": [],
      "source": [
        "#@title 绘制概率\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 类别标签\n",
        "categories = ['Class 0', 'Class 1', 'Class 2']\n",
        "\n",
        "# 绘制每一行的柱形图\n",
        "for i in range(cls_to_draw.size(0)):\n",
        "    plt.figure()\n",
        "    plt.bar(categories, cls_to_draw[i].detach().numpy(), color=['blue', 'orange', 'green'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel('Probability')\n",
        "    plt.title(f'Class Probability Distribution for Sample {i + 1}')\n",
        "    plt.show()\n",
        "    if i > 5:\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpNB_XiZfuRT"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# # 类别标签\n",
        "# categories = ['Class 0', 'Class 1', 'Class 2']\n",
        "# num_samples = cls_to_draw.size(0)\n",
        "\n",
        "# # 设置柱形图的宽度\n",
        "# bar_width = 0.2\n",
        "# x = np.arange(num_samples)\n",
        "\n",
        "# # 绘制柱形图\n",
        "# for i in range(cls_to_draw.size(1)):  # 遍历每个类别\n",
        "#     plt.bar(x + i * bar_width, cls_to_draw[:, i].detach().numpy(), width=bar_width, label=categories[i])\n",
        "\n",
        "# # 设置标签和标题\n",
        "# plt.xlabel('Samples')\n",
        "# plt.ylabel('Probability')\n",
        "# plt.title('Class Probability Distribution')\n",
        "# plt.xticks(x + bar_width, [f'Sample {i + 1}' for i in range(num_samples)])\n",
        "# plt.ylim(0, 1)\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTQEnT_LMqBu"
      },
      "source": [
        "## 验证"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UijTuZJhp_y"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/output'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "#@title 获取数据\n",
        "\n",
        "# filepath = '/kaggle/input/stock-time-series-20050101-to-20171231/AMZN_2006-01-01_to_2018-01-01.csv'\n",
        "# filepath = '/kaggle/input/amazon.csv'\n",
        "import pandas as pd\n",
        "\n",
        "file_paths = ['/content/drive/MyDrive/ftTrainDataDetail/mw_1250309.csv']\n",
        "dataframes = []\n",
        "tgtLength = 500\n",
        "\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(file_path, encoding='latin1')\n",
        "    df = df[::-1]  # 反转 DataFrame 的顺序\n",
        "\n",
        "    # 检查文件大小\n",
        "    if len(df) < tgtLength:\n",
        "        # 计算需要重复的行数\n",
        "        num_rows_to_add = tgtLength - len(df)\n",
        "        last_row = df.iloc[-1:]  # 获取最后一行\n",
        "        # 重复最后一行\n",
        "        repeated_rows = pd.concat([last_row] * num_rows_to_add, ignore_index=True)\n",
        "        df = pd.concat([df, repeated_rows], ignore_index=True)\n",
        "        # 创建补充的行，填充为零\n",
        "        # zero_rows = pd.DataFrame(0, index=range(num_rows_to_add), columns=df.columns)\n",
        "        # df = pd.concat([df, zero_rows], ignore_index=True)\n",
        "    else:\n",
        "        df = df.iloc[:tgtLength]\n",
        "\n",
        "    dataframes.append(df)\n",
        "\n",
        "# 合并所有数据\n",
        "data = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "\n",
        "# 保存为 CSV 文件\n",
        "data.to_csv('hello_output.csv', index=False)\n",
        "\n",
        "#@title 数据去除特殊符号\n",
        "\n",
        "dataMain = data[['Main']]\n",
        "dataEven = data[['Even']]\n",
        "dataLoss = data[['Loss']]\n",
        "dataResult = data[['Result']]\n",
        "\n",
        "dataMain['Main'] = dataMain['Main'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "dataEven['Even'] = dataEven['Even'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "dataLoss['Loss'] = dataLoss['Loss'].astype(str).str.replace('↑', '').str.replace('↓', '').astype(float)\n",
        "\n",
        "#@title 数据归一化\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scalerMain = MinMaxScaler(feature_range=(-1, 1))\n",
        "scalerEven = MinMaxScaler(feature_range=(-1, 1))\n",
        "scalerLoss = MinMaxScaler(feature_range=(-1, 1))\n",
        "scalerResult = MinMaxScaler(feature_range=(-1, 1))\n",
        "dataMain['Main'] = scalerMain.fit_transform(dataMain['Main'].values.reshape(-1,1))\n",
        "dataEven['Even'] = scalerEven.fit_transform(dataEven['Even'].values.reshape(-1,1))\n",
        "dataLoss['Loss'] = scalerLoss.fit_transform(dataLoss['Loss'].values.reshape(-1,1))\n",
        "dataResult['Result'] = dataResult['Result']\n",
        "print(dataMain['Main'] .shape)\n",
        "\n",
        "#@title 获取准备验证的数据\n",
        "\n",
        "import math, time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "input_dim = 1\n",
        "hidden_dim = 32\n",
        "num_layers = 2\n",
        "output_dim = 1\n",
        "output_cls_dim = 3\n",
        "num_epochs = 100\n",
        "batch_size = 2\n",
        "\n",
        "\n",
        "def split_data(dataMain, dataEven, dataLoss, dataResult, lookback, batch_size = 20):\n",
        "    data_raw_main = dataMain.to_numpy()\n",
        "    data_raw_even = dataEven.to_numpy()\n",
        "    data_raw_loss = dataLoss.to_numpy()\n",
        "    data_raw_result = dataResult.to_numpy()\n",
        "    data_main = []\n",
        "    data_even = []\n",
        "    data_loss = []\n",
        "    data_result = []\n",
        "\n",
        "    # create all possible sequences of length lookback\n",
        "    for index in range(len(data_raw_main) - lookback):\n",
        "        data_main.append(data_raw_main[index: index + lookback])\n",
        "    for index in range(len(data_raw_even) - lookback):\n",
        "        data_even.append(data_raw_even[index: index + lookback])\n",
        "    for index in range(len(data_raw_loss) - lookback):\n",
        "        data_loss.append(data_raw_loss[index: index + lookback])\n",
        "    for index in range(len(data_raw_result) - lookback):\n",
        "        data_result.append(data_raw_result[index: index + lookback])\n",
        "\n",
        "    data_main = np.array(data_main)\n",
        "    data_even = np.array(data_even)\n",
        "    data_loss = np.array(data_loss)\n",
        "    data_result = np.array(data_result)\n",
        "\n",
        "    test_set_size = int(np.round(0.2 * data_main.shape[0]))\n",
        "    train_set_size = data_main.shape[0]\n",
        "\n",
        "    # 将训练数据切割成多个批次\n",
        "    num_batches = train_set_size // batch_size\n",
        "    print(\"num_batches: \", num_batches, \" data_main.shape[0]: \", data_main.shape[0])\n",
        "\n",
        "\n",
        "    x_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, :-1, :], num_batches)\n",
        "    }\n",
        "    y_train = {\n",
        "        'main': np.array_split(data_main[:train_set_size, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[:train_set_size, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[:train_set_size, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[:train_set_size, -1, :], num_batches)\n",
        "    }\n",
        "\n",
        "    x_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, :-1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, :-1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, :-1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, :-1, :], num_batches)\n",
        "    }\n",
        "    y_test = {\n",
        "        'main': np.array_split(data_main[train_set_size:, -1, :], num_batches),\n",
        "        'even': np.array_split(data_even[train_set_size:, -1, :], num_batches),\n",
        "        'loss': np.array_split(data_loss[train_set_size:, -1, :], num_batches),\n",
        "        'result': np.array_split(data_result[train_set_size:, -1, :], num_batches)\n",
        "    }\n",
        "    # print(\"x_train['main']: \", x_train['main'][1])\n",
        "    return [x_train, y_train, x_test, y_test]\n",
        "\n",
        "\n",
        "lookback = 10 # choose sequence length\n",
        "x_train, y_train, x_test, y_test = split_data(dataMain, dataEven, dataLoss, dataResult, lookback, 490)\n",
        "\n",
        "# 打印批次的数量和每个批次的形状\n",
        "print('Number of training batches:', len(x_train['main']))\n",
        "if len(x_train['main']) > 0:\n",
        "    # print('Shape of each training batch (x):', x_train_batches[0].shape)\n",
        "    # print('Shape of each training batch (y):', y_train_batches[0].shape)\n",
        "\n",
        "    print('x_train.shape = ',x_train['main'][0].shape)\n",
        "    print('y_train.shape = ',y_train['main'][0].shape)\n",
        "    print('x_test.shape = ',x_test['main'][0].shape)\n",
        "    print('y_test.shape = ',y_test['main'][0].shape)\n",
        "    print('x_result.shape = ',y_train['result'][0].shape)\n",
        "\n",
        "# print(\"x_train: \", x_train)\n",
        "# print(\"y_train: \", y_train)\n",
        "\n",
        "# x_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train_batches]\n",
        "# y_train_batches_tensor = [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train_batches]\n",
        "import torch\n",
        "\n",
        "# 转换为字典\n",
        "train_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_train['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_train['result']]\n",
        "}\n",
        "\n",
        "test_tensors = {\n",
        "    'x_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['main']],\n",
        "    'x_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['even']],\n",
        "    'x_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['loss']],\n",
        "    'x_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in x_test['result']],\n",
        "    'y_main': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['main']],\n",
        "    'y_even': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['even']],\n",
        "    'y_loss': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['loss']],\n",
        "    'y_result': [torch.from_numpy(batch).type(torch.Tensor) for batch in y_test['result']]\n",
        "}\n",
        "\n",
        "for x_main_batch, x_even_batch, x_loss_batch, y_main_batch, y_even_batch, y_loss_batch, y_result_batch in zip(train_tensors['x_main'], train_tensors['x_even'], train_tensors['x_loss'], train_tensors['y_main'], train_tensors['y_even'], train_tensors['y_loss'], train_tensors['y_result']):\n",
        "\n",
        "    # 将数据移动到设备\n",
        "    x_main_batch = x_main_batch.to(device)\n",
        "    x_even_batch = x_even_batch.to(device)\n",
        "    x_loss_batch = x_loss_batch.to(device)\n",
        "    y_main_batch = y_main_batch.to(device)\n",
        "    y_even_batch = y_even_batch.to(device)\n",
        "    y_loss_batch = y_loss_batch.to(device)\n",
        "    y_result_batch = y_result_batch.to(device)\n",
        "    # print(\"ok: \")\n",
        "    # 替换前10个元素的内容为第11个元素的内容\n",
        "    if len(x_main_batch) > 10:  # 确保有足够的元素\n",
        "        x_main_batch[:10] = x_main_batch[10:11]\n",
        "    # print(\"x_main_batch: \", x_main_batch)\n",
        "    y_test_main_pred, cls_main = model(x_main_batch)\n",
        "    y_test_even_pred, cls_even = model(x_even_batch)\n",
        "    y_test_loss_pred, cls_loss = model(x_loss_batch)\n",
        "    tmp = torch.softmax(cls_main + cls_even + cls_loss, dim=1)\n",
        "    cls_to_draw = tmp\n",
        "# # make predictions\n",
        "# y_test_pred = model(x_test_tensor)\n",
        "# lstm = []\n",
        "# # invert predictions\n",
        "# y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())\n",
        "# # y_train = scaler.inverse_transform(y_train_lstm.detach().numpy())\n",
        "# y_test_main_pred = scalerMain.inverse_transform(y_test_main_pred.detach().numpy())\n",
        "# # y_test = scaler.inverse_transform(y_test_lstm_tensor.detach().numpy())\n",
        "\n",
        "# # calculate root mean squared error\n",
        "# trainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0]))\n",
        "# print('Train Score: %.2f RMSE' % (trainScore))\n",
        "# testScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0]))\n",
        "# print('Test Score: %.2f RMSE' % (testScore))\n",
        "# lstm.append(trainScore)\n",
        "# lstm.append(testScore)\n",
        "# lstm.append(training_time)\n",
        "\n",
        "predict_main = pd.DataFrame(scalerMain.inverse_transform(y_test_main_pred.detach().cpu().numpy()))\n",
        "# original_main = pd.DataFrame(scalerMain.inverse_transform(train_tensors['y_main'].detach().numpy()))\n",
        "\n",
        "predict_even = pd.DataFrame(scalerEven.inverse_transform(y_test_even_pred.detach().cpu().numpy()))\n",
        "# # original_even = pd.DataFrame(scalerEven.inverse_transform(train_tensors['y_even'].detach().numpy()))\n",
        "\n",
        "predict_loss = pd.DataFrame(scalerLoss.inverse_transform(y_test_loss_pred.detach().cpu().numpy()))\n",
        "\n",
        "# predict_cls = pd.DataFrame(scalerLoss.inverse_transform(cls_to_draw.detach().numpy()))\n",
        "# original_loss = pd.DataFrame(scalerLoss.inverse_transform(train_tensors['y_loss'].detach().numpy()))\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# ax = sns.lineplot(x = original_main.index, y = original_main[0], label=\"Data Main\", color='royalblue')\n",
        "ax = sns.lineplot(x = predict_main.index, y = predict_main[0], label=\"Training Prediction Main(LSTM)\", color='tomato')\n",
        "\n",
        "# ax = sns.lineplot(x = original_even.index, y = original_even[0], label=\"Data Even\", color='red')\n",
        "ax = sns.lineplot(x = predict_even.index, y = predict_even[0], label=\"Training Prediction Even(LSTM)\", color='#FF6347')\n",
        "\n",
        "# # ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "ax = sns.lineplot(x = predict_loss.index, y = predict_loss[0], label=\"Training Prediction Loss(LSTM)\", color='#F31347')\n",
        "\n",
        "# # ax = sns.lineplot(x = original_loss.index, y = original_loss[0], label=\"Data Loss\", color='green')\n",
        "# ax = sns.lineplot(x = predict_cls.index, y = predict_cls[0], label=\"Training Prediction Class(LSTM)\", color='#a31347')\n",
        "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
        "ax.set_xlabel(\"Days\", size = 14)\n",
        "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
        "ax.set_xticklabels('', size=10)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "ax = sns.lineplot(data=hist, color='royalblue')\n",
        "ax.set_xlabel(\"Epoch\", size = 14)\n",
        "ax.set_ylabel(\"Loss\", size = 14)\n",
        "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLEZWHXwESPV"
      },
      "outputs": [],
      "source": [
        "#@title 绘制图像\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 类别标签\n",
        "categories = ['Class 0', 'Class 1', 'Class 2']\n",
        "\n",
        "# 绘制每一行的柱形图\n",
        "for i in range(cls_to_draw.size(0)):\n",
        "    plt.figure()\n",
        "    plt.bar(categories, cls_to_draw[i].detach().cpu().numpy(), color=['blue', 'orange', 'green'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel('Probability')\n",
        "    plt.title(f'Class Probability Distribution for Sample {i + 1}')\n",
        "    plt.show()\n",
        "    if i > 5:\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6pm3DN8D5-F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# 类别标签\n",
        "categories = ['Class 0', 'Class 1', 'Class 2']\n",
        "\n",
        "# 绘制每一行的柱形图\n",
        "for i in range(cls_to_draw.size(0)):\n",
        "    plt.figure()\n",
        "    plt.bar(categories, cls_to_draw[i].detach().numpy(), color=['blue', 'orange', 'green'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel('Probability')\n",
        "    plt.title(f'Class Probability Distribution for Sample {i + 1}')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePRLskNmhp_y"
      },
      "outputs": [],
      "source": [
        "# shift train predictions for plotting\n",
        "trainPredictPlot = np.empty_like(price)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[lookback:len(y_train_pred)+lookback, :] = y_train_pred\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = np.empty_like(price)\n",
        "testPredictPlot[:, :] = np.nan\n",
        "testPredictPlot[len(y_train_pred)+lookback-1:len(price)-1, :] = y_test_pred\n",
        "\n",
        "original = scaler.inverse_transform(price['Close'].values.reshape(-1,1))\n",
        "\n",
        "predictions = np.append(trainPredictPlot, testPredictPlot, axis=1)\n",
        "predictions = np.append(predictions, original, axis=1)\n",
        "result = pd.DataFrame(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tH05FABhp_z"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[0],\n",
        "                    mode='lines',\n",
        "                    name='Train prediction')))\n",
        "fig.add_trace(go.Scatter(x=result.index, y=result[1],\n",
        "                    mode='lines',\n",
        "                    name='Test prediction'))\n",
        "fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[2],\n",
        "                    mode='lines',\n",
        "                    name='Actual Value')))\n",
        "fig.update_layout(\n",
        "    xaxis=dict(\n",
        "        showline=True,\n",
        "        showgrid=True,\n",
        "        showticklabels=False,\n",
        "        linecolor='white',\n",
        "        linewidth=2\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        title_text='Close (USD)',\n",
        "        titlefont=dict(\n",
        "            family='Rockwell',\n",
        "            size=12,\n",
        "            color='white',\n",
        "        ),\n",
        "        showline=True,\n",
        "        showgrid=True,\n",
        "        showticklabels=True,\n",
        "        linecolor='white',\n",
        "        linewidth=2,\n",
        "        ticks='outside',\n",
        "        tickfont=dict(\n",
        "            family='Rockwell',\n",
        "            size=12,\n",
        "            color='white',\n",
        "        ),\n",
        "    ),\n",
        "    showlegend=True,\n",
        "    template = 'plotly_dark'\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "annotations = []\n",
        "annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n",
        "                              xanchor='left', yanchor='bottom',\n",
        "                              text='Results (LSTM)',\n",
        "                              font=dict(family='Rockwell',\n",
        "                                        size=26,\n",
        "                                        color='white'),\n",
        "                              showarrow=False))\n",
        "fig.update_layout(annotations=annotations)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibf2gv4Qhp_z"
      },
      "outputs": [],
      "source": [
        "!pip install chart-studio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXr3BZs6hp_z"
      },
      "outputs": [],
      "source": [
        "# import chart_studio.plotly as py\n",
        "# import chart_studio\n",
        "\n",
        "# chart_studio.tools.set_credentials_file(username='rodolfo_saldanha', api_key='zWJIVWJs23wfiAp516Mh')\n",
        "# py.iplot(fig, filename='stock_prediction_lstm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjpe8i-Php_z"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
        "        out, (hn) = self.gru(x, (h0.detach()))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfWFqY2nhp_0"
      },
      "outputs": [],
      "source": [
        "model = GRU(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhuqmiqMhp_0"
      },
      "outputs": [],
      "source": [
        "hist = np.zeros(num_epochs)\n",
        "start_time = time.time()\n",
        "gru = []\n",
        "\n",
        "for t in range(num_epochs):\n",
        "    y_train_pred = model(x_train)\n",
        "\n",
        "    loss = criterion(y_train_pred, y_train_gru)\n",
        "    print(\"Epoch \", t, \"MSE: \", loss.item())\n",
        "    hist[t] = loss.item()\n",
        "\n",
        "    optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "training_time = time.time()-start_time\n",
        "print(\"Training time: {}\".format(training_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeuZe2BRhp_0"
      },
      "outputs": [],
      "source": [
        "predict = pd.DataFrame(scaler.inverse_transform(y_train_pred.detach().numpy()))\n",
        "original = pd.DataFrame(scaler.inverse_transform(y_train_gru.detach().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf5fFYRWhp_0"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "fig = plt.figure()\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "ax = sns.lineplot(x = original.index, y = original[0], label=\"Data\", color='royalblue')\n",
        "ax = sns.lineplot(x = predict.index, y = predict[0], label=\"Training Prediction (GRU)\", color='tomato')\n",
        "ax.set_title('Stock price', size = 14, fontweight='bold')\n",
        "ax.set_xlabel(\"Days\", size = 14)\n",
        "ax.set_ylabel(\"Cost (USD)\", size = 14)\n",
        "ax.set_xticklabels('', size=10)\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "ax = sns.lineplot(data=hist, color='royalblue')\n",
        "ax.set_xlabel(\"Epoch\", size = 14)\n",
        "ax.set_ylabel(\"Loss\", size = 14)\n",
        "ax.set_title(\"Training Loss\", size = 14, fontweight='bold')\n",
        "fig.set_figheight(6)\n",
        "fig.set_figwidth(16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVjVWgzThp_1"
      },
      "outputs": [],
      "source": [
        "import math, time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# make predictions\n",
        "y_test_pred = model(x_test)\n",
        "\n",
        "# invert predictions\n",
        "y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())\n",
        "y_train = scaler.inverse_transform(y_train_gru.detach().numpy())\n",
        "y_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy())\n",
        "y_test = scaler.inverse_transform(y_test_gru.detach().numpy())\n",
        "\n",
        "# calculate root mean squared error\n",
        "trainScore = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0]))\n",
        "print('Train Score: %.2f RMSE' % (trainScore))\n",
        "testScore = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0]))\n",
        "print('Test Score: %.2f RMSE' % (testScore))\n",
        "gru.append(trainScore)\n",
        "gru.append(testScore)\n",
        "gru.append(training_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0SJoPGthp_1"
      },
      "outputs": [],
      "source": [
        "# shift train predictions for plotting\n",
        "trainPredictPlot = np.empty_like(price)\n",
        "trainPredictPlot[:, :] = np.nan\n",
        "trainPredictPlot[lookback:len(y_train_pred)+lookback, :] = y_train_pred\n",
        "\n",
        "# shift test predictions for plotting\n",
        "testPredictPlot = np.empty_like(price)\n",
        "testPredictPlot[:, :] = np.nan\n",
        "testPredictPlot[len(y_train_pred)+lookback-1:len(price)-1, :] = y_test_pred\n",
        "\n",
        "original = scaler.inverse_transform(price['Close'].values.reshape(-1,1))\n",
        "\n",
        "predictions = np.append(trainPredictPlot, testPredictPlot, axis=1)\n",
        "predictions = np.append(predictions, original, axis=1)\n",
        "result = pd.DataFrame(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHNND6KThp_1"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[0],\n",
        "                    mode='lines',\n",
        "                    name='Train prediction')))\n",
        "fig.add_trace(go.Scatter(x=result.index, y=result[1],\n",
        "                    mode='lines',\n",
        "                    name='Test prediction'))\n",
        "fig.add_trace(go.Scatter(go.Scatter(x=result.index, y=result[2],\n",
        "                    mode='lines',\n",
        "                    name='Actual Value')))\n",
        "fig.update_layout(\n",
        "    xaxis=dict(\n",
        "        showline=True,\n",
        "        showgrid=True,\n",
        "        showticklabels=False,\n",
        "        linecolor='white',\n",
        "        linewidth=2\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        title_text='Close (USD)',\n",
        "        titlefont=dict(\n",
        "            family='Rockwell',\n",
        "            size=12,\n",
        "            color='white',\n",
        "        ),\n",
        "        showline=True,\n",
        "        showgrid=True,\n",
        "        showticklabels=True,\n",
        "        linecolor='white',\n",
        "        linewidth=2,\n",
        "        ticks='outside',\n",
        "        tickfont=dict(\n",
        "            family='Rockwell',\n",
        "            size=12,\n",
        "            color='white',\n",
        "        ),\n",
        "    ),\n",
        "    showlegend=True,\n",
        "    template = 'plotly_dark'\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "annotations = []\n",
        "annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n",
        "                              xanchor='left', yanchor='bottom',\n",
        "                              text='Results (GRU)',\n",
        "                              font=dict(family='Rockwell',\n",
        "                                        size=26,\n",
        "                                        color='white'),\n",
        "                              showarrow=False))\n",
        "fig.update_layout(annotations=annotations)\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrpcQQ68hp_1"
      },
      "outputs": [],
      "source": [
        "lstm = pd.DataFrame(lstm, columns=['LSTM'])\n",
        "gru = pd.DataFrame(gru, columns=['GRU'])\n",
        "result = pd.concat([lstm, gru], axis=1, join='inner')\n",
        "result.index = ['Train RMSE', 'Test RMSE', 'Train Time']\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5k92uuQ4hp_2"
      },
      "outputs": [],
      "source": [
        "py.iplot(fig, filename='stock_prediction_gru')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}